{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "JriDvJulOsns",
      "metadata": {
        "id": "JriDvJulOsns"
      },
      "source": [
        "# Deep Learning With Tensor Flow 1 (CSE 3793)\n",
        "# ASSIGNMENT-1: INTRODUCTION TO NEURAL NETWORK\n",
        "\n",
        "\n",
        "### Name:Tribhuwan Singh\n",
        "### Reg. No.: 2341019538\n",
        "### Section:23412C3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4L4aOF_ZPDDq",
      "metadata": {
        "id": "4L4aOF_ZPDDq"
      },
      "source": [
        "Write a Python code to build a feed-forward neural network for AND, OR, and NAND with a single\u0002layer perceptron from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssXtQUWD5-v2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssXtQUWD5-v2",
        "outputId": "52632b78-1dee-47ea-e1b0-6a6d44304191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 0\n",
            "Z= 0.0\n",
            "predicted 1 error -1 weights: [0. 0.], Bias: -0.1 Z= -0.1\n",
            "predicted 0 error 0 weights: [0. 0.], Bias: -0.1 Z= -0.1\n",
            "predicted 0 error 0 weights: [0. 0.], Bias: -0.1 Z= -0.1\n",
            "predicted 0 error 1 weights: [0.1 0.1], Bias: 0.0 \n",
            "Epochs: 1\n",
            "Z= 0.0\n",
            "predicted 1 error -1 weights: [0.1 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error -1 weights: [0.1 0. ], Bias: -0.2 Z= -0.1\n",
            "predicted 0 error 0 weights: [0.1 0. ], Bias: -0.2 Z= -0.1\n",
            "predicted 0 error 1 weights: [0.2 0.1], Bias: -0.1 \n",
            "Epochs: 2\n",
            "Z= -0.1\n",
            "predicted 0 error 0 weights: [0.2 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error -1 weights: [0.2 0. ], Bias: -0.2 Z= 0.0\n",
            "predicted 1 error -1 weights: [0.1 0. ], Bias: -0.30000000000000004 Z= -0.20000000000000004\n",
            "predicted 0 error 1 weights: [0.2 0.1], Bias: -0.20000000000000004 \n",
            "Epochs: 3\n",
            "Z= -0.20000000000000004\n",
            "predicted 0 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 Z= -2.7755575615628914e-17\n",
            "predicted 0 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 Z= 0.1\n",
            "predicted 1 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 \n",
            "Epochs: 4\n",
            "Z= -0.20000000000000004\n",
            "predicted 0 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 Z= -2.7755575615628914e-17\n",
            "predicted 0 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 Z= 0.1\n",
            "predicted 1 error 0 weights: [0.2 0.1], Bias: -0.20000000000000004 \n",
            "Final predictions [0, 0, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "#Q1\n",
        "#perceptron for AND gate\n",
        "import numpy as np\n",
        "def step_function(z):\n",
        "    return 1 if z>=0 else 0\n",
        "def perceptron_train(X, y, learning_rate=0.1, epochs=5):\n",
        "    weights = np.zeros (X.shape[1])\n",
        "    bias=0\n",
        "    for j in range(epochs):\n",
        "        print (\"Epochs:\",j)\n",
        "        for i in range(len(X)):\n",
        "            z=np.dot(weights,X[i])+bias\n",
        "            print(\"Z=\" , z)\n",
        "            y_pred=step_function (z)\n",
        "            print(\"predicted\",y_pred,end=' ')\n",
        "            error= y[i]-y_pred\n",
        "            print (\"error\",error,end=' ')\n",
        "            weights+= learning_rate*error*X[i]\n",
        "            bias+= learning_rate * error\n",
        "            print(f\"weights: {weights}, Bias: {bias}\", end=' ')\n",
        "        print()\n",
        "    return weights ,bias\n",
        "X= np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y=np.array([0,0,0,1])\n",
        "weights, bias= perceptron_train(X,y)\n",
        "def perceptron_predict(X,weights,bias):\n",
        "    return[step_function (np.dot(weights,x)+bias)for x in X]\n",
        "outputs= perceptron_predict(X,weights, bias)\n",
        "print(\"Final predictions\", outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nXd_4mZl6AOd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXd_4mZl6AOd",
        "outputId": "f42c2887-60c6-4934-be30-b0fbaf82b3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 0\n",
            "Z= 0.0\n",
            "predicted 1 error -1 weights: [0. 0.], Bias: -0.1 Z= -0.1\n",
            "predicted 0 error 1 weights: [0.  0.1], Bias: 0.0 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.  0.1], Bias: 0.0 Z= 0.1\n",
            "predicted 1 error 0 weights: [0.  0.1], Bias: 0.0 \n",
            "Epochs: 1\n",
            "Z= 0.0\n",
            "predicted 1 error -1 weights: [0.  0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.  0.1], Bias: -0.1 Z= -0.1\n",
            "predicted 0 error 1 weights: [0.1 0.1], Bias: 0.0 Z= 0.2\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: 0.0 \n",
            "Epochs: 2\n",
            "Z= 0.0\n",
            "predicted 1 error -1 weights: [0.1 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.1\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 \n",
            "Epochs: 3\n",
            "Z= -0.1\n",
            "predicted 0 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.1\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 \n",
            "Epochs: 4\n",
            "Z= -0.1\n",
            "predicted 0 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 Z= 0.1\n",
            "predicted 1 error 0 weights: [0.1 0.1], Bias: -0.1 \n",
            "Final predictions [0, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "#perceptron for OR gate\n",
        "import numpy as np\n",
        "def step_function(z):\n",
        "    return 1 if z>=0 else 0\n",
        "def perceptron_train(X, y, learning_rate=0.1, epochs=5):\n",
        "    weights = np.zeros (X.shape[1])\n",
        "    bias=0\n",
        "    for j in range(epochs):\n",
        "        print (\"Epochs:\",j)\n",
        "        for i in range(len(X)):\n",
        "            z=np.dot(weights,X[i])+bias\n",
        "            print(\"Z=\" , z)\n",
        "            y_pred=step_function (z)\n",
        "            print(\"predicted\",y_pred,end=' ')\n",
        "            error= y[i]-y_pred\n",
        "            print (\"error\",error,end=' ')\n",
        "            weights+= learning_rate*error*X[i]\n",
        "            bias+= learning_rate * error\n",
        "            print(f\"weights: {weights}, Bias: {bias}\", end=' ')\n",
        "        print()\n",
        "    return weights ,bias\n",
        "X= np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y=np.array([0,1,1,1])\n",
        "weights, bias= perceptron_train(X,y)\n",
        "def perceptron_predict(X,weights,bias):\n",
        "    return[step_function (np.dot(weights,x)+bias)for x in X]\n",
        "outputs= perceptron_predict(X,weights, bias)\n",
        "print(\"Final predictions\", outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xoW02vKH6DA-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoW02vKH6DA-",
        "outputId": "d6926d00-795b-4719-d77c-c269983bc7be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 0\n",
            "Z= 0.0\n",
            "predicted 1 error 0 weights: [0. 0.], Bias: 0.0 Z= 0.0\n",
            "predicted 1 error 0 weights: [0. 0.], Bias: 0.0 Z= 0.0\n",
            "predicted 1 error 0 weights: [0. 0.], Bias: 0.0 Z= 0.0\n",
            "predicted 1 error -1 weights: [-0.1 -0.1], Bias: -0.1 \n",
            "Epochs: 1\n",
            "Z= -0.1\n",
            "predicted 0 error 1 weights: [-0.1 -0.1], Bias: 0.0 Z= -0.1\n",
            "predicted 0 error 1 weights: [-0.1  0. ], Bias: 0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.1  0. ], Bias: 0.1 Z= 0.0\n",
            "predicted 1 error -1 weights: [-0.2 -0.1], Bias: 0.0 \n",
            "Epochs: 2\n",
            "Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.0 Z= -0.1\n",
            "predicted 0 error 1 weights: [-0.2  0. ], Bias: 0.1 Z= -0.1\n",
            "predicted 0 error 1 weights: [-0.1  0. ], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error -1 weights: [-0.2 -0.1], Bias: 0.1 \n",
            "Epochs: 3\n",
            "Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.1 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.1 Z= -0.1\n",
            "predicted 0 error 1 weights: [-0.1 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error -1 weights: [-0.2 -0.2], Bias: 0.1 \n",
            "Epochs: 4\n",
            "Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.2], Bias: 0.1 Z= -0.1\n",
            "predicted 0 error 1 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 5\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 6\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 7\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 8\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 9\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 10\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 11\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 12\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 13\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 14\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 15\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 16\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 17\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 18\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Epochs: 19\n",
            "Z= 0.2\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.1\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= 0.0\n",
            "predicted 1 error 0 weights: [-0.2 -0.1], Bias: 0.2 Z= -0.10000000000000003\n",
            "predicted 0 error 0 weights: [-0.2 -0.1], Bias: 0.2 \n",
            "Final predictions [1, 1, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "#perceptron for NAND gate\n",
        "import numpy as np\n",
        "def step_function(z):\n",
        "    return 1 if z>=0 else 0\n",
        "def perceptron_train(X, y, learning_rate=0.1, epochs=20):\n",
        "    weights = np.zeros (X.shape[1])\n",
        "    bias=0\n",
        "    for j in range(epochs):\n",
        "        print (\"Epochs:\",j)\n",
        "        for i in range(len(X)):\n",
        "            z=np.dot(weights,X[i])+bias\n",
        "            print(\"Z=\" , z)\n",
        "            y_pred=step_function (z)\n",
        "            print(\"predicted\",y_pred,end=' ')\n",
        "            error= y[i]-y_pred\n",
        "            print (\"error\",error,end=' ')\n",
        "            weights+= learning_rate*error*X[i]\n",
        "            bias+= learning_rate * error\n",
        "            print(f\"weights: {weights}, Bias: {bias}\", end=' ')\n",
        "        print()\n",
        "    return weights ,bias\n",
        "X= np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y=np.array([1,1,1,0])\n",
        "weights, bias= perceptron_train(X,y)\n",
        "def perceptron_predict(X,weights,bias):\n",
        "    return[step_function (np.dot(weights,x)+bias)for x in X]\n",
        "outputs= perceptron_predict(X,weights, bias)\n",
        "print(\"Final predictions\", outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c27cd3d",
      "metadata": {
        "id": "0c27cd3d"
      },
      "source": [
        "2.write a python code to show different activation function?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82167c43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "82167c43",
        "outputId": "c41f67f0-7c6b-4bb4-f6e2-f9505f02970f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7de28364bdd0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGzCAYAAAASZnxRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcXBJREFUeJzt3Xd4FOXax/HvbE3vjUBIIHTpVbCAVEFUxMKxgg1RsKEe5FUp6gG7ICqoR7FyLFiwoBAQQUERKUovIXRIg/Rk28z7R5IlIQESyGayyf25rr1m5pnZmXufbLK/zMzOKJqmaQghhBBC6MCgdwFCCCGEaLgkiAghhBBCNxJEhBBCCKEbCSJCCCGE0I0EESGEEELoRoKIEEIIIXQjQUQIIYQQupEgIoQQQgjdSBARQgghhG4kiAjhhcaMGUNCQoIu2542bRqKouiy7by8PO666y5iYmJQFIWHHnpIlzrORs8+EsLbSBARwgPefPNNFEWhV69e57yOI0eOMG3aNDZt2lRzhVVRQUEB06ZN45dffqn1bZ/JjBkzeP/997n33nv56KOPuPXWW3Wrpa72kRDeRpF7zQhR8y666CKOHDnCvn372L17Ny1atKj2Ov766y969OjB/PnzGTNmTLl5DocDVVWxWq01VHF5GRkZREZGMnXqVKZNm1ZuntPpxOl04uPj45Ftn8mFF16IyWTit99+q/Vtn6qu9pEQ3kb2iAhRw1JSUlizZg2vvPIKkZGRfPLJJzW+DbPZ7LEQcjYmk0m3D9i0tDRCQkJ02XZ16NlHQngdTQhRo5555hktNDRUs9ls2r333qu1bNmy0uVOnDihPfTQQ1p8fLxmsVi0xo0ba7feequWnp6urVixQgMqPObPn69pmqaNHj1ai4+P1zRN0+x2uxYaGqqNGTOmwjays7M1q9WqPfLII5qmaZrNZtOeeuoprWvXrlpQUJDm5+enXXzxxdrPP//sfk5KSkql2546daqmaZo2depU7dQ/HQ6HQ3v66ae15s2baxaLRYuPj9cmT56sFRUVlVsuPj5eu+KKK7Rff/1V69Gjh2a1WrVmzZppH3zwwRn79HT9kZKSos2fP989XtlzVqxY4W7r27evdsEFF2hbt27V+vXrp/n6+mqxsbHa888/X2GbhYWF2tSpU7WWLVtqVqtVi4mJ0a655hptz549dbKPhPBWskdEiBr2ySefMHLkSCwWCzfeeCO7d+9m3bp15ZbJy8vjkksuYc6cOQwePJjZs2czbtw4duzYwaFDh2jbti1PP/00AGPHjuWjjz7io48+4tJLL62wPbPZzDXXXMM333yD3W4vN++bb77BZrPxr3/9C4CcnBz++9//0q9fP55//nmmTZtGeno6Q4YMcZ+LEhkZydy5cwG45ppr3NseOXLkaV/zXXfdxZQpU+jatSuvvvoqffv2ZebMme7tlrVnzx6uu+46Bg0axMsvv0xoaChjxoxh69atp11/27Zt+eijj4iIiKBz587umiIjI0/7nNM5ceIEl19+OZ06deLll1+mTZs2TJo0iR9//NG9jMvlYvjw4UyfPp1u3brx8ssv8+CDD5Kdnc2WLVvqZB8J4bX0TkJC1Cd//fWXBmhJSUmapmmaqqpakyZNtAcffLDcclOmTNEA7auvvqqwDlVVNU3TtHXr1pXbC1JW2T0imqZpS5Ys0QDtu+++K7fcsGHDtObNm7unnU6nZrPZyi1z4sQJLTo6Wrvjjjvcbenp6eX+wy/r1P/2N23apAHaXXfdVW65Rx99VAPK7W2Jj4/XAG3VqlXutrS0tHJ7bc6kdG9BWdXdIwJoH374obvNZrNpMTEx2rXXXutue++99zRAe+WVVyrUUPrzqat9JIS3kT0iQtSgTz75hOjoaC677DIAFEVh1KhRfPrpp7hcLvdyX375JZ06deKaa66psI5z+dpn//79iYiI4LPPPnO3nThxgqSkJEaNGuVuMxqNWCwWAFRV5fjx4zidTrp3786GDRuqvV2AxYsXAzBx4sRy7Y888ggAP/zwQ7n2du3acckll7inIyMjad26NXv37j2n7VdXQEAAt9xyi3vaYrHQs2fPctv/8ssviYiI4P7776/w/HP5+XhbHwlRmySICFFDXC4Xn376KZdddhkpKSns2bOHPXv20KtXL1JTU1m+fLl72eTkZNq3b19j2zaZTFx77bUsWrQIm80GwFdffYXD4SgXRAA++OADOnbsiI+PD+Hh4URGRvLDDz+QnZ19Ttvev38/BoOhwjeDYmJiCAkJYf/+/eXamzZtWmEdoaGhnDhx4py2X11NmjSpECZO3X5ycjKtW7fGZDLVyDa9rY+EqE0SRISoIT///DNHjx7l008/pWXLlu7HDTfcAOCRb8+U9a9//Yvc3Fz3uQ6ff/45bdq0oVOnTu5lPv74Y8aMGUNiYiLvvvsuP/30E0lJSfTv3x9VVc9r+1XdU2A0Gitt187xSgKn227ZPVCe3H516NVHQtRlNRP3hRB88sknREVF8cYbb1SY99VXX/H1118zb948fH19SUxMZMuWLWdcX3UPAVx66aU0atSIzz77jIsvvpiff/6ZJ554otwyCxcupHnz5nz11Vfl1j916tRz3nZ8fDyqqrJ7927atm3rbk9NTSUrK4v4+PhqvY7qCg0NBSArK6tc+6l7GaojMTGRtWvX4nA4MJvNlS7jTX0kRF0me0SEqAGFhYV89dVXDB8+nOuuu67CY8KECeTm5vLtt98CcO211/L333/z9ddfV1hX6X+9/v7+QMUP2NMxGAxcd911fPfdd3z00Uc4nc4Kh2VK/9Mu+5/12rVr+f3338st5+fnV+VtDxs2DIBZs2aVa3/llVcAuOKKK6pU/7lKTEwEYNWqVe42l8vF22+/fc7rvPbaa8nIyOD111+vMK+077ypj4Soy2SPiBA14NtvvyU3N5errrqq0vkXXnih++Jmo0aN4rHHHmPhwoVcf/313HHHHXTr1o3jx4/z7bffMm/ePDp16kRiYiIhISHMmzePwMBA/P396dWrF82aNTttHaNGjWLOnDlMnTqVDh06lPvvG2D48OF89dVXXHPNNVxxxRWkpKQwb9482rVrR15enns5X19f2rVrx2effUarVq0ICwujffv2lZ7X0qlTJ0aPHs3bb79NVlYWffv25c8//+SDDz5gxIgR7hN3PeWCCy7gwgsvZPLkyRw/fpywsDA+/fRTnE7nOa/ztttu48MPP2TixIn8+eefXHLJJeTn57Ns2TLuu+8+rr76aq/qIyHqND2/siNEfXHllVdqPj4+Wn5+/mmXGTNmjGY2m7WMjAxN0zQtMzNTmzBhgta4cWPNYrFoTZo00UaPHu2er2matmjRIq1du3aayWQ67QXNylJVVYuLi9MA7dlnn610/owZM7T4+HjNarVqXbp00b7//vtK17dmzRqtW7dumsViqdLFuqZPn641a9ZMM5vNWlxc3Bkv1nWqvn37an379j1t353t+cnJydrAgQM1q9WqRUdHa//3f/+nJSUlnfaCZqeq7PUXFBRoTzzxhPs1xcTEaNddd52WnJzsXqYu9pEQ3kbuNSOEEEII3cg5IkIIIYTQjQQRIYQQQuhGgogQQgghdCNBRAghhBC6kSAihBBCCN1IEBFCCCGEbur0Bc1UVeXIkSMEBgae0x0vhRBCCFH7NE0jNzeX2NhYDIYz7/Oo00HkyJEjxMXF6V2GEEIIIc7BwYMHadKkyRmXqdNBJDAwECh+IUFBQTpXoz+Hw8HSpUsZPHjwaW/EJc6f9HPtkH6uHdLPtUf6+qScnBzi4uLcn+NnUqeDSOnhmKCgIAkiFL/J/fz8CAoKavBvck+Sfq4d0s+1Q/q59khfV1SV0yrkZFUhhBBC6EaCiBBCCCF0I0FECCGEELqp0+eIVIWmaTidTlwul96leJzD4cBkMlFUVFTvX6/RaMRkMsnXtoUQop7z6iBit9s5evQoBQUFepdSKzRNIyYmhoMHDzaID2g/Pz8aNWqExWLRuxQhhBAe4rVBRFVVUlJSMBqNxMbGYrFY6v2Hs6qq5OXlERAQcNYLxHgzTdOw2+2kp6eTkpJCy5Yt6/XrFUKIhsxrg4jdbkdVVeLi4vDz89O7nFqhqip2ux0fH596/8Hs6+uL2Wxm//797tcshBCi/vH6T7P6/oHckMnPVggh6j/5Sy+EEEII3UgQEUIIIYRuPBpEpk2bhqIo5R5t2rTx5Ca92pgxYxgxYoTeZQCQkJDArFmzzriMoih88803tVKPEEKI+snjJ6tecMEFLFu27OQGTV57fqzHzZ49G03T9C4DgHXr1uHv7693GUIIIeo5j6cCk8lETExMlZa12WzYbDb3dE5ODlB8IS+Hw1FuWYfDgaZpqKqKqqo1V7COSu9SeLrXUxpSSl+3J4WHh5+xllKe7H9VVdE0DYfDgdFo9Mg2KlP6Xjv1PSdqlvRz7ZB+rj3e1te5qSn8eddIom6/iQ4jH6vRdVenDzweRHbv3k1sbCw+Pj707t2bmTNn0rRp00qXnTlzJtOnT6/QvnTp0gpf0S0NOHl5edjtdqD4A7rIoU8o8TEbqnwdk0WLFvH888+TkpKCr68vHTt25JNPPuGxxx4jOzubTz75BIDc3FwmTpzI4sWLCQwM5IEHHmDx4sV06NCBmTNnAtCxY0duu+029uzZw/fff09oaCjPP/88PXv25IEHHmDVqlXEx8fz+uuv06VLF3cN3377LTNnzmTv3r1ER0czduxYJkyY4J7fsWNH7r33Xu69914AkpOTuf/++9mwYQMJCQnu7RcWFroDY02z2+0UFhayatUqnE6nR7ZxJklJSbW+zYZI+rl2SD/XHm/oa9VpxzR/Oi32uch85SO+U+IxWgNqbP3VudCoR4NIr169eP/992ndujVHjx5l+vTpXHLJJWzZssX9339ZkydPZuLEie7pnJwc4uLiGDx4MEFBQeWWLSoq4uDBgwQEBLivMVFgd9LleX3eAFumDcLPcvbuPHr0KHfddRfPP/88I0aMIDc3l99++43AwEDMZjMmk8n9Wh999FHWrVvHN998Q3R0NFOnTuWff/6hW7du7mUMBgNz587l2WefZfr06cyaNYt7772X3r17c/vtt/PKK6/w+OOPM378eDZv3oyiKKxfv57bb7+dqVOncsMNN7BmzRomTJhAbGwsY8aMca/Xx8eHoKAgVFVlzJgxREdH8/vvv5Odne3+Ofn6+lb42dSUoqIifH19ufTSS2v1OiIOh4OkpCQGDRokt/L2IOnn2iH9XHu8qa+TJg6h2R4XDiP4PzaGK6++oUbXX51/UD0aRIYOHeoe79ixI7169SI+Pp7PP/+cO++8s8LyVqsVq9Vaod1sNlf4obpcLhRFwWAwuK83oed1J8rWcSapqak4nU6uvfZa4uPjAejUqROA+4Reg8FAbm4uH374IQsWLGDQoEEAvPfeezRp0sS9vVLDhg1z77mYOnUq8+bNo2fPnowaNQqAxx9/nN69e5Oenk5MTAyzZs1iwIABTJkyBYA2bdqwY8cOXn75Ze644w73ektrWbZsGTt27GDJkiXExsYCMGPGDIYOHVrl130uDIbivUyV/fxrg17bbWikn2uH9HPtqet9vWbuAzRLOgpAxr860v+6STW+jeq8/lo9czQkJIRWrVqxZ88ej6zf12xk29NDPLLuqmy7Kjp16sSAAQPo0KEDQ4YMYfDgwVx33XWEhoaWW27v3r04HA569uzpbgsODqZFixYV1tmxY0f3eHR0NAAdOnSo0JaWlkZMTAzbt2/n6quvLreOiy66iFmzZuFyuSqcj7F9+3bi4uLcIQSgd+/eVXq9Qggh6o49qxbgM7f4yEFyz0CGP/WZzhXVchDJy8sjOTmZW2+91SPrVxSlSodH9GQ0GklKSmLNmjUsXbqUOXPm8MQTT7B27dpzXmfZ5Fl6nkplbfXlpF4hhBDVl31kJ0cmPUOkHQ7EGRg8r26cy+LRYxmPPvooK1euZN++faxZs4ZrrrkGo9HIjTfe6MnN1nmKonDRRRcxffp0Nm7ciMVi4euvvy63TPPmzTGbzaxbt87dlp2dTXJy8nlvv23btqxevbpc2+rVq2nVqlWl305p27YtBw8e5OjRo+62P/7447zrEEIIUTtcDjtrxl5H5Ak4EQhd35yPxS9Y77IAD+8ROXToEDfeeCOZmZlERkZy8cUX88cffxAZGenJzdZpa9euZfny5QwePJioqCjWrl1Leno6bdu25Z9//nEvFxgYyOjRo3nssccICwsjKiqKKVOmuM+bOB+PPPIIPXr04JlnnmHUqFH8/vvvvP7667z55puVLj9w4EBatWrF6NGjefHFF8nJyeGJJ544rxqEEELUnp8eGETzPU7sJgieeh+RLXue/Um1xKNB5NNPP/Xk6r1SUFAQq1atYtasWeTk5BAfH8/LL7/M0KFD+eyz8sfqXnnlFcaNG8fw4cMJCgriscceY//+/ef9DZKuXbvy+eefM2XKFJ555hkaNWrE008/7f7GzKkMBgNff/01d955Jz179iQhIYHXXnuNyy+//LzqEEII4Xm/zb6H5ivSADhxS3f6Db9f54rKq9snVNRDbdu25aeffqp03vvvv19uOjAw0H1NESi+rsj06dO555573G379u2rsJ5Tr86akJBQoe3aa6/l2muvPW2dp663VatW/Prrr2fcjhBCiLpl17L5+P93FQDJfUIY/vhHOldUkQSROmzjxo3s2LGDnj17kp2d7b7Y26nfeBFCCCFOdWL/ZlKffIEIB+xPMDLkjbpxcuqpJIjUcS+99BI7d+7EYrHQtWtXFi9eTEREhN5lCSGEqMMchXmsHXcj8VmQGQw9532C2bfmrpxakySI1GFdunRh/fr17mlVVT12OXUhhBD1x9L7h9A8xYXNDBFPP0xYQie9Szot/S5FKoQQQogat/LF22n+23EAcm6/iDZDxupc0ZlJEBFCCCHqie0/vknwB8XXedp7aQSXTvyvzhWdnQQRIYQQoh7ISF7P8alzsDphX3MjQ+Ys1bukKpEgIoQQQng5e0E26++9jbAcSA+BC9/6DJPVV++yqkSCiBBCCOHllt43hKYHVAot0GjG44TGXaB3SVUmQUQIIYTwYitm3ETiH9kAFI69jJb9R+tcUfVIEPES/fr14+GHH9a7DCGEEHXI5m9eJeyTjQCkDIjmogmV3zOsLpMgIoQQQnihtB1ryH32bSwuSGlpZsgs7zg59VQSROoAu92udwlCCCG8iD0vi00T7iY0D9LC4OK3v8Jotuhd1jmpX0FE08Cer8+jGjeA69evHxMmTOChhx4iIiKCIUOGsGXLFoYOHUpAQADR0dHceuutZGRknHYdiqLwzTfflGsLCQmpcOM8IYQQ9c/ScYOIO6RSYIG4F6YT1KiF3iWds/p1iXdHAcyI1Wfb/3cELP5VXvyDDz7g3nvvZfXq1WRlZdG/f3/uuusuXn31VQoLC5k0aRI33HADP//8sweLFkII4W2WTb2OxL/yUAHH+MtpfvENepd0XupXEPEiLVu25IUXXgDg2WefpUuXLsyYMcM9/7333iMuLo5du3bRqlUrvcoUQghRh/y98Dmiv9gKwIHLGzP0nld1ruj81a8gYvYr3jOh17aroVu3bu7xv//+mxUrVhAQUPHOiMnJyRJEhBBCcHTLLxTO/IBgFfa2sTD05SV6l1Qj6lcQUZRqHR7Rk7//yTrz8vK48soref755yss16hRo0qfrygK2innpTgcjpotUgghRJ1QmJ3G1vvvo3E+HItU6Pv2IgxGo95l1Yj6FUS8VNeuXfnyyy9JSEjAZKrajyQyMpKjR4+6p3fv3k1BQYGnShRCCKET1eVi+dihJB7VyPeB5i/PJCAqQe+yakz9+taMlxo/fjzHjx/nxhtvZN26dSQnJ7NkyRJuv/12XC5Xpc/p378/r7/+Ohs3buSvv/5i3LhxmM3mWq5cCCGEpy2bMpLEvwtQFVAfvJr4nlfrXVKNkiBSB8TGxrJ69WpcLheDBw+mQ4cOPPTQQ4SEhGAwVP4jevnll4mLi+OSSy7hpptu4tFHH8XPr3rnqQghhKjbNnwyldivdgFwaHgCPW9/TueKap4cmtHBL7/8UqGtZcuWfPXVV2d8jqqq5OTkAMXhZcmS8icqZWVl1WSZQgghdHR44xKcL32OrwbJ7X0Y9tz3epfkEbJHRAghhKhj8jOPsOOhhwkshCPRCpe99UO9OTn1VBJEhBBCiDpEdblYcc8VxKZq5PpC61dfwT9cp4t11gIJIkIIIUQdkjT5ShK3FOFSwPjYv2jS9XK9S/IoCSJCCCFEHfHn/Mk0+S4FgCMjW9Htpqk6V+R5EkSEEEKIOmD/n4swzP4GgwbJnf0Y+PTpv8BQn0gQEUIIIXSWl7aPvY9Mxr8IDjdSGPDWj/X25NRTSRARQgghdKS6XKwcezUx6RrZ/nDBnDfxDY7Su6xaI0FECCGE0NGSRy6n+Q47TgP4TLqNRu376V1SrZIgIoQQQujkj7cn0vSnQwCkXn8BnW+YrHNFtU+CiA769evHQw89VGfXVxMSEhKYNWuW3mUIIUSdtfe3zzG/8SMGILlbAAOnL9S7JF1IEBFCCCFqWc7RPRyYNBU/GxxsYmDwW0l6l6QbCSKiWux2u94lCCGEV3M57Pw2diTRmXAiADq//g6WgBC9y9JNvQoimqZR4CjQ5aFp2jnVbLPZePTRR2ncuDH+/v706tWr3E3xMjMzufHGG2ncuDEBAQH06dOH//3vf2dc5w8//EBwcDCffPIJ/fv3Z8KECeXmp6enY7FYWL58+VnrS0hI4JlnnuG2224jKCiIsWPHAvDbb79xySWX4OvrS1xcHA888AD5+fmVrmPfvn0oisKmTZvcbVlZWSiKUukNAIUQoj5b8tBgmu12YDdC0FP3ENWmj94l6ape3X230FlIrwW9dNn22pvW4mf2q/bzJkyYwLZt2/j000+JjY3l66+/5vLLL2fz5s20bNmSoqIiunXrxqRJkwgICOCrr75i9OjRtGzZkp49e1ZY34IFCxg3bhwLFixg+PDhKIrChAkTePnll7FarQB8/PHHNG7cmP79+1epxpdeeokpU6YwdWrxFf6Sk5O5/PLLefbZZ3nvvfdIT09nwoQJTJgwgfnz51e7D4QQoqFY/fp9NFueCkDmjZ3pf/VD+hZUB9SrPSLe5sCBA8yfP58vvviCSy65hMTERB599FEuvvhi9wd648aNefTRR+ncuTPNmzdn7NixDBkyhM8//7zC+t544w3uu+8+vvvuO4YPHw7AyJEjAVi0aJF7uffff58xY8agKEqV6uzfvz+PPPIIiYmJJCYmMnPmTG6++WYeeughWrZsSZ8+fXjttdf48MMPKSoqOt9uEUKIemnPig/xfXsFAMkXBtP/yTPv3W4o6tUeEV+TL2tvWqvbtqtr8+bNuFwuWrVqVa7dZrMRHh4OgMvlYsaMGXz++eccPnwYu92OzWbD39+/3HMWLlxIWloaq1evpkePHu52Hx8fbr31Vt577z1uuOEGNmzYwJYtW/j222+rXGf37t3LTf/999/8888/fPLJJ+42TdNQVZWUlBTatm1b5XULIURDkHVoO0cmzyTSDgeaGhj85hK9S6oz6lUQURTlnA6P6CUvLw+j0cj69esxnnIp34CAAABefPFFZs+ezaxZs7jgggvQNI2nnnqqwkmjXbp0YcOGDbz33nt079693N6Ou+66i86dO3Po0CHmz59P//79iY+Pr3Kdp4aevLw87rnnHh544IEKyzZt2rRCm8FQvOOt7Hk0DoejytsXQghv5rQV8vvYG0jIguNB0G3uh1j8gvUuq86oV0HE23Tp0gWXy0VaWhqXXHJJpcusXr2aq6++mltuuQVVVcnKymL37t20a9eu3HKJiYm8/PLL9OvXD6PRyOuvv+6e16FDB7p3784777zDggULys07F127dmXbtm20aNGiSstHRkYCcPToUbp06QJQ7sRVIYSoz5Y8MJjme53YTBA2/X4iErvpXVKdIueI6KhVq1bcfPPN3HbbbXz11VekpKTw559/MnPmTH744QcAWrZsSVJSEmvWrGH79u08/PDDpKamnnZ9K1as4Msvv6xwgbO77rqL5557Dk3TuOaaa86r7kmTJrFmzRomTJjApk2b2L17N4sWLarw7ZxSvr6+XHjhhTz33HNs376dlStX8uSTT55XDUII4Q1WvXIXzVdmAJA9+kLaDr1P54rqHgkiOps/fz633XYbjzzyCK1bt2bEiBGsW7fOfYjjySefpGvXrgwZMoT+/fsTFRXF1Vdffdr1tW7dmp9//pn//e9/PPLII+72G2+8EZPJxI033oiPj8951dyxY0dWrlzJrl27uOSSS+jSpQtTpkwhNjb2tM957733cDqddOvWjYceeohnn332vGoQQoi6bseSdwiavxqAvReH0fcx+VZhZeTQjA7KXjvDbDYzffp0pk+fXumyYWFhfPPNNwCoqkpOTg5BQUHu8y5OXR9A27ZtK+w1ycjIoKioiDvvvLNate7bt6/S9h49erB06dIqP69t27asWbOmXNu5XntFCCHquuP7/iZjyiuEO2B/MyND3limd0l1lgSRes7hcJCZmcmTTz7JhRdeSNeuXfUuSQgh6jVHUR5/jruZ+GzICIFe8/6HyVr9b1Y2FHJopp5bvXo1jRo1Yt26dcybN6/cvF9//ZWAgIDTPoQQQlTfzw8OI36fiyIzRD37KKHxHfQuqU6rtT0izz33HJMnT+bBBx+Uu7LWon79+p32EEj37t3l2ytCCFGD7L/Opf2aLADy77qULgOrdzi8IaqVILJu3TreeustOnbsWBubE1Xk6+tb5a/gCiGEOLPtP75Jq5/2A7C3XyRXPPiWzhV5B48Hkby8PG6++Wbeeeeds35TwmazYbPZ3NM5OTlA8XkOp14Ay+FwuK/mqapqzRdeB5Xu2Sh93fWdqqpomobD4ahwwTdPKn2vyUXXPEv6uXZIP9eOjOS/yH1mHmFOSEk0MeDVHxt0n1fntSuah7+6MHr0aMLCwnj11Vfp168fnTt3Pu2hmWnTplX67ZEFCxbg51f+iqkmk4mYmBji4uKwWCyeKF3ozG63c/DgQY4dO4bT6dS7HCGEqJTqKMB33jPEH9JIC4Pj4x/EENBI77J0VVBQwE033UR2djZBQUFnXNaje0Q+/fRTNmzYwLp166q0/OTJk5k4caJ7Oicnh7i4OAYPHlzhhRQVFXHw4EECAgLO+7oY3kLTNHJzcwkMDKzyDeu8WVFREb6+vlx66aW1+jN2OBwkJSUxaNAgzGZzrW23oZF+rh3Sz5635I6LiD+kUWiBjBuvYug1tzX4vi49olEVHgsiBw8e5MEHHyQpKanKHyJWq9V9q/qyzGZzhR+qy+VCURQMBkO5a2rUZ6WHY0pfd31nMBhQFKXSn39t0Gu7DY30c+2QfvaM5U+PInFdLgAF9wzA1KSP9DVU6/V77NNs/fr1pKWl0bVrV0wmEyaTiZUrV/Laa69hMplwuVye2rQQQgjhcf989RKRn/0DQMqgWHqNfVXniryTx4LIgAED2Lx5M5s2bXI/unfvzs0338ymTZtq9eRDb/LLL7+gKApZWVl6lyKEEOI0Urf9Rv5/3sXsgpTWZoa88qPeJXktjx2aCQwMpH379uXa/P39CQ8Pr9AuhBBCeAtbbib/TLiHJvmQGg6XvP0NRrMFtQF/S+Z81P8TDYQQQogaorpcJI0dQpMjKvlWSHjpPwRGN9e7LK9Wq0Hkl19+8ehVVTVNQy0o0OVRnW9Bq6rKzJkzadasGb6+vnTq1ImFCxdWuuy0adPo3LlzubbZs2eTkJBwHj0lhBDiXPw87XoSN+ajKuC6fzgJvUfqXZLXq1c3vdMKC9nZtZsu2269YT3KKdc6OZ2ZM2fy8ccfM2/ePFq2bMmqVau45ZZbiIyM9HCVQgghztXGT58l+svtABwc2pTL73pR54rqh3oVRLyBzWZjxowZLFu2jN69ewPQvHlzfvvtN9566y3Gjh2rc4VCCCFOdeTv5dhf+AQfFfa2szL0xcV6l1Rv1Ksgovj60nrDet22XRV79uyhoKCAQYMGlWu32+106dLFE6UJIYQ4DwUnjrH9wfuJLYCjUQp93/kOg3zzs8bUryCiKFU+PKKXvLw8AH744QcaN25cbp7VaiU5Oblcm8FgqHD+SUO+f4EQQtQm1eXi53uGkXhMI88XWr76IgHhcXqXVa/UqyDiDdq1a4fVauXAgQP07du3wvxTg0hkZCTHjh0rF0Y2bdrk6TKFEEIAy564msR/CnEpwEMjiet2hd4l1TsSRGpZYGAgjz76KA8//DCqqnLxxReTnZ3N6tWrCQoKIj4+vtzy/fr1Iz09nRdeeIGRI0fyzTff8NNPP531JkJCCCHOz18fPUnsouJ/Dg9f1Zwho/+jc0X1k1xHRAfPPPMMTz31FDNnzqRt27Zcfvnl/PDDDzRr1qzCsm3btuXNN9/kjTfeoEuXLmzYsIFHHnlEh6qFEKLhOLj+B7RXvsSoQXIHXwbN+Fbvkuot2SOiA0VRePDBB3nwwQcrnX/qOSHjxo1j3LhxqKpKTk4OQUFBPPHEE7VRqhBCNDj5mUfY/fBjNCqEwzEK/d9eLCenepDsERFCCCFKqC4Xv9w9jEZpGjl+0G72a/iFxuhdVr0mQUQIIYQosfTfw2i+zYbTAJZ/30xsp4F6l1TvSRARQgghgLXvPkbcDwcAOHZtG7r860mdK2oYJIgIIYRo8Pb9/hXG177HACR38WfAtMrv/yVqngQRIYQQDVpu6l72PfYE/jY4FKsw6O0lcnJqLZIgIoQQosFyOez8OnYE0RmQ5Q8d5szDGhiud1kNigQRIYQQDdbSR4bSbKcDhxH8n7iTmAsu1bukBkeCiBBCiAZpzdwHaLr0CADpozrQceSjOlfUMEkQEUII0eDsWbUAn7lJxSen9gxkwJTP9S6pwZIg4mV27dpFnz598PHxoXPnznqXI4QQXif7yE6OTHoGXzsciDMweF6S3iU1aBJEvMxzzz2Hv78/O3fuZPny5bz//vuEhIToXZYQQngFl8POmrHXEXkCTgRC1zfnY/EL1rusBk3uNeNlUlJSuPLKKyvcpVcIIcTZ/fTAIJrvcWI3QfDU+4hs2VPvkhq8erVHRNM0HDaXLo9Tb1R3JgsXLqRDhw74+voSHh7OwIEDyc/PR1VVnn76aZo0aYLVaqVz58789NNP7ucZjUY2bdrEM888g6Io9OvXj9tvv53s7GwURUFRFKZNmwZAQkICzz77LLfddhsBAQHEx8fz7bffkp6eztVXX01AQAAdO3bkr7/+cq8/MzOTG2+8kcaNG+Pn50eHDh343//+556fnp5OTEwMM2bMcLetWbMGi8XC8uXLz+MnJ4QQnvfb7HtoviINgBO3dOeC4ffrXJGAerZHxGlXefvBlbpse+zsvpitZ78AztGjR7nxxht54YUXuOaaa8jNzeXXX39F0zRmz57Nyy+/zFtvvUWXLl147733uOqqq9i6dSstW7bk8OHDDBgwgGHDhvHYY4/h5+fH/PnzmTJlCjt37gQgICDAva1XX32VGTNm8NRTT/Hqq69y66230qdPH+644w5efPFFJk2axG233cbWrVtRFIWioiK6devGpEmTCAoK4ocffuDWW28lMTGRnj17EhkZyXvvvceIESMYPHgwrVu35tZbb2XChAkMGDDAY30rhBDna9ey+fj/dxUAyX1CGP74RzpXJErVqyDiDY4ePYrT6WTkyJHuwysdOnQA4KWXXmLSpEn861//AuD5559nxYoVzJo1izfeeIOYmBhMJhMBAQHExBTfDTI4OBhFUdzTZQ0bNox77rkHgClTpjB37lx69OjB9ddfD8CkSZPo3bs3qampxMTE0LhxYx599OTX1+6//36WLFnC559/Ts+ePd3rvPvuu7n55pvp3r07/v7+zJw500O9JYQQ5+/E/s2kPvkCEQ7YH29kyBtycmpdUq+CiMliYOzsvrptuyo6derEgAED6NChA0OGDGHw4MFcd911GI1Gjhw5wkUXXVRu+Ysuuoi///77nGrq2LGjezw6Oho4GXrKtqWlpRETE4PL5WLGjBl8/vnnHD58GLvdjs1mw8/Pr9x6X3rpJdq3b88XX3zB+vXrsVqt51SfEEJ4mtNWyNpxNxKfBZnB0GPuh5h9A876PFF76tU5IoqiYLYadXkoilKlGo1GI0lJSfz444+0a9eOOXPm0Lp1a1JSUmq8P8xmc7m+OV2bqqoAvPjii8yePZtJkyaxYsUKNm3axJAhQ7Db7eXWm5yczJEjR1BVlX379tV43UIIUVOWjB9IfIoLmxkinn6Y8OZd9S5JnKJeBRFvoSgKF110EdOnT2fjxo3ukz1jY2NZvXp1uWVXr15Nu3btTrsui8WCy+WqkbpWr17N1VdfzS233EKnTp1o3rw5u3btKreM3W7nlltuYdSoUTzzzDPcddddpKWl1cj2hRCiJq188Xaa/3YcgJzbL6LNkLE6VyQqU68OzXiDtWvXsnz5cgYPHkxUVBRr164lPT2dtm3b8thjjzF16lQSExPp3Lkz8+fPZ9OmTXzyySenXV9CQgJ5eXksX76cTp064efnV+FQSlW1bNmShQsXsmbNGkJDQ3nllVdITU0tF4SeeOIJsrOzee211wgICGDx4sXccccdfP/99+e0TSGE8ITtP75J8Ad/ALD30giumPhfnSsSpyNBpJYFBQWxatUqZs2aRU5ODvHx8bz88ssMHTqUIUOGkJ2dzSOPPEJaWhrt2rXj22+/pWXLlqddX58+fRg3bhyjRo0iMzOTqVOnur/CW11PPvkke/fuZciQIfj5+TF27FhGjBhBdnY2AL/88guzZs1ixYoVBAUFAfDRRx/RqVMn5s6dy7333ntO2xVCiJqUkbye41PnEOaEfc2NDJmzVO+SxBlIEKllbdu2LXdtkLIMBgNTp05l6tSpp33+r7/+6g4BpebOncvcuXPLtVV27sap1zpJSEgo1xYWFsY333xz2m3369cPh8NRYR2lQUUIIfRmL8hm/b230TQH0kPgwrc+w2T11bsscQZyjogQQoh6Y+l9Q2h6QKXQAo1mPE5o3AV6lyTOQoKIEEKIemHFjJtI/KN4D23h3f1o2X+0zhWJqpAgIoQQwuttWTSLsE82ApDSP5qL7p97lmeIukKCiBBCCK+WtvN3cp55C4sLUlqaGTJbTk71JhJEhBBCeC17Xhabxt9FaB6khcHFb3+F0WzRuyxRDRJEhBBCeK2l4wYTd0ilwAJxL0wnqFELvUsS1SRBRAghhFdaNu06Ev/KRQUc4y+n+cU36F2SOAcSRIQQQnidvxc+R/TnWwE4cHljLrznVZ0rEudKgogQQgivcnTLLxTO/ACTCnvbWBjy8hK9SxLnQYJIPZKQkMCsWbP0LkMIITymMDuNrfffR3A+HItQ6Pv2IgxGo95lifMgQUQH/fr146GHHtK7DCGE8Cqqy8Xye4bS+KhGvg80f/k/BEQl6F2WOE8SRIQQQniFZVNGkripAFUB9cGrie91jd4liRpQr4KIpmk4iop0eZx6Q7nTGTNmDCtXrmT27NkoioKiKCQnJ3PnnXfSrFkzfH19ad26NbNnz67wvGuuuYY5c+bQuHFjwsPDGT9+fIWb0BUUFHDHHXcQGBhI06ZNefvtt2usf4UQQi8bPplG7Fe7ADg0PIGetz+nc0WiptSru+86bTZeG32dLtt+4IOFmH18zrrc7Nmz2bVrF+3bt+fpp58GIDQ0lCZNmvDFF18QHh7OmjVrGDt2LI0aNeKGG05+He2XX34hPDyc5cuXs3fvXkaNGkXnzp25++673cu8/PLLPPPMM/zf//0fCxcu5N5776Vv3760bt265l+0EELUgsMbl+B86TN8NUhu78Ow577XuyRRg+rVHhFvEBwcjMViwc/Pj5iYGGJiYrBarUyfPp3u3bvTrFkzbr75Zm6//XY+//zzcs8NDQ3lxRdfpE2bNgwfPpwrrriC5cuXl1tm2LBh3HfffbRo0YJJkyYRERHBihUravMlCiFEjcnPPMKOhx4msBCORCtc9tYPcnJqPVOv9oiYrFYe+GChbts+H2+88QbvvfceBw4coLCwELvdTufOncst065dO4xlfgEbNWrE5s2byy3TsWNH97iiKMTExJCWlnZetQkhhB5Ul4sV91xBYqpGri+0fvUV/MNj9S5L1DCP7hGZO3cuHTt2JCgoiKCgIHr37s2PP/7ose0pioLZx0eXh6Io51z3p59+yqOPPsqdd97J0qVL2bRpE7fffjt2u73ccmazucLrVVW12ssIIYQ3SJp8JYlbinApYHr0Bpp0vVzvkoQHeHSPSJMmTXjuuedo2bIlmqbxwQcfcPXVV7Nx40YuuOACT266TrNYLLhcLvf06tWr6dOnD/fdd5+7LTk5WY/ShBCiTlj3wf/R5LsUAI6MaMngm6frXJHwFI8GkSuvvLLc9H/+8x/mzp3LH3/8UWkQsdls2Gw293ROTg4ADoejwrdDHA4HmqahqqrX/ccfHx/P2rVr2bt3LwEBAbRo0YIPP/yQH3/8kWbNmvHxxx+zbt06mjVr5n5tZb+VU/q6S9vKvv7SeWVV1uYNSl+jw+Eod0jK00rfa6e+50TNkn6uHd7Yzwf++g5e/RqDBsmdfBk09XOvqN8b+9pTqtMHtXaOiMvl4osvviA/P5/evXtXuszMmTOZPr1i6l26dCl+fn7l2kwmEzExMeTl5VU4hFHX3XPPPdx33320b9+ewsJC/vzzT4YPH86//vUvFEXh2muv5Y477mDZsmXlwpjT6QQgNzcXALvdjtPpdC+jqipFRUXuaSjud5vNVq7NW9jtdgoLC1m1apX7tdempKSkWt9mQyT9XDu8pZ9d+ZmEvPkijYrgUAw4Rj7MT0u86xLu3tLXnlRQUFDlZRWtqhfAOEebN2+md+/eFBUVERAQwIIFCxg2bFily1a2RyQuLo6MjAyCgoLKLVtUVMTBgwdJSEjApwpfm60PNE0jNzeXwMDA8zonxVsUFRWxb98+4uLiavVn7HA4SEpKYtCgQRXOuRE1R/q5dnhTP6suF8v+1YvmO+xk+0Pjd+cQc0FfvcuqMm/qa0/LyckhIiKC7OzsCp/fp/L4HpHWrVuzadMmsrOzWbhwIaNHj2blypW0a9euwrJWqxVrJd8+MZvNFX6oLpcLRVEwGAwYDA3jW8ilh1dKX3d9ZzAYik9AruTnXxv02m5DI/1cO7yhn398bBjNd9hxGsBn0m3EdR6od0nnxBv62tOq8/o9HkQsFgstWrQAoFu3bqxbt47Zs2fz1ltveXrTQgghvMQfb0+k6U+HAEi9/gIG3jBZ54pEban1f6tVVS13+EUIIUTDtve3zzG/8SMGILlbAAOn63M9KKEPj+4RmTx5MkOHDqVp06bk5uayYMECfvnlF5Z42YlHQgghPCPn6B4OTJpKtA0ONjEw+C050bOh8WgQSUtL47bbbuPo0aMEBwfTsWNHlixZwqBBg2psGx4+11boSH62QtRvLoed3+4ZSbNMOBEAnV9/B0tAiN5liVrm0SDy7rvvemzdpSfCFBQU4Ovr67HtCP2Ufv2roZ/0JUR9teThITTb5cBuhKCn7iGqTR+9SxI68Np7zRiNRkJCQtz3UfHz86v3X2lVVRW73U5RUVG9/taMpmkUFBSQlpZGSEhIrV7MTAhRO1a/Pp5my44BkHljZ/pf/ZC+BQndeG0QAYiJiQFoMDd10zSNwsJCfH19633oAggJCXH/jIUQ9ceeFR/i+/bPACRfGMzwJ/+nc0VCT14dRBRFoVGjRkRFRTWIS+o6HA5WrVrFpZdeWu8PV5jNZtkTIkQ9lHVoO0cmzyTSDgeaGhj8pnx5oaHz6iBSymg0NogPLaPRiNPpxMfHp94HESFE/eO0FfL72BtIyILjQdBt7odY/IL1LkvorP6eaCCEEKJOWfLAYBL2OrGZIGz6/UQkdtO7JFEHSBARQgjhcateuYvmKzMAyLqtF22H3qdzRaKukCAihBDCo3YsfYeg+asB2HtxGP3+/b6+BYk6RYKIEEIIjzm+728ynnoFqwP2NzMy5I1lepck6hgJIkIIITzCUZjHn+NuJjwbMkKg17z/YbLKBShFeRJEhBBCeMSS8YOJ3+eiyAxRzz5KaHwHvUsSdZAEESGEEDVuxfO3kbjmBAD5d11K64F36lyRqKskiAghhKhRW7+fQ9hH6wDY2y+Six98S+eKRF0mQUQIIUSNSd/9J1nT38TihH0tTAyZLVdOFWcmQUQIIUSNsBdks+G+2wnLhfRQ6D3vczk5VZyVBBEhhBA1Yum4wTQ9qFJogdjnniSkSVu9SxJeQIKIEEKI8/bzM6NI/DMHgKJ7B9Gi7806VyS8hQQRIYQQ5+Wfr18i4tN/AEgZFEufe1/TuSLhTSSICCGEOGep234j/9l3MbsgpbWZIa/8qHdJwstIEBFCCHFObLmZ/DPhHkLyITUcLnn7G4xmi95lCS8jQUQIIUS1qS4XSWOH0OSISr4VEl76D4HRzfUuS3ghCSJCCCGq7edp15O4MR8VcN0/nITeI/UuSXgpCSJCCCGqZeOnzxL95XYADg6Lo9ddL+pckfBmEkSEEEJU2ZF/lmN/4RNMKuxtZ2Xwi3Jyqjg/EkSEEEJUScGJY2x74H6CCuBolELfd77DYDTqXZbwchJEhBBCnJXqcvHzPcNofEwjzxdavvoiAeFxepcl6gEJIkIIIc5q2RMjSPynEJcCPDSSuG5X6F2SqCckiAghhDijvz56ithFewA4fFVzeoz+j84VifpEgogQQojTOrj+B7RXFmLUILmDL4NmfKt3SaKekSAihBCiUvmZR9j98GMEFMKRGIX+by+Wk1NFjZMgIoQQogLV5eKXu4fRKE0jxw/azn4Nv9AYvcsS9ZAEESGEEBUs/fcVNN9mw2kAy79vJrbTQL1LEvWUBBEhhBDl/Pnuv4lbvB+AY9e2ocu/ntS5IlGfSRARQgjhtu/3rzC89h0GDZK7+DNg2kK9SxL1nAQRIYQQAOSmprDvsSfwt8GhWIVBby+Rk1OFx0kQEUIIgcvp4NexI4jOgCx/6DBnHtbAcL3LEg2ABBEhhBAsnXg5zXbacRjB/4k7ibngUr1LEg2EBBEhhGjg1sx9gKZLjwCQfkMHOo58VOeKREMiQUQIIRqwPasWYJ2bhAFI7hHIgKmf612SaGAkiAghRAOVfWQnhx9/Bj87HGxiYPDcpXqXJBogCSJCCNEAuRx2Vo+9jqjjcCIQusydjyUgRO+yRAMkQUQIIRqgnx4YRLM9TuwmCJ56H5Ete+pdkmigJIgIIUQD89vse2i+Ig2AE7d054Lh9+tckWjIJIgIIUQDsmvZfPz/uwqA5D4h9Hv8I50rEg2dBBEhhGggTuzfTOqTL+DjgP3xRoa8kaR3SUJIEBFCiIbAaStk7bgbiciCzGDoMfdDzL4BepclhGeDyMyZM+nRoweBgYFERUUxYsQIdu7c6clNCiGEqMTyh4cSn+LCZoKIpx8mvHlXvUsSAvBwEFm5ciXjx4/njz/+ICkpCYfDweDBg8nPz/fkZoUQQpRhX/02ib8eByDn9j60GTJW54qEOMnkyZX/9NNP5abff/99oqKiWL9+PZdeWvE+BjabDZvN5p7OyckBwOFw4HA4PFmqVyjtA+kLz5J+rh3Sz7Vj249zabl4LwB7Lw1n8APzpM89RN7TJ1WnDxRN0zQP1lLOnj17aNmyJZs3b6Z9+/YV5k+bNo3p06dXaF+wYAF+fn61UaIQQtQbrqwUot94i7Ac2JtgwH73NAwmi95liQagoKCAm266iezsbIKCgs64bK0FEVVVueqqq8jKyuK3336rdJnK9ojExcWRkZFx1hfSEDgcDpKSkhg0aBBms1nvcuot6efaIf3sWfbCPH697mLiD6ikh0CLjz4mMqGj3mXVa/KePiknJ4eIiIgqBRGPHpopa/z48WzZsuW0IQTAarVitVortJvN5gb/Qy1L+qN2SD/XDulnz1gydhiJB1QKLZD+r2H0Tugo/VxL5D1NtV5/rQSRCRMm8P3337Nq1SqaNGlSG5sUQogGa8WMm0n8PQuA/Dv7YkqoeE6eEHWFR781o2kaEyZM4Ouvv+bnn3+mWbNmntycEEI0eFsWzSLskw0ApPSP5sL75uhckRBn5tE9IuPHj2fBggUsWrSIwMBAjh07BkBwcDC+vr6e3LQQQjQ4abv+IOeZtwh1QUpLM0NmL0XVuyghzsKje0Tmzp1LdnY2/fr1o1GjRu7HZ5995snNCiFEg2PPy2LTfXcSmgdpYXDx219hNMs3ZETd59E9IrX4zWAhhGjQlo4bTOIhlQILxL0wnaBGLfQuSYgqkXvNCCGEl1s27ToS/8pFBRzjL6f5xTfoXZIQVSZBRAghvNjfC58j+vOtABy4vDEX3vOqzhUJUT0SRIQQwksd3bKSwpkfYFJhbxsLQ15eondJQlSbBBEhhPBChdlpbL3/XoLz4ViEQt+3F2EwGvUuS4hqkyAihBBeRnW5WH7PUBof1cj3geYv/4eAqAS9yxLinEgQEUIIL7N8ykgSNxWgKqA+cBXxva7RuyQhzpkEESGE8CLrF0wn5utdABy8IoGedzyvc0VCnB8JIkII4SUOb1yC68VPMamQ3N6Hwc9/r3dJQpw3CSJCCOEF8o8fZcdDDxNYCEeiFS576wc5OVXUCxJEhBCijlNdLlaMHUZsqkauL7R+9RX8w2P1LkuIGiFBRAgh6rikyVeSuKUIlwKmR2+gSdfL9S5JiBojQUQIIeqwdR/8H02+SwHgyIiWdL15us4VCVGzJIgIIUQdtf/PRfDq1xg0SO7kx8Bnv9a7JCFqnAQRIYSog/IyD7L3kckEFMHhRgoD3v5RTk4V9ZIEESGEqGNUl4uVdw0nJl0j2x/azZ6Db3CU3mUJ4RESRIQQoo5Z8sjlNN9ux2kAn0m3EdtxgN4lCeExEkSEEKIO+eOdiTT96RAAqddfQOcbJutckRCeJUFECCHqiL2rv8D8+o8YgORuAQycvlDvkoTwOAkiQghRB+QcS+bAv6fgZ4ODTQwMfitJ75KEqBUSRIQQQmcuh53fxl5DdCZkBUDn19/BEhCid1lC1AoJIkIIobMlDw+h2S4HDiMEPDmWqDZ99C5JiFojQUQIIXS0+o3xNFt2DICMGzvRYcTDOlckRO2SICKEEDrZs+JDfN/6GYDkXsH0f/JTnSsSovZJEBFCCB1kHdrOkckz8bXDgaYGBs9dondJQuhCgogQQtQyp62Q38feQGQWHA+CbnM/xOIXrHdZQuhCgogQQtSyJQ8MJmGvE5sJwqbfT0RiN71LEkI3EkSEEKIWrXrlLpqvzAAg67ZetB16n84VCaEvCSJCCFFLdix9h6D5qwHYe3EY/f79vr4FCVEHSBARQohacHzf32Q89QpWB+xPMDJ4jpycKgRIEBFCCI9zFObx57ibCc+GjBDo9db/MPsG6F2WEHWCBBEhhPCwJeMHE7/PRZEZop59lND4DnqXJESdIUFECCE8aMXzt5G45gQA+XddSuuBd+pckRB1iwQRIYTwkK3fv07YR+sA2NsvkosffEvnioSoeySICCGEB6Tv/pOs6W9gccK+FiaGzJaTU4WojAQRIYSoYfaCbDbcdzthuZAeCr3nfY7J6qt3WULUSRJEhBCihi0dN5imB1UKLRD73JOENGmrd0lC1FkSRIQQogb9/MwoEv/MAaBo3EBa9L1Z54qEqNskiAghRA355+uXifj0HwBSBjaiz31zdK5IiLpPgogQQtSA1G2/kf+f/2J2QUprM0Ne/UnvkoTwChJEhBDiPNlyM/lnwj2E5EFqOFzy9jcYzRa9yxLCK0gQEUKI86CqKkn3DKHJEZV8KyS89B8Co5vrXZYQXkOCiBBCnIefp11H4oZ8VMB1/3ASeo/UuyQhvIoEESGEOEcbP/0P0Qu3A3BwWBy97npR54qE8D4SRIQQ4hwc+Wc59hc+xqTC3nZWBr/4o94lCeGVJIgIIUQ1FWanse2B+wkqgKORCn3f+Q6D0ah3WUJ4JY8GkVWrVnHllVcSGxuLoih88803ntycEEJ4nOpysXzs5TQ+ppHnCy1eeZ6A8Di9yxLCa3k0iOTn59OpUyfeeOMNT25GCCFqzbInRpD4dyEuBXhoJE17XKl3SUJ4NZMnVz506FCGDh1a5eVtNhs2m809nZNTfJlkh8OBw+Go8fq8TWkfSF94lvRz7fDGft64YBqxi/YAcOiq5gy4aVqdr1/PftY0DdXlxOV0ojrLD12u4nFNVdFUFVVTT46XDDVNQ1Nd5dvc7ae0qSqapqKpGhoaaO4q0DQNNK24SSuZPmW8XFvJsPi5pW3F69RK2k8+/+Q2XC4XGSkp/JqVhsFQvf/z3duvOKPy5jLz1ZKXq5WOl7wuDVBLatRKhqr79YFW8vpCGzWm71U1G6ir835TtNO++pqlKApff/01I0aMOO0y06ZNY/r06RXaFyxYgJ+fnwerE0KIM3Me2UTc258SWAg725nhlmkoXn5eiKZpqA47qt3uHrocNve05nKiOl1oLiea04nqdJS0OSvMKw0MmqtkqLpO+yEq6paMwCZceGXVdxpURUFBATfddBPZ2dkEBQWdcdk6FUQq2yMSFxdHRkbGWV9IQ+BwOEhKSmLQoEGYzWa9y6m3pJ9rhzf1c/6JI2y4biiN0jSOxCj0+GIpviHRepdVKVtBAXnHM8jNzKAwJ5u8E8fZvvkfosNCKcrLpSA7i6LcXGyF+dgLC2s3LCgKRpMJg9FUMjSiGIwYDAYUg4KiGFAMxQ9DyRDFAIqCqhiK/6NXFFRKHlrx0IWCSwOXpuBUNVwaxcOScZdavKfAqYFaMl3afnL/hnLKsHgcQFPKt2sl7aCUzDvd/Eo7oULLaX8ClazmzOuuuAJFKf78NUDJOChl2gGiGjfm6UfHVGO9Z5eTk0NERESVgohHD81Ul9VqxWq1Vmg3m811/g9VbZL+qB3Sz7Wjrvez6nKxetzVNE/TyPGDtrNfIyiyiW71OO12so4d4fjRw5w4cpicjDRyMzPIzUgnNzMDW0F+pc/LOsM6TWYLVn9/LH7++Pj5F4/7+mG2+mCyWjFbrZgsxUOz1VrcZrFisvqUDK2YzGYMpuKAYTSZSsbNKAYj+U6NrCKVXLtKbpGTnCIHOYWlQ0cl0053e5FDPVnoqZ/BSiXj57GTymxU8DEZsZoNWMsOTQasJgMWkwGTQcFsNGAxGjAZi8fNJUMDGgf276N1y0SsZlO5eSajAbOhdFwpeX7JOgwGDAYwKgpGg4LBoGAyKBhKpo1lxk0l842KUu45py5XPL86oaVmVed3uk4FESGEqGuW/vsKmm+z4TSA+bGbiO00sFa267TbyTi4n7R9yWQeOsiJI4c4fvQwOWlpaJp6xuf6+AcQEB6Bf0goPgGBpB4/QbtOnQkMDcMvOATfwCCs/gFY/fyw+vljslT9vjiqqnG8wE5ajo3DeTZO5Ns5nm7neL6d4wWFxdMljxMFdk4UOHCp57/XxWRQ8Lea8LcY8bUY8bea8LMY8beY8Ctp97MUt/lZS9pLli0NFz7m0lBhxKdMyPAxG7GYDBjP84Pb4XCwePFehg1sWafDdV0jQUQIIU7jz3f/Tdzi/QAcG9maQTc+5ZHtaKpKxqEDHN6xjWN7dpKWkkzm4YOoLlely1v9/AmNbUxYo8YERcUQGB5BUEQkgeGRBEZEYPHxdS9b/OG4mO7Dhp3xw1FVNTLybBzNLiIt10ZabhFpOTbScm2k55a05djIyLPhPIdgEWA1EexrJsjXTJCPiSBfM4E+JoJ8yrcVTxe3B/uaCbCa8LeasJjkslf1lUeDSF5eHnv27HFPp6SksGnTJsLCwmjatKknNy2EEOdl3+9fYXjtOwwaJHfxY9j0L2ts3ZqqkpqSzP5/NnJox1aO7tpR6SEVn8AgopslEhHXlLDYuOLwEdsEv+AQ9/H9qrI5VQ5n53M4q7D4caJ4eKRk+mhWEXbXmfe0lFIUCPe3EBFgJTzAQqifhXB/C6H+FsL8K06H+Jmxmrz7xF7hOR4NIn/99ReXXXaZe3rixIkAjB49mvfff9+TmxZCiHOWm5rCvseeINoGh2IVBr299LyvnFqQk03Kxr/Y9/cG9v+zkcLcnHLzzVYfGrVsTWzrtkQ3a0FUs0QCwyOqFTiyCxykZOazPzOflIx89mcWkJKRx95jRh78fdlZn29QICrQh6ggK1GBViIDfYgKtJZM+xBdMgwPsGA2yh4KUTM8GkT69et3+u9GCyFEHeRyOvh17AiaZUCWP3SYMw9rYPg5rasgO4vdf65h1x+rObhtM5p6co+DxdeXuAs60bR9Rxq3bkdkfLMqhR27U2VfZj67U/PYnZbLvox89mUWsC8zn6yC0127oTjM+JgNNA7xpXGoH41DfErGfYkNLh7GBPlgkoAhapmcIyKEEGUsnXg5zXbacRjB/4k7ibng0mo932G3sXvtGrasSOLQti3lTiyNTGhO8y7dSejYlUat2mA0nf5PcJHDxd70fHan5bInLe9k8MgsOOPJn1GBVhIi/EkI9yMhwp8mwVYObNvA9VcMJCrYr9qHdITwNAkiQghRYs3cB2m69AgA6Td0YMDIR6v83LR9e9n88xK2//pLufM9YhJb0rLXRbS68GJComMqfW5Gno1tR3LYdjTHPdybnsfp8kaA1USLqABaRAXQPNKfZuH+xIf7Ex/uh7+1/J91h8PB4gMQ5m+RECLqJAkiQggB7Pn1f1jnLsUAJPcIZPjUz8/6HE1V2bN+Leu+/ZKju3a424Mio2h/2SDaXdKf4KiTFz7TNI1DJwr5+1BWueCRlmurbPUE+5ppFR1Ai6hAWkYF0DK6OHzEBPlIqBD1hgQRIUSDl31kJ4cnPU2UHQ42MTB47tIzLu+029n26wr++v5rThw5BIDBaKJFz9506D+Y+PadUAwGjufbWbEzjb8PZhU/DmVzPN9eYX2KAs3C/WkbG0S7RkG0KxlGBVolcIh6T4KIEKJBcznsrB57Hc2Ow4lA6Pzmu1gCQipf1ungn+VLWPvVZ+RnnQDA6u9Pp0HD6DB4OPsKjCzff4JNn/3N3wezOHC8oMI6zEaFto2CuCA22B042sQEVjikIkRDIe98IUSD9tMDg2i+x4ndBMFT7iWq1YUVltFUlZ1//MbqTz8iK/UoAP5h4YT3GsSByI68ebiATbPWUeioeAGy5hH+dIoLoVOTYDo3DaVto0C5poYQZUgQEUI0WL/NvofmK9IAOHFLd/pd+UCFZQ5u/YeVH79H6t7iizOqPgHsbtyHn9UEnDuMsOOge9lgXzPd40Pp0jSETnEhdGwcQrCfXOpbiDORICKEaJB2LZuP/39XAZDcJ4Thj39Ubv6x1Ax++O9bZP3zOwB2xcyG4M5sCu6EQy0OF01CfemREEb3hFB6JITRIjJA1xuNCeGNJIgIIRqcE/u3kPrkC0Q4YH+8kSFvJJFvc7Ju33F+T04nZc0vxCf/jI9qQwM2B17An6HdiW8czY3NwuieEEaPhFAaBfuedVtCiDOTICKEaFCctkLWjvsX8VmQGQz7757BzR9uZsP+E/jbshiQvoLWtuLzQLL9IlEuuo5RvbrySrMwwgOs+hYvRD0kQUQI0WBk5NlYOaE/7VJc2Ezw9oWXs+ovI2iZtMnbSb/jv2FWHShmC11G3Ejfa0ae9z1mhBBnJkFECFFvOVwqGw9ksXJXGqt2ZdBj8/Nc/0cWAIv6tGB98BAGN/Wn476l2DI2AtCkXXuG3jeRoMgoHSsXouGQICKEqFeO59tZsSONZdtT+W13Brk2JwB9C5O4ak3xN1/+6RnM4Envc1vBYZLmvkpuZjoGo5E+N9xCj6tGYjDIXhAhaosEESGE19ubnsey7aks25bGX/uPl7tHS6ifmWFRGQx9bwlWJ+xrbmTk2z+z+eckvv7oXTRVJbRRLMMmPEpMi1b6vQghGigJIkIIr+NSNTYeOEHStlSStqeyNz2/3Px2jYIY2C6a/m2iaB2msOqaPoTnQHoIdH/9Y5a/9xZbVy4HoO0llzHwrvuw+Mg3YITQgwQRIYRXyLc5+XV3Bsu2p/LzjrRy92wxGxUubB7OwLbRDGgbRZNQP/e872/vTeIBlUILhDzxIEvf/ZBje3ahKAb63nonXYddJfdzEUJHEkSEEHVWak5RySGXVFYnZ2J3qu55QT4m+reJYmC7aC5tFUmQT8UrmK6YcTOJv2cBkP6v3qR8v5r8rBP4+Acw/KHHie/YuZZeiRDidCSICCHqDE3T2HEsl2Ulh1z+OZRdbn5cmC+D2sYwsF0UPRLCMBsNp13XlkWzCPtkAwCbL47l2I4cHLYiIuLiufqxpwiJjvHoaxFCVI0EESGEruxOlbX7ig+5JG1L5XBWYbn5XZqGMLBtNIPaRdMyKqBKh1HSdv1BzjNvEeqCTW2COVYQgOoqIr5jF66aOBmLr99Z1yGEqB0SRIQQtS670MHybUf5aJeBJzb8Ql7JV2wBrCYDl7SMYGDbaPq3jSIq0Kda67bnZbHpvjuJy4N/4oM5Yo0Al4s2F/Xl8vsewmiSm9AJUZdIEBFC1IqDxwtI2pbKsu2p/JlyHKeqAQbASUSAhQFtohnYLpqLW0Tgazn363gsHTeY5odUNjcJ41BIKADdrriavrfciWI4/aEcIYQ+JIgIITxCVTX+PpTlvr7HztTccvNbRPqTYM5l7PAL6Z4QUSN3rV027Xqa/5XLtthwDoaHAHDpzbfT/cqR8s0YIeooCSJCiBpTaHexek8Gy3eksmx7Gum5Nvc8o0GhR0IoA9tGM7BtNI2DLSxevJgucSE1EkL+XvgcUZ9vYXtsOPsjQwAYNHYCHQdcft7rFkJ4jgQRIcR5OZZdxPIdqSzfnsbqPRnYynzFNsBqom+rSAa1i6Zf60hC/CzueQ6Ho8ZqOLplJQUzP+BwTNkQcj8dBwypsW0IITxDgogQolpUVWPLkWyWbU9j+fZUth7JKTe/cYgv/dtEMahdNL2ah2E1efa+LYU56Wy5/16yg0tCiKIweOz9dOg/2KPbFULUDAkiQoizKrA7Wb0nk+UlVzVNK3PIRVGgc1zxV2z7t4miTUxgrZ2PobpcLLv7cuxKmHtPyOB77qfDZRJChPAWEkSEEJU6dKKAX3ams3x7KmuSM8sdcvG3GLm0VST920RxWZsoIgKsutS4fMpIOGphX6MQAAaPe0BCiBBeRoKIEAIoPtH0j5RMVu1KZ+Wu9Ao3kmsS6uu+l0vPZp4/5HI26xdMx7HyGLsaRwJw2ei7JYQI4YUkiAjRQGmaxu60PHfwWJtyvNy9XIwGhS5xIfRvG8XAtlW/qmltOLxxCUfnfc/O2CgAel1zA12HXa1zVUKIcyFBRIgGJLvAwW97Mli1K51Vu9M5ml1Ubn7jEF8ubRVB31aR9E6MINi37l2FNP/4Uf789/+REhMDisIFl/bjolG36l2WEOIcSRARoh7LszlZt+84vydnsiY5g61HctC0k/OtJgMXNg/n0laR9G0VQWJk3dnrURnV5eLHsVdzKDgaTVFo2rYlQ+6dWKdrFkKcmQQRIeqRIoeLDQdOlASPTP4+mFVyKfWTWkQF0LdVJH1bRdKzWRg+Zn3P9aiOHx67kqNqGC6zgdDIIEY++YJctl0ILydBRAgvllvkYMOBLP7ad5x1+46z4UBWufM8AOLCfOnTPII+LcLp3TycqKDq3USurljz339zONmJ3ceCjwVuefG/cgM7IeoBCSJCeJHUnCLW7TvOupTjrNt3gh3HcjhlhwdRgVYuahFB78Ti4BEX5v23vE/5/Wu2LVpPvp8vJlzc8vL7WHy9/3UJISSICFFnFTlcbD+aw98Hs9h0MIv1B05w8HhhheWahvnRPSGUHglh9EgIIzHSv16dM5GbcYBfZswi2y8Qg6oyctqzBEdF612WEKKGSBARog5QVY29GXlsOpjN3wez+PtQFtuP5uBwld/dYVCgXWwQ3eOLQ0f3hFCivfRQS1WoLhdf3XMzx/2CQdO49KZribugm95lCSFqkAQRIWqZw6WyJy2PbUdy2HY0h21HcthyOJtcm7PCsuH+FjrHhdApLoTOcSF0jQ8lwNpwfm2/GD+MDFMwAG26taDbNXfqXJEQoqY1nL9oQuggu9DBjqMnA8e2oznsTs3D7lIrLOtrNtKhcTCd4oLpFBdCpyYhNAn1rVeHWapj+avjOZphBCNEhBq5YtJsvUsSQniABBEhasCJfDu70/LYnZbL7tSTw7I3hysr0MdE20ZBtGsURLvYINrHBtMqOgCTUb6KCrB92UfsWLUHl8WMn2Lj1jd+0rskIYSHSBARooqKHC4OnShgX0YB+zLzScnIJzk9jz1peWTk2U/7vMYhvrSLPRk62jUKatB7Os4m6/Aufp3zPkU+vlhcDm5+fT4Go/dc60QIUT0SRIQooWka2YUO9qXn8s9xhaOr93HwRBH7MvPZl1HAkezCclclPVWTUF9aRgXQMjqQFlEBtIwKoEVUAIE+cq2LqnI57Hz1wN3k+hR/Q2bIA/cRFNNE77KEEB4kQUQ0GC5VIy23iMMnCjmcVfIoGT9SMp5vd5UsbYSduyqsI8BqIj7cj4QIfxLC/UiMDKBlVCCJUf74WeTX6Xx9Ou4KTpgCAejYtzOtLpUb2QlR38lfTuH1nC6VjDw7ablFpOXYSMu1FY/n2kjLsZFeMp6ea6twufPKhPtb8MNGx2aNaBYZ4A4dCRH+hPtb5JCKh/w44y5ScyxggEbRFgZMmKF3SUKIWiBBRNQ5NqeLE/kOjufbOVFgJzPfzol8e6XTGXl2MvNtZzxkUpbJoBAT7EPjEF8ah/oWD0vGY0vGjagsXryYYcM6YjbLYZXasPn7t9i9/hCayUSgwcaNs7/TuyQhRC2RICJqlKZpFDpc5Ntc5Nuc5BQ5yCksHTrKTecWOSu05RQ6yhweqTqjQSEiwEJUoA9RgVaigqzF46XDMm1Gw5n3aDgcFb9aKzwnM2Uzv727EIfFio/Txs3//Vj2OgnhAapLxV7kwl7oxF7kxF7oxFbowsfPRKMWIbrVVStB5I033uDFF1/k2LFjdOrUiTlz5tCzZ8/a2LQoQ1U17C6VIocLm1PF5lApcrqwOVRsThdFJUObs3iZIodKgd1Jgd1Fvt1Jge3ksMDhosDmJN/uosDuJN/motDupMDhqvLeiTMxGhRC/SyE+ZsJ87cQ5m8pmbaUmw4vCR9h/pazBgxR97gcNr57/BEKLP6YXC6umPw4/qFy+XbR8GiahurUcDpVXA4Vp92Fy6nidBRPuxzF406HC6fNhcPmwl4ydD+KiodOe8U2h614fZWJbx/O8AkhtfuCy/B4EPnss8+YOHEi8+bNo1evXsyaNYshQ4awc+dOoqKiPL35alNVDZem4VI11NKhSoW2cvM1DZeKu73i809tK17Wqao4XCoOl4bDpeIsGTpcGk5XyTxVw+FUcaoaRQ4nKfsMrFi4GafGyeXdyxQ/1+4sGyxKwoVDrfQiWp4UYDUR5GMiyNdMkI+ZIF9TydBMoI+pQluQT3F7qJ+FIF+T/Fdczx3P3EPmF6+RYw5B0TS6X3ERCd0H6F2WqMM0TUPTQFM1tJK/rWjFf7c1TUNTcbdrZdu00jbc7e5pTXOvT1PB5VJRXVrJo/y4y1kyrha3u5xl52s4HU6O7/Ph54ydxXWVznOquFwnw4TLoZYLGaXD2mI0G7D4mrD6mrD4GAmO8q21bVdG0bSa+P/19Hr16kWPHj14/fXXAVBVlbi4OO6//34ef/zxcsvabDZstpMXgMrJySEuLo6MjAyCgoJqrKY1K/5g108b0ACK38fu8fPjqQ9Oz6xXURQMJUNFKb6PiXuck20GRSleVlGKpw1KSRsn28oMS9dRlao1D72283lXaxrk5+XhHxBAxSxU9Xo99Zt12j7TNDRALXkjF4+DVjJd8jfb3aK5h8XjpefxVjavwniZ35tT56MV11D68isso4GmqRhTDmCzbQMFAkM6Et/jxqr3QXX6thrLatVb2COq974p/mAuraW4b0/+IEs/cDMyMggPDy8O99rJeSf/7pV8wGvlp931aGWnS95Favnl3OssO13yfnCvw91eZp2nLFc2PLgDQmn48OinVd1iNBswmQ0YTYaT4yXTZqsBk9WI+ZSHyWLEbDWctq30OUaT5y+cmJOTQ0REBNnZ2Wf9/PboHhG73c769euZPHmyu81gMDBw4EB+//33CsvPnDmT6dOnV2hfunQpfn41d8vvnJ0HsRd0qLH1iXrqhN4F1C8KJ2Oc6srEZv8ZFDBaOuJQBrLnr3Q9y6vnTBzJyNa7iNqhlKQVheJ/JEoeCtrJ8TJvxuJxrVy7YihuKx4WTyuKBoaSZQxaybB4WjGUeb6h/LRiOLm8YiyZZwDFWLx+xaCVtJ8cd6/rDJwlD/f9uFWgqORRBxQUFFR5WY8GkYyMDFwuF9HR5Y/5RkdHs2PHjgrLT548mYkTJ7qnS/eIDB48uEb3iKQ128aun/4qeXOe/ONY/F/8yX/lldK2knfEmd4XJ/8T1Ur+8zz532n5/wg193+pmvs/x1P+4zylvXRc1TTy8vPx8/dDUyiz3OnWW367GqBqFZevsA7tDLVX8vxT11v63PKvrXTdoKGW2fbJ51SNJ5bk5B+vml5vtZb27GszFL+7UVDc4walbBvlx0v3mKGUG3e3UbIXjbLzlDLzSoYU/x6ZHBCdXEDo/jz2Bh0DoxOrNZweN4zGZLYWl1qdHWTVWLZa+908tLCnjjQqpX+oKPP3qsyHrMvlYuu2rbRv3x6TyVSultJl3bWVvh/cf/xKVlXmb2PpPHd7yUj5D/ey00qZ7ZU8t+z23es5+RyDoqAYlJIP55L3X/GbDUNpu3sZBUOZaT05HA6SkpIYNGhQg//GXU5OTpWXrVPfmrFarVit1grtZrO5Rn+oxxsZeL/DKlyaC1VTyw/V4uGpbU7NWW5ZVVNxqS73ssKzjIoRg2IoNzQaKrYZFAMmgwmDYqh0+QrrqaRNQeHYkWM0bdIUk9FU6bpLpytrO9O6S9vOZdnqvo5T+0cvmstF9jffkPbKqzgzM1nXrBF2ox8BwaFEDBhK1yHNG/wfbU9yOBzsy/6btr1jpZ9rSU1/Znmj6rx+jwaRiIgIjEYjqamp5dpTU1OJiYnx5KbPKNeey4a0DbW2PQXltB+cZ/swLduuoJB9IpuIiAhMBtOZP4RO+cA627qrWke1P3ir88F5mmUNiqFWT1x1OBzF1xHpPazB/zE5H5qmkZuURPprr2HfkwzA7tbNyPAxYLJYuPLfT7Fu63adqxRC6M2jQcRisdCtWzeWL1/OiBEjgOKTVZcvX86ECRM8uekzahXWilf6vVLlAHC6D86q/uddUx+i7g/IAfIBKeouTdPI/2016bNnU7RlCwCGoCCyrxrKns1/AXD5fQ8TGd8MJIgI0eB5/NDMxIkTGT16NN27d6dnz57MmjWL/Px8br/9dk9v+rQifCMYFD9It+0LUR9pmkb+qlVkvP0OhevXA6D4+RE2+jYc/S7hxxefAaDXNaNo3fsSHA6HnuUKIeoIjweRUaNGkZ6ezpQpUzh27BidO3fmp59+qnACqxDCO2lOJzk/LSHznXew7dwJgGKxEHrjjYSPvZsig8Inkx/C5XCQ2L0XF91ws84VCyHqklo5WXXChAm6HooRQtQ8V14e2V99zfGPPsJx8CBQvAckdNQowsaMxhwdjcNuY9G0x8k7cZzwJk0ZOv4RFIN+J84KIeqeOvWtGSFE3Wfbu5cTH39C9jffoJZcK8AYEkLobbcSdtNNGENCANBUlZ/enMWx5N34BARy9WNPYq3B6wEJIeoHCSJCiLNSbTbyli8na+FC8tecvBihJTGR0JtvImTECAynhIw1C//Hrt9/xWA0cdUj/0doTGxtly2E8AISRIQQp1W0YwdZC78k+7vvULNLrsypKARcdhlht9yMX+/elX4rbPvqlfzx5f8AGHj3fcS1kysZCyEqJ0FECFGO/dAhcn5YTM4PP2DbtcvdboqJIWTkNQSPHImlSZPTPv/o7p0smTsLgO5XjqTDZYM9XbIQwotJEBFCYD90mLyfl5Pzw2IK//775AyzmcD+/Qm57lr8+/RBMRrPuJ6cjDS+efEZ9zdkLrlptIcrF0J4OwkiQjRAmqZRtHUbeT8vJ3f5z+6v3QKgKPj16kXQFcMIGjwYY3BwldZpLyrkmxeeoSA7i8imCQy7/1EMhjMHFyGEkCAiRAOh5udTsH49eb/8Qu7PK3AeO3ZypsGAX9euBA4eRODll2OOiqreulUXi+e8RPr+FPyCQxgxaQoWH98afgVCiPpIgogQ9ZTmcFC4eTP5a34n/4/fKdz0Nzid7vmKnx8BF11EwID+BPTtiyk09Ny2o2mseP8dkv9ai9Fs5upHnyQoonpBRgjRcEkQEaKeUG02irZupXDjRgr+XEfBunXu63yUMjdpgn+fPgQO6I/fhRdiqORu19X156KFbFryPQBDx08ktlWb816nEKLhkCAihJdypqdTsHEjhRs3UbhhA4XbtsEp928xhoTg1/tC/Hv3xr93byxxcTVaw9aVy/ntfx8A0O+2u2nd+5IaXb8Qov6TICJEHadpGs7UVIq2baNo67bi4bZtOFNTKyxrjIjAr0tnfLt0xb/3hVhbt/bYJdVTNq1n6VuvAcVf0+12xdUe2Y4Qon6TICJEHeLKycGWnIw9ORnbnmRse/ZQtG0bruPHKy6sKFhbtcK3S2f8unTBt2tXzE2aVHqBsZqWuncP370yE9Xlou3F/bj0pjEe36YQon6SICJELdNUFWd6Oo4DB7Al7y0JHnuw7UnGmZZW+ZOMRqyJifi0a1f8uKAd1tZtMAb4127xQMbB/Xw5YwoOWxFNO3RmyL0Pyo3shBDnTIKIEB6gFhXhOHQI+8GDOA4exH7wUMnwII5Dh9BsttM+1xQTgzUxEWuLRCzNE/Fp2wZrq1YYfHxq8RVU7sTRwyx89kkKc3OIbt6Sqyb+H0aTWe+yhBBeTIKIENWkFhTgSE3FmZqK49gxnMdScaQew3n0WHH7sWO4Tpw480qMRsyNGmFp3gxrYgusLRKxJiZiSUzEGBhYOy+kmrLTUvnimSfJzzpBZNMErn3iabmbrhDivEkQEQ2epqq4srNxHT+OMzMTW2oaIWvWkJm8Fy0rC2dmJq7MTPfw1K/Eno4hIABz0zgsTeKwNI3D3CQOc1wTLE2bYo6JQTF7z56E3OMZfPHsE+RmphMW24TrnnwW34C6GZiEEN5FgojwepqmoRUWoubno+bn48orHqq5OcUBI7tkmJONWm66eKjm5ICmlVtnFHCmfRqKnx/mmBjMMTGYYmIwx0Rjii4ZxsRgjo7GEBxcKyeOelp+1gm+eOZJslOPERLdiOuf+g9+wSF6lyWEqCckiAiP0zQNzeFAKypCLSo6ObTZ3OPF7TbUokK0IhuarQi1sKh4WFASMgpOCRplHqjqeddpCArCFBaGISyMdLudJhdcgDkyElNEOMawMEwREZjCwjBGRGDw968XIeNsctLT+OLZJ8g6dpTA8Eiuf+o/BISF612WEKIekSBSx2iaVvyhqqrF4y4XmssFLheuoiKMOTk4jx1DUwzgcqK51JLhyeU0pwvUMkOXC83pLF6n0+V+nuZyllu/5nAWBwaHHc3uKBk/5WG3V95e2XybDbUkbNREUDgrRcHg7+9+GAMDMYQEYwwKxhgcjDEoCGNIMIagoOK2kJK24GAMwcEYLBYAHA4H/yxeTJdhwzB70eGTmnbi6GG+eOZJcjPTCYqM5vqn/kNQpFy6XQhRsxpkECn85x/SXnwJTVNBo/wHf0kQqDivOsuenIeqonGGeWWnTzk8UJlEYN9/ZniyezzLYMDg44Pi44PiY8Xg41s8tPpg8PVBKTMsnW/w9SkXMAz+AWXG/YpDR0AAiq9vg9hLURvSD+xj4bNPUpCdRWhsE65/8lkCwyP0LksIUQ81yCDiysmlYN06vcs4J5qioJhMxQ+DAdxDI4qxTJvRCEZDcZvRCEZj8dBkRDEYUUxGMJS2FT9PMZuLHxZzmXHLyfGSB2XGDRZLuemTDwuKxYzB1xfFasXg41P89VOzWcJCHXcseTdfzphCUV4ukfHNuO6JZ+ScECGExzTIIOLTuhWNX30FFAUUAxiU4g9wRQGlZNxgAJQy8wwl85TieYoBFE4uW2FaKf7ALZmnGIrXXfl0me0bDGWep5QLEQ5V5ccff2RYAz9kIDwnef1afpj9Ig5bEY1atGbk5On4BAToXZYQoh5rkEHEFBlJ0NChepdRbcopNzQToiZt+PFbfvngv2iaStMOnbn6kf/D4ivXCRFCeFaDDCJCiJNU1cUvH/yXjT99B0CH/oMZcOd9GE3y50EI4Xnyl0aIBsxWUMDiOS+yd0PxOVOX3DSGHlddK+fxCCFqjQQRIRqo9P0pfPfqTE4cPYLJbGHohIm0uvBivcsSQjQwEkSEaIA2r1jKz+/Ow+mwExAewVUTJ9OoRWu9yxJCNEASRIRoQBy2Ipa/O4+tK5cBkNC5G0PHT8QvKFjnyoQQDZUEESEaiGN7dvHT3FlkHjqAohjoc8PN9BpxffFXx4UQQicSRISo55wOB78vXMC6b79EU1X8gkO44oHHaNq+k96lCSGEBBEh6rNjybv56c1XyTx0AIDWfS6l/+33yKEYIUSdIUFEiHqoKC+P3xcuYOOS7917QQbeeR8te/XRuzQhhChHgogQ9Yiquti8fCmrP/uIwtwcAFr3voT+d4yTvSBCiDpJgogQ9cSBLf/wy4fvkL4/BYDwJk3pN/puEjp20bkyIYQ4PQkiQni5g1v/Yc3CBRzatgUAq78/fa6/hU6Dhspl2oUQdZ78lRLCC2maxsGtm/n9y5MBxGgy0WHAEHpfd5MchhFCeA0JIkJ4Eafdzs7ff2XjT9+RuncPUBxA2vcfQs+rryMoIlLnCoUQonokiAjhBXIy0vg76Uc2L1/iPgnVaDbToSSABIZH6FyhEEKcGwkiQtRRtoJ8dq1dzfZff+Hgts2gaQAERkTSefAVtL9skByCEUJ4PQkiQtQhtoJ8UjatZ/faNSSvX4vL4XDPa9q+I52HDCexWy8MRqOOVQohRM2RICKEjjRNIzv1GHs3riP5r7Uc2r4F1eVyzw9rHEe7Sy6j7cX9CIqM0rFSIYTwDAkiQtSynIw0Dm7dzMGt/3Bg6z/kZqSXmx8W24Tm3XrS5qK+RCU0R1EUnSoVQgjPkyAihAfZCvJJ3ZvM0T07ObZnF8eSd5F3PLPcMgajkdjWbUns1ovEbj0JbdRYp2qFEKL2SRARogbYiwrJPHSAzIMHyDh0wD2em5leYVnFYCAmsSVxF3Qkrl0HGrduh9nHR4eqhRBCfxJEhKgCl9NB3vFMstPSyE4/Rk56GtlpqeSkp5KdllphL0dZgRGRNGrRmkYtWhHTohXRzVpI8BBCiBIeCyL/+c9/+OGHH9i0aRMWi4WsrCxPbUqIatE0DafdRlF+HkV5edjy8krGcynKyyX3eCbHtm3l67/XUpCdRX7WCYrycs+6Xr/gECLimhLeJJ7wJk2JiCse+gQE1MKrEkII7+SxIGK327n++uvp3bs37777rqc2I+oJTdNQXU5cDgcupxOX04nqdOJ0OFCdJ9tcJeNqybjTbsdhK8JRZMNRVFg8brOVtBWVG9qLirCVBA6X03nWmvJOmTaaTARFRhMcFU1QZJR7PDgympCYRvgGBnmmc4QQoh7zWBCZPn06AO+//36Vn2Oz2bDZbO7pnJziK0g6HA4cZa6ncL6OHz7I5uVLyrVpJReLAq1im7vpTPNAK50omaeVmUcV5mnlG0/Zjoaqqhw7dowf927HYDCUm1ehQq2SWql+XaqqoqkqmqqhaSqa6ioeV1VUTXWPa6paMl9FVVV3vaVtxfO1MvOLh5pLLQ4WrrMHg5pmMBqx+vtj9QvAJyAAq38APv4B+AQGcSgtnc7dexAUHolfSAh+wSH4BASe8RssNfkebQhK+0v6zbOkn2uP9PVJ1emDOnWOyMyZM90BpqylS5fi5+dXY9vJP3KQo7/8VGPrq2279+3Ru4RaoRiNKAYjisEAJcPiNkOZcSMGkxnFZMJgMqGYzOWGBpMJxVjSZjZjtFgxWKwYLRYUk7lCsHAB+UBoeCP259sg/xAcOKTL628okpKS9C6hQZB+rj3S11BQUFDlZetUEJk8eTITJ050T+fk5BAXF8fgwYMJCqq53d5Zx46wPbjscfviDyP3Z1K5D6fSeUrZyZJRpcLyJ0crmXfKQpX9d12u7ZT5qqqya9cuWrVq5b69+6nrLL+OcsVWueayNbg/9A0GFEXBYDCAYsBQpq14vlIcChQDGIqXOznPgKIUD8u1l7QZzWaMJhPGkvBgNJswGPS7cqjD4SApKYlBgwZhNpt1q6O+k36uHdLPtUf6+qTSIxpVUa0g8vjjj/P888+fcZnt27fTpk2b6qzWzWq1YrVaK7SbzeYa/aFGxsUTedOYGltfbXE4HKQtXkz3YcMa/Ju8NtT0+05UTvq5dkg/1x7pa6r1+qsVRB555BHGjBlzxmWaN29enVUKIYQQogGrVhCJjIwkMjLSU7UIIYQQooHx2DkiBw4c4Pjx4xw4cACXy8WmTZsAaNGiBQFyXQUhhBBC4MEgMmXKFD744AP3dJcuXQBYsWIF/fr189RmhRBCCOFFDJ5a8fvvv1983YhTHhJChBBCCFHKY0FECCGEEOJsJIgIIYQQQjcSRIQQQgihGwkiQgghhNCNBBEhhBBC6EaCiBBCCCF0I0FECCGEELqRICKEEEII3Xjsyqo1QdM0oHq3E67PHA4HBQUF5OTkNPg7O3qS9HPtkH6uHdLPtUf6+qTSz+3Sz/EzqdNBJDc3F4C4uDidKxFCCCFEdeXm5hIcHHzGZRStKnFFJ6qqcuTIEQIDA1EURe9ydJeTk0NcXBwHDx4kKChI73LqLenn2iH9XDukn2uP9PVJmqaRm5tLbGwsBsOZzwKp03tEDAYDTZo00buMOicoKKjBv8lrg/Rz7ZB+rh3Sz7VH+rrY2faElJKTVYUQQgihGwkiQgghhNCNBBEvYrVamTp1KlarVe9S6jXp59oh/Vw7pJ9rj/T1uanTJ6sKIYQQon6TPSJCCCGE0I0EESGEEELoRoKIEEIIIXQjQUQIIYQQupEgIoQQQgjdSBDxcjabjc6dO6MoCps2bdK7nHpl37593HnnnTRr1gxfX18SExOZOnUqdrtd79LqhTfeeIOEhAR8fHzo1asXf/75p94l1SszZ86kR48eBAYGEhUVxYgRI9i5c6feZdV7zz33HIqi8NBDD+lditeQIOLl/v3vfxMbG6t3GfXSjh07UFWVt956i61bt/Lqq68yb948/u///k/v0rzeZ599xsSJE5k6dSobNmygU6dODBkyhLS0NL1LqzdWrlzJ+PHj+eOPP0hKSsLhcDB48GDy8/P1Lq3eWrduHW+99RYdO3bUuxTvogmvtXjxYq1Nmzba1q1bNUDbuHGj3iXVey+88ILWrFkzvcvwej179tTGjx/vnna5XFpsbKw2c+ZMHauq39LS0jRAW7lypd6l1Eu5ublay5YttaSkJK1v377agw8+qHdJXkP2iHip1NRU7r77bj766CP8/Pz0LqfByM7OJiwsTO8yvJrdbmf9+vUMHDjQ3WYwGBg4cCC///67jpXVb9nZ2QDy/vWQ8ePHc8UVV5R7X4uqqdN33xWV0zSNMWPGMG7cOLp3786+ffv0LqlB2LNnD3PmzOGll17SuxSvlpGRgcvlIjo6ulx7dHQ0O3bs0Kmq+k1VVR566CEuuugi2rdvr3c59c6nn37Khg0bWLdund6leCXZI1KHPP744yiKcsbHjh07mDNnDrm5uUyePFnvkr1SVfu5rMOHD3P55Zdz/fXXc/fdd+tUuRDnZvz48WzZsoVPP/1U71LqnYMHD/Lggw/yySef4OPjo3c5XknuNVOHpKenk5mZecZlmjdvzg033MB3332HoijudpfLhdFo5Oabb+aDDz7wdKlerar9bLFYADhy5Aj9+vXjwgsv5P3338dgkPx+Pux2O35+fixcuJARI0a420ePHk1WVhaLFi3Sr7h6aMKECSxatIhVq1bRrFkzvcupd7755huuueYajEaju83lcqEoCgaDAZvNVm6eqEiCiBc6cOAAOTk57ukjR44wZMgQFi5cSK9evWjSpImO1dUvhw8f5rLLLqNbt258/PHH8gelhvTq1YuePXsyZ84coPjQQdOmTZkwYQKPP/64ztXVD5qmcf/99/P111/zyy+/0LJlS71Lqpdyc3PZv39/ubbbb7+dNm3aMGnSJDkUVgVyjogXatq0abnpgIAAABITEyWE1KDDhw/Tr18/4uPjeemll0hPT3fPi4mJ0bEy7zdx4kRGjx5N9+7d6dmzJ7NmzSI/P5/bb79d79LqjfHjx7NgwQIWLVpEYGAgx44dAyA4OBhfX1+dq6s/AgMDK4QNf39/wsPDJYRUkQQRIU4jKSmJPXv2sGfPngoBT3Yknp9Ro0aRnp7OlClTOHbsGJ07d+ann36qcAKrOHdz584FoF+/fuXa58+fz5gxY2q/ICFOQw7NCCGEEEI3ctadEEIIIXQjQUQIIYQQupEgIoQQQgjdSBARQgghhG4kiAghhBBCNxJEhBBCCKEbCSJCCCGE0I0EESGEEELoRoKIEEIIIXQjQUQIIYQQupEgIoQQQgjd/D+Ra4yZLDZH8gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "x= np.linspace(-5,5,200)\n",
        "sigmoid = 1/(1+np.exp(-x)) #sigmoid\n",
        "relu=np.maximum(0,x) #relu\n",
        "leaky_relu=np.where(x>0,x,0.01*x) #alpha = 0.01\n",
        "elu = np.where(x>0,x,0.5*(np.exp(x)-1)) #alpha=0.5\n",
        "softmax=np.exp(x)/np.sum(np.exp(x)) #softmax\n",
        "tanh=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "plt.plot(x,sigmoid,label=\"sigmoid\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title(\"Activation function\")\n",
        "plt.plot(x,relu,label=\"relu\")\n",
        "plt.legend()\n",
        "plt.plot(x,leaky_relu,label=\"leaky_relu\")\n",
        "plt.legend()\n",
        "plt.plot(x,elu,label=\"elu\")\n",
        "plt.legend()\n",
        "plt.plot(x,softmax,label=\"softmax\")\n",
        "plt.legend()\n",
        "plt.plot(x,tanh,label=\"tanh\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rk49XhHzq7n0",
      "metadata": {
        "id": "rk49XhHzq7n0"
      },
      "source": [
        " 3. Write a Python code to implement the binary cross-entropy loss function from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc95114e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc95114e",
        "outputId": "456be6e7-33fd-49c4-92fe-02ed1770c948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.10536052 0.10536052 0.22314355 1.2039728 ]\n"
          ]
        }
      ],
      "source": [
        "def bce(actual,predicted,eps=1e-9): #3\n",
        "    predicted = np.clip(predicted,eps,1-eps)\n",
        "    return -(actual*np.log(predicted)+(1-actual)*np.log(1-predicted))\n",
        "actual =np.array([0,1,1,0])\n",
        "predicted = np.array([0.1,0.9,0.8,0.7])\n",
        "print(bce(actual,predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "949fa65f",
      "metadata": {
        "id": "949fa65f"
      },
      "source": [
        "4. Write a Python code to implement the Stochastic Gradient Descent Optimization technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9zAlgaTDZNaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9zAlgaTDZNaG",
        "outputId": "2a7b712c-72b9-468c-e634-6b9992980e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss = [24.01]\n",
            "Epoch 100: Loss = [0.4222866]\n",
            "Epoch 200: Loss = [0.00742715]\n",
            "Epoch 300: Loss = [0.00013063]\n",
            "Epoch 400: Loss = [2.29748516e-06]\n",
            "Epoch 500: Loss = [4.04080462e-08]\n",
            "Epoch 600: Loss = [7.1069456e-10]\n",
            "Epoch 700: Loss = [1.2499658e-11]\n",
            "Epoch 800: Loss = [2.19843317e-13]\n",
            "Epoch 900: Loss = [3.86659252e-15]\n",
            "\n",
            "Optimized parameters: [8.41483679e-09]\n",
            "Minimum value of the function: [7.08094781e-17]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sgd(f, gradient, initial_params, learning_rate, epochs):\n",
        "\n",
        "    params = np.copy(initial_params)\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        grad = gradient(params)\n",
        "        params -= learning_rate * grad\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {f(params)}\")\n",
        "    return params\n",
        "\n",
        "def objective_function(x):\n",
        "    return x**2\n",
        "\n",
        "def gradient_function(x):\n",
        "    return 2 * x\n",
        "\n",
        "initial_parameters = np.array([5.0])\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "optimized_parameters = sgd(objective_function, gradient_function, initial_parameters, learning_rate, epochs)\n",
        "print(f\"\\nOptimized parameters: {optimized_parameters}\")\n",
        "print(f\"Minimum value of the function: {objective_function(optimized_parameters)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L4i3wCWutJqr",
      "metadata": {
        "id": "L4i3wCWutJqr"
      },
      "source": [
        "5. Build a multilayer perceptron from scratch using numpy and compare it with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GbwFmBj1a8c2",
      "metadata": {
        "id": "GbwFmBj1a8c2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def sigmoid(z): return 1/ (1+np.exp(-z))\n",
        "def forward_sigmoid(X,w1,b1,w2,b2):\n",
        "  z1= X @ w1+b1\n",
        "  a1= sigmoid(z1)\n",
        "  z2= X @ w2+b2\n",
        "  a2= sigmoid(z2)\n",
        "  return(a2>0.5).astype(int)\n",
        "X= np.array([[0,0],[1,0],[0,1],[1,1]])\n",
        "y= np.array([[0],[1],[1],[0]])\n",
        "w1= np.array([[20,20],[20,20]])\n",
        "b1= np.array([[-10,30]])\n",
        "w2= np.array([[20],[-20]])\n",
        "b2= np.array([[-10]])\n",
        "print (\"Sigmoid NN Predictions:\", forward_sigmoid(X,w1,b1,w2,b2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LIQKx2t5EFfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIQKx2t5EFfe",
        "outputId": "a1abd071-2f0d-4201-eec6-525e68ad72ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.324659\n",
            "Epoch 1000, Loss: 0.240589\n",
            "Epoch 2000, Loss: 0.196030\n",
            "Epoch 3000, Loss: 0.120663\n",
            "Epoch 4000, Loss: 0.030459\n",
            "Epoch 5000, Loss: 0.012541\n",
            "Epoch 6000, Loss: 0.007368\n",
            "Epoch 7000, Loss: 0.005093\n",
            "Epoch 8000, Loss: 0.003847\n",
            "Epoch 9000, Loss: 0.003071\n",
            "\n",
            "Final predictions after training:\n",
            "[[0.053]\n",
            " [0.952]\n",
            " [0.952]\n",
            " [0.052]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "# Input dataset (XOR inputs)\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "# Output dataset (XOR outputs)\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "np.random.seed(42)\n",
        "input_layer_neurons = X.shape[1]\n",
        "hidden_layer_neurons = 2\n",
        "output_neurons = 1\n",
        "# Weights and biases\n",
        "W1 = np.random.uniform(size=(input_layer_neurons,\n",
        "hidden_layer_neurons))\n",
        "b1 = np.random.uniform(size=(1, hidden_layer_neurons))\n",
        "W2 = np.random.uniform(size=(hidden_layer_neurons,\n",
        "output_neurons))\n",
        "b2 = np.random.uniform(size=(1, output_neurons))\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "# Number of epochs\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "  # Forward propagation\n",
        "  hidden_input = np.dot(X, W1) + b1\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "  final_input = np.dot  (hidden_output, W2) + b2\n",
        "  y_pred = sigmoid(final_input)\n",
        "  # Compute Error\n",
        "  error = y - y_pred\n",
        "  # Backpropagation\n",
        "  d_y_pred = error * sigmoid_derivative(y_pred)\n",
        "  error_hidden_layer = d_y_pred.dot(W2.T)\n",
        "  d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_output)\n",
        "\n",
        "# Update weights and biases\n",
        "  W2 += hidden_output.T.dot(d_y_pred) * lr\n",
        "  b2 += np.sum(d_y_pred, axis=0, keepdims=True) * lr\n",
        "  W1 += X.T.dot(d_hidden_layer) * lr\n",
        "  b1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr\n",
        "\n",
        "# Print loss occasionally\n",
        "  if epoch % 1000 == 0:\n",
        "    loss = np.mean(np.square(error))\n",
        "    print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "# -------- Final results --------\n",
        "print(\"\\nFinal predictions after training:\")\n",
        "print(y_pred.round(3))   # Rounded for clarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q3d-3kPPbnQd",
      "metadata": {
        "id": "q3d-3kPPbnQd"
      },
      "source": [
        "6. Write a Python code to classify the XOR function and use sigmoid, ReLU, and LeakyReLU in the\n",
        "hidden layer of a neural network. Discuss the accuracy of different activation functions used in the\n",
        "hidden layer. Also, show the comparative analysis in a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "Qk85I_JFcl-h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "Qk85I_JFcl-h",
        "outputId": "14c92433-d0fa-4417-e104-8b51fc035559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with Sigmoid activation...\n",
            "Accuracy with Sigmoid: 1.0000\n",
            "\n",
            "Training with ReLU activation...\n",
            "Accuracy with ReLU: 0.5000\n",
            "\n",
            "Training with LeakyReLU activation...\n",
            "Accuracy with LeakyReLU: 0.5000\n",
            "\n",
            "Comparative Analysis of Activation Functions:\n",
            "Sigmoid Accuracy: 1.0000\n",
            "ReLU Accuracy: 0.5000\n",
            "LeakyReLU Accuracy: 0.5000\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# ReLU activation function and its derivative\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# LeakyReLU activation function and its derivative\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "# Neural network training function\n",
        "def train_nn(X, y, activation_func, activation_derivative, epochs=10000, learning_rate=0.1):\n",
        "    input_layer_neurons = X.shape[1]\n",
        "    hidden_layer_neurons = 2\n",
        "    output_neurons = 1\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
        "    b1 = np.random.uniform(size=(1, hidden_layer_neurons))\n",
        "    W2 = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
        "    b2 = np.random.uniform(size=(1, output_neurons))\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward propagation\n",
        "        hidden_layer_input = np.dot(X, W1) + b1\n",
        "        hidden_layer_output = activation_func(hidden_layer_input)\n",
        "        output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
        "        predicted_output = sigmoid(output_layer_input) # Using sigmoid for the output layer\n",
        "\n",
        "        # Backpropagation\n",
        "        error = y - predicted_output\n",
        "        d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "        error_hidden_layer = d_predicted_output.dot(W2.T)\n",
        "        d_hidden_layer = error_hidden_layer * activation_derivative(hidden_layer_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        W2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "        b2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "        W1 += X.T.dot(d_hidden_layer) * learning_rate\n",
        "        b1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        loss = np.mean(np.square(error))\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return predicted_output, loss_history\n",
        "\n",
        "# Train with different activation functions and store accuracies and loss histories\n",
        "results = {}\n",
        "activations = {\n",
        "    \"Sigmoid\": (sigmoid, sigmoid_derivative),\n",
        "    \"ReLU\": (relu, relu_derivative),\n",
        "    \"LeakyReLU\": (leaky_relu, leaky_relu_derivative)\n",
        "}\n",
        "\n",
        "for name, (func, derivative) in activations.items():\n",
        "    print(f\"Training with {name} activation...\")\n",
        "    predicted_output, loss_history = train_nn(X, y, func, derivative)\n",
        "    accuracy = np.mean((predicted_output.round() == y).astype(int))\n",
        "    results[name] = {\"accuracy\": accuracy, \"loss_history\": loss_history}\n",
        "    print(f\"Accuracy with {name}: {accuracy:.4f}\\n\")\n",
        "\n",
        "# Discuss the accuracy\n",
        "print(\"Comparative Analysis of Activation Functions:\")\n",
        "for name, data in results.items():\n",
        "    print(f\"{name} Accuracy: {data['accuracy']:.4f}\")\n",
        "\n",
        "# Plot the loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "for name, data in results.items():\n",
        "    plt.plot(data['loss_history'], label=f'{name} Loss')\n",
        "plt.title('Loss History Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EHzi80U0u9nx",
      "metadata": {
        "id": "EHzi80U0u9nx"
      },
      "source": [
        "7. Write code to define a simple neural network in TensorFlow. Use the MNIST dataset, reshape the data\n",
        " to 610,000 x 784, normalize the input, and build and train the model using the SGD optimizer and\n",
        " categorical cross-entropy as the loss function to compute the accuracy. Also, find the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TT-xZ9hj-NH1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT-xZ9hj-NH1",
        "outputId": "390ecb12-adae-441f-d9e7-187de012c813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.3380 - loss: 1.9306 - val_accuracy: 0.8343 - val_loss: 0.7591 - learning_rate: 0.0100\n",
            "Epoch 2/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7286 - loss: 0.8831 - val_accuracy: 0.8815 - val_loss: 0.4787 - learning_rate: 0.0100\n",
            "Epoch 3/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8000 - loss: 0.6579 - val_accuracy: 0.8953 - val_loss: 0.3936 - learning_rate: 0.0100\n",
            "Epoch 4/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8289 - loss: 0.5569 - val_accuracy: 0.9027 - val_loss: 0.3496 - learning_rate: 0.0100\n",
            "Epoch 5/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8488 - loss: 0.5028 - val_accuracy: 0.9086 - val_loss: 0.3218 - learning_rate: 0.0100\n",
            "Epoch 6/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8616 - loss: 0.4621 - val_accuracy: 0.9142 - val_loss: 0.3004 - learning_rate: 0.0100\n",
            "Epoch 7/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8716 - loss: 0.4262 - val_accuracy: 0.9178 - val_loss: 0.2827 - learning_rate: 0.0100\n",
            "Epoch 8/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8806 - loss: 0.4008 - val_accuracy: 0.9218 - val_loss: 0.2702 - learning_rate: 0.0100\n",
            "Epoch 9/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8837 - loss: 0.3908 - val_accuracy: 0.9248 - val_loss: 0.2586 - learning_rate: 0.0100\n",
            "Epoch 10/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8870 - loss: 0.3717 - val_accuracy: 0.9268 - val_loss: 0.2490 - learning_rate: 0.0100\n",
            "Epoch 11/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8953 - loss: 0.3524 - val_accuracy: 0.9300 - val_loss: 0.2391 - learning_rate: 0.0100\n",
            "Epoch 12/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9009 - loss: 0.3403 - val_accuracy: 0.9326 - val_loss: 0.2290 - learning_rate: 0.0100\n",
            "Epoch 13/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9031 - loss: 0.3299 - val_accuracy: 0.9349 - val_loss: 0.2225 - learning_rate: 0.0100\n",
            "Epoch 14/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9084 - loss: 0.3109 - val_accuracy: 0.9368 - val_loss: 0.2156 - learning_rate: 0.0100\n",
            "Epoch 15/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9058 - loss: 0.3158 - val_accuracy: 0.9385 - val_loss: 0.2105 - learning_rate: 0.0100\n",
            "Epoch 16/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9084 - loss: 0.3065 - val_accuracy: 0.9406 - val_loss: 0.2035 - learning_rate: 0.0100\n",
            "Epoch 17/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9151 - loss: 0.2911 - val_accuracy: 0.9417 - val_loss: 0.1984 - learning_rate: 0.0100\n",
            "Epoch 18/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9176 - loss: 0.2848 - val_accuracy: 0.9420 - val_loss: 0.1936 - learning_rate: 0.0100\n",
            "Epoch 19/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2733 - val_accuracy: 0.9435 - val_loss: 0.1892 - learning_rate: 0.0100\n",
            "Epoch 20/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9213 - loss: 0.2688 - val_accuracy: 0.9458 - val_loss: 0.1846 - learning_rate: 0.0100\n",
            "Epoch 21/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9233 - loss: 0.2578 - val_accuracy: 0.9456 - val_loss: 0.1811 - learning_rate: 0.0100\n",
            "Epoch 22/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9263 - loss: 0.2541 - val_accuracy: 0.9481 - val_loss: 0.1762 - learning_rate: 0.0100\n",
            "Epoch 23/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9257 - loss: 0.2511 - val_accuracy: 0.9490 - val_loss: 0.1736 - learning_rate: 0.0100\n",
            "Epoch 24/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9268 - loss: 0.2482 - val_accuracy: 0.9493 - val_loss: 0.1707 - learning_rate: 0.0100\n",
            "Epoch 25/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9300 - loss: 0.2366 - val_accuracy: 0.9502 - val_loss: 0.1668 - learning_rate: 0.0100\n",
            "Epoch 26/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9299 - loss: 0.2398 - val_accuracy: 0.9513 - val_loss: 0.1645 - learning_rate: 0.0100\n",
            "Epoch 27/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9338 - loss: 0.2229 - val_accuracy: 0.9519 - val_loss: 0.1615 - learning_rate: 0.0100\n",
            "Epoch 28/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9338 - loss: 0.2210 - val_accuracy: 0.9534 - val_loss: 0.1584 - learning_rate: 0.0100\n",
            "Epoch 29/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9347 - loss: 0.2206 - val_accuracy: 0.9532 - val_loss: 0.1564 - learning_rate: 0.0100\n",
            "Epoch 30/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9363 - loss: 0.2110 - val_accuracy: 0.9541 - val_loss: 0.1537 - learning_rate: 0.0100\n",
            "Epoch 31/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9345 - loss: 0.2161 - val_accuracy: 0.9544 - val_loss: 0.1513 - learning_rate: 0.0100\n",
            "Epoch 32/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9385 - loss: 0.2099 - val_accuracy: 0.9549 - val_loss: 0.1495 - learning_rate: 0.0100\n",
            "Epoch 33/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9393 - loss: 0.2061 - val_accuracy: 0.9559 - val_loss: 0.1474 - learning_rate: 0.0100\n",
            "Epoch 34/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9380 - loss: 0.2069 - val_accuracy: 0.9567 - val_loss: 0.1447 - learning_rate: 0.0100\n",
            "Epoch 35/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9418 - loss: 0.2002 - val_accuracy: 0.9572 - val_loss: 0.1430 - learning_rate: 0.0100\n",
            "Epoch 36/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9407 - loss: 0.2011 - val_accuracy: 0.9572 - val_loss: 0.1426 - learning_rate: 0.0100\n",
            "Epoch 37/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9416 - loss: 0.1971 - val_accuracy: 0.9580 - val_loss: 0.1400 - learning_rate: 0.0100\n",
            "Epoch 38/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9418 - loss: 0.1955 - val_accuracy: 0.9587 - val_loss: 0.1386 - learning_rate: 0.0100\n",
            "Epoch 39/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9424 - loss: 0.1928 - val_accuracy: 0.9589 - val_loss: 0.1368 - learning_rate: 0.0100\n",
            "Epoch 40/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9474 - loss: 0.1807 - val_accuracy: 0.9598 - val_loss: 0.1344 - learning_rate: 0.0100\n",
            "Epoch 41/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9455 - loss: 0.1840 - val_accuracy: 0.9603 - val_loss: 0.1334 - learning_rate: 0.0100\n",
            "Epoch 42/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9456 - loss: 0.1800 - val_accuracy: 0.9607 - val_loss: 0.1321 - learning_rate: 0.0100\n",
            "Epoch 43/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9472 - loss: 0.1778 - val_accuracy: 0.9617 - val_loss: 0.1305 - learning_rate: 0.0100\n",
            "Epoch 44/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9479 - loss: 0.1766 - val_accuracy: 0.9610 - val_loss: 0.1294 - learning_rate: 0.0100\n",
            "Epoch 45/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9496 - loss: 0.1738 - val_accuracy: 0.9622 - val_loss: 0.1280 - learning_rate: 0.0100\n",
            "Epoch 46/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.1693 - val_accuracy: 0.9621 - val_loss: 0.1271 - learning_rate: 0.0100\n",
            "Epoch 47/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9520 - loss: 0.1658 - val_accuracy: 0.9628 - val_loss: 0.1264 - learning_rate: 0.0100\n",
            "Epoch 48/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9480 - loss: 0.1729 - val_accuracy: 0.9632 - val_loss: 0.1246 - learning_rate: 0.0100\n",
            "Epoch 49/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.1662 - val_accuracy: 0.9638 - val_loss: 0.1234 - learning_rate: 0.0100\n",
            "Epoch 50/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9511 - loss: 0.1662 - val_accuracy: 0.9638 - val_loss: 0.1227 - learning_rate: 0.0100\n",
            "Epoch 51/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9532 - loss: 0.1570 - val_accuracy: 0.9649 - val_loss: 0.1205 - learning_rate: 0.0100\n",
            "Epoch 52/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9509 - loss: 0.1648 - val_accuracy: 0.9653 - val_loss: 0.1198 - learning_rate: 0.0100\n",
            "Epoch 53/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9523 - loss: 0.1596 - val_accuracy: 0.9652 - val_loss: 0.1194 - learning_rate: 0.0100\n",
            "Epoch 54/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9523 - loss: 0.1594 - val_accuracy: 0.9654 - val_loss: 0.1185 - learning_rate: 0.0100\n",
            "Epoch 55/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9541 - loss: 0.1527 - val_accuracy: 0.9659 - val_loss: 0.1172 - learning_rate: 0.0100\n",
            "Epoch 56/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.1543 - val_accuracy: 0.9659 - val_loss: 0.1166 - learning_rate: 0.0100\n",
            "Epoch 57/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9536 - loss: 0.1523 - val_accuracy: 0.9666 - val_loss: 0.1148 - learning_rate: 0.0100\n",
            "Epoch 58/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9554 - loss: 0.1499 - val_accuracy: 0.9663 - val_loss: 0.1152 - learning_rate: 0.0100\n",
            "Epoch 59/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1455 - val_accuracy: 0.9672 - val_loss: 0.1135 - learning_rate: 0.0100\n",
            "Epoch 60/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1445 - val_accuracy: 0.9673 - val_loss: 0.1129 - learning_rate: 0.0100\n",
            "Epoch 61/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.1485 - val_accuracy: 0.9674 - val_loss: 0.1122 - learning_rate: 0.0100\n",
            "Epoch 62/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9563 - loss: 0.1452 - val_accuracy: 0.9676 - val_loss: 0.1117 - learning_rate: 0.0100\n",
            "Epoch 63/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9586 - loss: 0.1399 - val_accuracy: 0.9682 - val_loss: 0.1103 - learning_rate: 0.0100\n",
            "Epoch 64/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1453 - val_accuracy: 0.9682 - val_loss: 0.1098 - learning_rate: 0.0100\n",
            "Epoch 65/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1438 - val_accuracy: 0.9687 - val_loss: 0.1089 - learning_rate: 0.0100\n",
            "Epoch 66/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9609 - loss: 0.1347 - val_accuracy: 0.9685 - val_loss: 0.1086 - learning_rate: 0.0100\n",
            "Epoch 67/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9588 - loss: 0.1349 - val_accuracy: 0.9682 - val_loss: 0.1077 - learning_rate: 0.0100\n",
            "Epoch 68/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9599 - loss: 0.1367 - val_accuracy: 0.9692 - val_loss: 0.1072 - learning_rate: 0.0100\n",
            "Epoch 69/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9606 - loss: 0.1333 - val_accuracy: 0.9689 - val_loss: 0.1067 - learning_rate: 0.0100\n",
            "Epoch 70/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9599 - loss: 0.1319 - val_accuracy: 0.9700 - val_loss: 0.1059 - learning_rate: 0.0100\n",
            "Epoch 71/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1345 - val_accuracy: 0.9697 - val_loss: 0.1053 - learning_rate: 0.0100\n",
            "Epoch 72/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.1304 - val_accuracy: 0.9695 - val_loss: 0.1047 - learning_rate: 0.0100\n",
            "Epoch 73/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9620 - loss: 0.1312 - val_accuracy: 0.9697 - val_loss: 0.1038 - learning_rate: 0.0100\n",
            "Epoch 74/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9604 - loss: 0.1316 - val_accuracy: 0.9701 - val_loss: 0.1037 - learning_rate: 0.0100\n",
            "Epoch 75/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9624 - loss: 0.1281 - val_accuracy: 0.9698 - val_loss: 0.1028 - learning_rate: 0.0100\n",
            "Epoch 76/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9618 - loss: 0.1257 - val_accuracy: 0.9706 - val_loss: 0.1018 - learning_rate: 0.0100\n",
            "Epoch 77/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9613 - loss: 0.1297 - val_accuracy: 0.9709 - val_loss: 0.1020 - learning_rate: 0.0100\n",
            "Epoch 78/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9626 - loss: 0.1268 - val_accuracy: 0.9708 - val_loss: 0.1013 - learning_rate: 0.0100\n",
            "Epoch 79/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9625 - loss: 0.1240 - val_accuracy: 0.9710 - val_loss: 0.1004 - learning_rate: 0.0100\n",
            "Epoch 80/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9633 - loss: 0.1240 - val_accuracy: 0.9712 - val_loss: 0.0998 - learning_rate: 0.0100\n",
            "Epoch 81/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9639 - loss: 0.1229 - val_accuracy: 0.9719 - val_loss: 0.0998 - learning_rate: 0.0100\n",
            "Epoch 82/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.1178 - val_accuracy: 0.9719 - val_loss: 0.0992 - learning_rate: 0.0100\n",
            "Epoch 83/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9623 - loss: 0.1239 - val_accuracy: 0.9714 - val_loss: 0.0990 - learning_rate: 0.0100\n",
            "Epoch 84/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9636 - loss: 0.1200 - val_accuracy: 0.9715 - val_loss: 0.0982 - learning_rate: 0.0100\n",
            "Epoch 85/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9659 - loss: 0.1153 - val_accuracy: 0.9718 - val_loss: 0.0982 - learning_rate: 0.0100\n",
            "Epoch 86/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9646 - loss: 0.1179 - val_accuracy: 0.9713 - val_loss: 0.0972 - learning_rate: 0.0100\n",
            "Epoch 87/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9646 - loss: 0.1202 - val_accuracy: 0.9717 - val_loss: 0.0973 - learning_rate: 0.0100\n",
            "Epoch 88/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9665 - loss: 0.1160 - val_accuracy: 0.9721 - val_loss: 0.0970 - learning_rate: 0.0100\n",
            "Epoch 89/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9660 - loss: 0.1106 - val_accuracy: 0.9728 - val_loss: 0.0967 - learning_rate: 0.0100\n",
            "Epoch 90/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9669 - loss: 0.1125 - val_accuracy: 0.9724 - val_loss: 0.0957 - learning_rate: 0.0100\n",
            "Epoch 91/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.1151 - val_accuracy: 0.9725 - val_loss: 0.0957 - learning_rate: 0.0100\n",
            "Epoch 92/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9670 - loss: 0.1130 - val_accuracy: 0.9720 - val_loss: 0.0954 - learning_rate: 0.0100\n",
            "Epoch 93/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.1091 - val_accuracy: 0.9728 - val_loss: 0.0943 - learning_rate: 0.0100\n",
            "Epoch 94/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.1150 - val_accuracy: 0.9730 - val_loss: 0.0947 - learning_rate: 0.0100\n",
            "Epoch 95/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9679 - loss: 0.1054 - val_accuracy: 0.9726 - val_loss: 0.0940 - learning_rate: 0.0100\n",
            "Epoch 96/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9674 - loss: 0.1083 - val_accuracy: 0.9724 - val_loss: 0.0947 - learning_rate: 0.0100\n",
            "Epoch 97/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9657 - loss: 0.1113 - val_accuracy: 0.9729 - val_loss: 0.0932 - learning_rate: 0.0100\n",
            "Epoch 98/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.1089 - val_accuracy: 0.9730 - val_loss: 0.0931 - learning_rate: 0.0100\n",
            "Epoch 99/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9677 - loss: 0.1076 - val_accuracy: 0.9729 - val_loss: 0.0929 - learning_rate: 0.0100\n",
            "Epoch 100/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9676 - loss: 0.1046 - val_accuracy: 0.9730 - val_loss: 0.0927 - learning_rate: 0.0100\n",
            "Epoch 101/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9683 - loss: 0.1062 - val_accuracy: 0.9732 - val_loss: 0.0921 - learning_rate: 0.0100\n",
            "Epoch 102/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9675 - loss: 0.1054 - val_accuracy: 0.9732 - val_loss: 0.0918 - learning_rate: 0.0100\n",
            "Epoch 103/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9695 - loss: 0.1039 - val_accuracy: 0.9728 - val_loss: 0.0911 - learning_rate: 0.0100\n",
            "Epoch 104/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.1039 - val_accuracy: 0.9730 - val_loss: 0.0914 - learning_rate: 0.0100\n",
            "Epoch 105/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9687 - loss: 0.1000 - val_accuracy: 0.9738 - val_loss: 0.0906 - learning_rate: 0.0100\n",
            "Epoch 106/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.1002 - val_accuracy: 0.9732 - val_loss: 0.0909 - learning_rate: 0.0100\n",
            "Epoch 107/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9684 - loss: 0.1019 - val_accuracy: 0.9734 - val_loss: 0.0902 - learning_rate: 0.0100\n",
            "Epoch 108/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9706 - loss: 0.0991 - val_accuracy: 0.9737 - val_loss: 0.0898 - learning_rate: 0.0100\n",
            "Epoch 109/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.0978 - val_accuracy: 0.9735 - val_loss: 0.0899 - learning_rate: 0.0100\n",
            "Epoch 110/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9696 - loss: 0.1005 - val_accuracy: 0.9737 - val_loss: 0.0893 - learning_rate: 0.0100\n",
            "Epoch 111/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.0998 - val_accuracy: 0.9732 - val_loss: 0.0887 - learning_rate: 0.0100\n",
            "Epoch 112/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0968 - val_accuracy: 0.9739 - val_loss: 0.0891 - learning_rate: 0.0100\n",
            "Epoch 113/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.0968 - val_accuracy: 0.9740 - val_loss: 0.0883 - learning_rate: 0.0100\n",
            "Epoch 114/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9708 - loss: 0.0967 - val_accuracy: 0.9734 - val_loss: 0.0885 - learning_rate: 0.0100\n",
            "Epoch 115/200\n",
            "\u001b[1m360/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9704 - loss: 0.0966\n",
            "Epoch 115: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9704 - loss: 0.0965 - val_accuracy: 0.9737 - val_loss: 0.0883 - learning_rate: 0.0100\n",
            "Epoch 116/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9718 - loss: 0.0939 - val_accuracy: 0.9740 - val_loss: 0.0880 - learning_rate: 0.0050\n",
            "Epoch 117/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9723 - loss: 0.0907 - val_accuracy: 0.9739 - val_loss: 0.0879 - learning_rate: 0.0050\n",
            "Epoch 118/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9715 - loss: 0.0946 - val_accuracy: 0.9744 - val_loss: 0.0874 - learning_rate: 0.0050\n",
            "Epoch 119/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9715 - loss: 0.0949 - val_accuracy: 0.9743 - val_loss: 0.0877 - learning_rate: 0.0050\n",
            "Epoch 120/200\n",
            "\u001b[1m363/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0912\n",
            "Epoch 120: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.0913 - val_accuracy: 0.9744 - val_loss: 0.0874 - learning_rate: 0.0050\n",
            "Epoch 121/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9732 - loss: 0.0932 - val_accuracy: 0.9743 - val_loss: 0.0873 - learning_rate: 0.0025\n",
            "Epoch 122/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.0923 - val_accuracy: 0.9740 - val_loss: 0.0873 - learning_rate: 0.0025\n",
            "Epoch 123/200\n",
            "\u001b[1m367/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9732 - loss: 0.0894\n",
            "Epoch 123: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9732 - loss: 0.0894 - val_accuracy: 0.9741 - val_loss: 0.0873 - learning_rate: 0.0025\n",
            "Epoch 124/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9714 - loss: 0.0964 - val_accuracy: 0.9741 - val_loss: 0.0873 - learning_rate: 0.0012\n",
            "Epoch 125/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.0947 - val_accuracy: 0.9739 - val_loss: 0.0871 - learning_rate: 0.0012\n",
            "Epoch 126/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9736 - loss: 0.0903 - val_accuracy: 0.9743 - val_loss: 0.0871 - learning_rate: 0.0012\n",
            "Epoch 127/200\n",
            "\u001b[1m361/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9724 - loss: 0.0898\n",
            "Epoch 127: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.0898 - val_accuracy: 0.9741 - val_loss: 0.0870 - learning_rate: 0.0012\n",
            "Epoch 128/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9709 - loss: 0.0957 - val_accuracy: 0.9741 - val_loss: 0.0871 - learning_rate: 6.2500e-04\n",
            "Epoch 129/200\n",
            "\u001b[1m370/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0909\n",
            "Epoch 129: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9728 - loss: 0.0909 - val_accuracy: 0.9743 - val_loss: 0.0871 - learning_rate: 6.2500e-04\n",
            "Epoch 130/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9720 - loss: 0.0960 - val_accuracy: 0.9742 - val_loss: 0.0870 - learning_rate: 3.1250e-04\n",
            "Epoch 131/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9727 - loss: 0.0910 - val_accuracy: 0.9742 - val_loss: 0.0870 - learning_rate: 3.1250e-04\n",
            "Epoch 132/200\n",
            "\u001b[1m364/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.0950\n",
            "Epoch 132: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9712 - loss: 0.0949 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 3.1250e-04\n",
            "Epoch 133/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.0876 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 1.5625e-04\n",
            "Epoch 134/200\n",
            "\u001b[1m374/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9720 - loss: 0.0911\n",
            "Epoch 134: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9720 - loss: 0.0911 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 1.5625e-04\n",
            "Epoch 135/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9723 - loss: 0.0924 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 7.8125e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m369/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9729 - loss: 0.0884\n",
            "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9729 - loss: 0.0884 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 7.8125e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9741 - loss: 0.0890 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 3.9062e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m366/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.0914\n",
            "Epoch 138: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9716 - loss: 0.0914 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 3.9062e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9728 - loss: 0.0914 - val_accuracy: 0.9743 - val_loss: 0.0870 - learning_rate: 1.9531e-05\n",
            "Epoch 139: early stopping\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.0942\n",
            "Test accuracy: 0.9746999740600586\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES =10\n",
        "N_HIDDEN= 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "#load dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
        "#MNIST  dataset =60,000 trainng images + 10,000 test images\n",
        "RESHAPED = 784\n",
        "X_train = X_train.reshape(60000,RESHAPED)\n",
        "X_test = X_test.reshape(10000,RESHAPED)\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "#convert pixel values to floats betm 0 and 1\n",
        "y_train = tf.keras.utils.to_categorical(y_train,NB_CLASSES)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,NB_CLASSES)\n",
        "#convert lables into one hot encoding\n",
        "#model architecture\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(RESHAPED,)),\n",
        "    keras.layers.Dense(N_HIDDEN,activation='relu',kernel_initializer = 'he_normal'),\n",
        "\n",
        "\n",
        "    #keras.layer.Dropout(0.2),\n",
        "    keras.layers.Dense(N_HIDDEN//2,activation='relu',\n",
        "    kernel_initializer='he_normal'),\n",
        "\n",
        "    #keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(NB_CLASSES,activation='softmax')\n",
        "])\n",
        "#input = 784 features\n",
        "#model compilation\n",
        "model.compile(Optimizer= keras.optimizer.SG,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#model training\n",
        "#es = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5)\n",
        "#rs =keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.5, mode='min',verbose=1,patience=2,min_lr=0.00001)\n",
        "history = model.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=VERBOSE,validation_split=VALIDATION_SPLIT,callbacks=[es,rs])\n",
        "#model evaluation\n",
        "test_loss,test_acc = model.evaluate(X_test,y_test)\n",
        "print('Test accuracy:',test_acc,'Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93Bx3CLh19EW",
      "metadata": {
        "id": "93Bx3CLh19EW"
      },
      "source": [
        "8 Train the MLP as specified in Q7 with regularization (dropout and L2), different optimizers, and\n",
        "discuss a detailed comparative analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Xf89UbhBrRV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xf89UbhBrRV",
        "outputId": "c832a2d4-587b-4190-d160-d29bd825b8b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.1415 - loss: 2.6435 - val_accuracy: 0.3410 - val_loss: 2.4200\n",
            "Epoch 2/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3838 - loss: 2.3637 - val_accuracy: 0.5042 - val_loss: 2.1883\n",
            "Epoch 3/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5203 - loss: 2.1421 - val_accuracy: 0.6153 - val_loss: 1.9593\n",
            "Epoch 4/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6225 - loss: 1.9128 - val_accuracy: 0.6978 - val_loss: 1.7346\n",
            "Epoch 5/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6911 - loss: 1.6991 - val_accuracy: 0.7529 - val_loss: 1.5290\n",
            "Epoch 6/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7436 - loss: 1.5054 - val_accuracy: 0.7905 - val_loss: 1.3571\n",
            "Epoch 7/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7801 - loss: 1.3472 - val_accuracy: 0.8161 - val_loss: 1.2227\n",
            "Epoch 8/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8007 - loss: 1.2246 - val_accuracy: 0.8306 - val_loss: 1.1198\n",
            "Epoch 9/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8205 - loss: 1.1257 - val_accuracy: 0.8410 - val_loss: 1.0411\n",
            "Epoch 10/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8290 - loss: 1.0587 - val_accuracy: 0.8512 - val_loss: 0.9802\n",
            "Epoch 11/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8365 - loss: 1.0039 - val_accuracy: 0.8577 - val_loss: 0.9321\n",
            "Epoch 12/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8482 - loss: 0.9517 - val_accuracy: 0.8624 - val_loss: 0.8934\n",
            "Epoch 13/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8512 - loss: 0.9198 - val_accuracy: 0.8678 - val_loss: 0.8618\n",
            "Epoch 14/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8562 - loss: 0.8899 - val_accuracy: 0.8720 - val_loss: 0.8356\n",
            "Epoch 15/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8632 - loss: 0.8560 - val_accuracy: 0.8749 - val_loss: 0.8136\n",
            "Epoch 16/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8653 - loss: 0.8365 - val_accuracy: 0.8774 - val_loss: 0.7945\n",
            "Epoch 17/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8724 - loss: 0.8157 - val_accuracy: 0.8808 - val_loss: 0.7783\n",
            "Epoch 18/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8719 - loss: 0.8015 - val_accuracy: 0.8839 - val_loss: 0.7641\n",
            "Epoch 19/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8745 - loss: 0.7925 - val_accuracy: 0.8863 - val_loss: 0.7515\n",
            "Epoch 20/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8764 - loss: 0.7814 - val_accuracy: 0.8885 - val_loss: 0.7403\n",
            "Epoch 21/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8816 - loss: 0.7655 - val_accuracy: 0.8898 - val_loss: 0.7305\n",
            "Epoch 22/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8806 - loss: 0.7583 - val_accuracy: 0.8912 - val_loss: 0.7215\n",
            "Epoch 23/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8828 - loss: 0.7453 - val_accuracy: 0.8928 - val_loss: 0.7133\n",
            "Epoch 24/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8874 - loss: 0.7339 - val_accuracy: 0.8939 - val_loss: 0.7060\n",
            "Epoch 25/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8872 - loss: 0.7257 - val_accuracy: 0.8954 - val_loss: 0.6991\n",
            "Epoch 26/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8876 - loss: 0.7284 - val_accuracy: 0.8963 - val_loss: 0.6929\n",
            "Epoch 27/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8866 - loss: 0.7186 - val_accuracy: 0.8974 - val_loss: 0.6873\n",
            "Epoch 28/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8926 - loss: 0.7028 - val_accuracy: 0.8983 - val_loss: 0.6816\n",
            "Epoch 29/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8944 - loss: 0.7000 - val_accuracy: 0.8992 - val_loss: 0.6766\n",
            "Epoch 30/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8921 - loss: 0.6967 - val_accuracy: 0.9007 - val_loss: 0.6717\n",
            "Epoch 31/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8943 - loss: 0.6895 - val_accuracy: 0.9018 - val_loss: 0.6673\n",
            "Epoch 32/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8964 - loss: 0.6877 - val_accuracy: 0.9022 - val_loss: 0.6630\n",
            "Epoch 33/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8980 - loss: 0.6849 - val_accuracy: 0.9028 - val_loss: 0.6590\n",
            "Epoch 34/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8997 - loss: 0.6762 - val_accuracy: 0.9044 - val_loss: 0.6554\n",
            "Epoch 35/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8991 - loss: 0.6732 - val_accuracy: 0.9043 - val_loss: 0.6517\n",
            "Epoch 36/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8981 - loss: 0.6746 - val_accuracy: 0.9048 - val_loss: 0.6482\n",
            "Epoch 37/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9006 - loss: 0.6648 - val_accuracy: 0.9055 - val_loss: 0.6448\n",
            "Epoch 38/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8985 - loss: 0.6686 - val_accuracy: 0.9057 - val_loss: 0.6419\n",
            "Epoch 39/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9006 - loss: 0.6615 - val_accuracy: 0.9065 - val_loss: 0.6388\n",
            "Epoch 40/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9017 - loss: 0.6567 - val_accuracy: 0.9077 - val_loss: 0.6357\n",
            "Epoch 41/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9030 - loss: 0.6539 - val_accuracy: 0.9078 - val_loss: 0.6328\n",
            "Epoch 42/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9055 - loss: 0.6491 - val_accuracy: 0.9088 - val_loss: 0.6300\n",
            "Epoch 43/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9061 - loss: 0.6473 - val_accuracy: 0.9091 - val_loss: 0.6275\n",
            "Epoch 44/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9047 - loss: 0.6443 - val_accuracy: 0.9101 - val_loss: 0.6248\n",
            "Epoch 45/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9082 - loss: 0.6368 - val_accuracy: 0.9096 - val_loss: 0.6225\n",
            "Epoch 46/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9053 - loss: 0.6395 - val_accuracy: 0.9100 - val_loss: 0.6198\n",
            "Epoch 47/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9085 - loss: 0.6333 - val_accuracy: 0.9113 - val_loss: 0.6175\n",
            "Epoch 48/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9092 - loss: 0.6297 - val_accuracy: 0.9111 - val_loss: 0.6151\n",
            "Epoch 49/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9064 - loss: 0.6343 - val_accuracy: 0.9121 - val_loss: 0.6129\n",
            "Epoch 50/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9087 - loss: 0.6284 - val_accuracy: 0.9126 - val_loss: 0.6108\n",
            "Epoch 51/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9114 - loss: 0.6246 - val_accuracy: 0.9129 - val_loss: 0.6086\n",
            "Epoch 52/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9120 - loss: 0.6180 - val_accuracy: 0.9137 - val_loss: 0.6065\n",
            "Epoch 53/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9138 - loss: 0.6139 - val_accuracy: 0.9145 - val_loss: 0.6045\n",
            "Epoch 54/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9124 - loss: 0.6188 - val_accuracy: 0.9145 - val_loss: 0.6025\n",
            "Epoch 55/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9109 - loss: 0.6215 - val_accuracy: 0.9147 - val_loss: 0.6007\n",
            "Epoch 56/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9130 - loss: 0.6138 - val_accuracy: 0.9155 - val_loss: 0.5987\n",
            "Epoch 57/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9132 - loss: 0.6127 - val_accuracy: 0.9153 - val_loss: 0.5969\n",
            "Epoch 58/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9119 - loss: 0.6116 - val_accuracy: 0.9159 - val_loss: 0.5950\n",
            "Epoch 59/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9123 - loss: 0.6099 - val_accuracy: 0.9171 - val_loss: 0.5932\n",
            "Epoch 60/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9130 - loss: 0.6082 - val_accuracy: 0.9168 - val_loss: 0.5915\n",
            "Epoch 61/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9141 - loss: 0.6032 - val_accuracy: 0.9172 - val_loss: 0.5898\n",
            "Epoch 62/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9147 - loss: 0.6015 - val_accuracy: 0.9183 - val_loss: 0.5881\n",
            "Epoch 63/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9169 - loss: 0.5959 - val_accuracy: 0.9182 - val_loss: 0.5863\n",
            "Epoch 64/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9165 - loss: 0.5974 - val_accuracy: 0.9187 - val_loss: 0.5848\n",
            "Epoch 65/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9189 - loss: 0.5906 - val_accuracy: 0.9188 - val_loss: 0.5830\n",
            "Epoch 66/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9164 - loss: 0.5908 - val_accuracy: 0.9191 - val_loss: 0.5814\n",
            "Epoch 67/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9175 - loss: 0.5977 - val_accuracy: 0.9189 - val_loss: 0.5799\n",
            "Epoch 68/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.5916 - val_accuracy: 0.9203 - val_loss: 0.5784\n",
            "Epoch 69/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9180 - loss: 0.5870 - val_accuracy: 0.9196 - val_loss: 0.5768\n",
            "Epoch 70/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9207 - loss: 0.5857 - val_accuracy: 0.9205 - val_loss: 0.5754\n",
            "Epoch 71/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9206 - loss: 0.5813 - val_accuracy: 0.9215 - val_loss: 0.5739\n",
            "Epoch 72/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.5907 - val_accuracy: 0.9212 - val_loss: 0.5723\n",
            "Epoch 73/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9205 - loss: 0.5802 - val_accuracy: 0.9214 - val_loss: 0.5709\n",
            "Epoch 74/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9217 - loss: 0.5741 - val_accuracy: 0.9216 - val_loss: 0.5694\n",
            "Epoch 75/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9198 - loss: 0.5768 - val_accuracy: 0.9222 - val_loss: 0.5680\n",
            "Epoch 76/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9201 - loss: 0.5796 - val_accuracy: 0.9223 - val_loss: 0.5667\n",
            "Epoch 77/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9195 - loss: 0.5758 - val_accuracy: 0.9231 - val_loss: 0.5653\n",
            "Epoch 78/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9219 - loss: 0.5721 - val_accuracy: 0.9231 - val_loss: 0.5639\n",
            "Epoch 79/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9193 - loss: 0.5783 - val_accuracy: 0.9235 - val_loss: 0.5625\n",
            "Epoch 80/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9186 - loss: 0.5759 - val_accuracy: 0.9239 - val_loss: 0.5612\n",
            "Epoch 81/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9220 - loss: 0.5695 - val_accuracy: 0.9241 - val_loss: 0.5599\n",
            "Epoch 82/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9234 - loss: 0.5694 - val_accuracy: 0.9243 - val_loss: 0.5585\n",
            "Epoch 83/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9223 - loss: 0.5688 - val_accuracy: 0.9244 - val_loss: 0.5573\n",
            "Epoch 84/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9240 - loss: 0.5652 - val_accuracy: 0.9252 - val_loss: 0.5559\n",
            "Epoch 85/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9215 - loss: 0.5706 - val_accuracy: 0.9245 - val_loss: 0.5547\n",
            "Epoch 86/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9240 - loss: 0.5674 - val_accuracy: 0.9258 - val_loss: 0.5534\n",
            "Epoch 87/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9236 - loss: 0.5676 - val_accuracy: 0.9254 - val_loss: 0.5522\n",
            "Epoch 88/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9248 - loss: 0.5552 - val_accuracy: 0.9261 - val_loss: 0.5510\n",
            "Epoch 89/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9250 - loss: 0.5578 - val_accuracy: 0.9264 - val_loss: 0.5497\n",
            "Epoch 90/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9248 - loss: 0.5611 - val_accuracy: 0.9262 - val_loss: 0.5485\n",
            "Epoch 91/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9261 - loss: 0.5536 - val_accuracy: 0.9266 - val_loss: 0.5473\n",
            "Epoch 92/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9258 - loss: 0.5522 - val_accuracy: 0.9269 - val_loss: 0.5460\n",
            "Epoch 93/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9250 - loss: 0.5558 - val_accuracy: 0.9273 - val_loss: 0.5449\n",
            "Epoch 94/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9274 - loss: 0.5529 - val_accuracy: 0.9277 - val_loss: 0.5438\n",
            "Epoch 95/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9267 - loss: 0.5480 - val_accuracy: 0.9277 - val_loss: 0.5426\n",
            "Epoch 96/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9267 - loss: 0.5468 - val_accuracy: 0.9276 - val_loss: 0.5414\n",
            "Epoch 97/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9264 - loss: 0.5501 - val_accuracy: 0.9285 - val_loss: 0.5402\n",
            "Epoch 98/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9277 - loss: 0.5440 - val_accuracy: 0.9287 - val_loss: 0.5391\n",
            "Epoch 99/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9278 - loss: 0.5472 - val_accuracy: 0.9287 - val_loss: 0.5379\n",
            "Epoch 100/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9267 - loss: 0.5428 - val_accuracy: 0.9290 - val_loss: 0.5368\n",
            "Epoch 101/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9280 - loss: 0.5415 - val_accuracy: 0.9293 - val_loss: 0.5356\n",
            "Epoch 102/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9284 - loss: 0.5425 - val_accuracy: 0.9293 - val_loss: 0.5346\n",
            "Epoch 103/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9292 - loss: 0.5383 - val_accuracy: 0.9297 - val_loss: 0.5334\n",
            "Epoch 104/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9282 - loss: 0.5401 - val_accuracy: 0.9301 - val_loss: 0.5324\n",
            "Epoch 105/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9298 - loss: 0.5355 - val_accuracy: 0.9297 - val_loss: 0.5312\n",
            "Epoch 106/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9272 - loss: 0.5418 - val_accuracy: 0.9303 - val_loss: 0.5302\n",
            "Epoch 107/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9288 - loss: 0.5361 - val_accuracy: 0.9304 - val_loss: 0.5291\n",
            "Epoch 108/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9286 - loss: 0.5376 - val_accuracy: 0.9308 - val_loss: 0.5281\n",
            "Epoch 109/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9289 - loss: 0.5366 - val_accuracy: 0.9308 - val_loss: 0.5272\n",
            "Epoch 110/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9311 - loss: 0.5288 - val_accuracy: 0.9314 - val_loss: 0.5260\n",
            "Epoch 111/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9299 - loss: 0.5351 - val_accuracy: 0.9314 - val_loss: 0.5250\n",
            "Epoch 112/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9313 - loss: 0.5299 - val_accuracy: 0.9318 - val_loss: 0.5240\n",
            "Epoch 113/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9325 - loss: 0.5234 - val_accuracy: 0.9315 - val_loss: 0.5230\n",
            "Epoch 114/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9314 - loss: 0.5268 - val_accuracy: 0.9324 - val_loss: 0.5220\n",
            "Epoch 115/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9297 - loss: 0.5290 - val_accuracy: 0.9327 - val_loss: 0.5209\n",
            "Epoch 116/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9310 - loss: 0.5241 - val_accuracy: 0.9330 - val_loss: 0.5199\n",
            "Epoch 117/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9332 - loss: 0.5217 - val_accuracy: 0.9325 - val_loss: 0.5190\n",
            "Epoch 118/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9328 - loss: 0.5197 - val_accuracy: 0.9333 - val_loss: 0.5179\n",
            "Epoch 119/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9316 - loss: 0.5294 - val_accuracy: 0.9333 - val_loss: 0.5169\n",
            "Epoch 120/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9338 - loss: 0.5184 - val_accuracy: 0.9338 - val_loss: 0.5159\n",
            "Epoch 121/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9314 - loss: 0.5238 - val_accuracy: 0.9345 - val_loss: 0.5149\n",
            "Epoch 122/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9314 - loss: 0.5225 - val_accuracy: 0.9341 - val_loss: 0.5140\n",
            "Epoch 123/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9337 - loss: 0.5167 - val_accuracy: 0.9343 - val_loss: 0.5131\n",
            "Epoch 124/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9320 - loss: 0.5255 - val_accuracy: 0.9347 - val_loss: 0.5122\n",
            "Epoch 125/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9333 - loss: 0.5184 - val_accuracy: 0.9347 - val_loss: 0.5112\n",
            "Epoch 126/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9345 - loss: 0.5171 - val_accuracy: 0.9345 - val_loss: 0.5103\n",
            "Epoch 127/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9333 - loss: 0.5161 - val_accuracy: 0.9352 - val_loss: 0.5093\n",
            "Epoch 128/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9339 - loss: 0.5128 - val_accuracy: 0.9353 - val_loss: 0.5084\n",
            "Epoch 129/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9365 - loss: 0.5121 - val_accuracy: 0.9352 - val_loss: 0.5075\n",
            "Epoch 130/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9326 - loss: 0.5176 - val_accuracy: 0.9359 - val_loss: 0.5066\n",
            "Epoch 131/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9344 - loss: 0.5099 - val_accuracy: 0.9358 - val_loss: 0.5057\n",
            "Epoch 132/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9352 - loss: 0.5106 - val_accuracy: 0.9359 - val_loss: 0.5048\n",
            "Epoch 133/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9359 - loss: 0.5105 - val_accuracy: 0.9362 - val_loss: 0.5038\n",
            "Epoch 134/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9360 - loss: 0.5040 - val_accuracy: 0.9360 - val_loss: 0.5029\n",
            "Epoch 135/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9351 - loss: 0.5085 - val_accuracy: 0.9366 - val_loss: 0.5021\n",
            "Epoch 136/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9378 - loss: 0.5027 - val_accuracy: 0.9365 - val_loss: 0.5011\n",
            "Epoch 137/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9362 - loss: 0.5017 - val_accuracy: 0.9366 - val_loss: 0.5003\n",
            "Epoch 138/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9371 - loss: 0.5003 - val_accuracy: 0.9365 - val_loss: 0.4995\n",
            "Epoch 139/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9374 - loss: 0.4970 - val_accuracy: 0.9373 - val_loss: 0.4986\n",
            "Epoch 140/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9378 - loss: 0.5008 - val_accuracy: 0.9373 - val_loss: 0.4977\n",
            "Epoch 141/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9388 - loss: 0.4938 - val_accuracy: 0.9376 - val_loss: 0.4967\n",
            "Epoch 142/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9379 - loss: 0.5012 - val_accuracy: 0.9377 - val_loss: 0.4959\n",
            "Epoch 143/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9362 - loss: 0.4985 - val_accuracy: 0.9379 - val_loss: 0.4950\n",
            "Epoch 144/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9359 - loss: 0.4985 - val_accuracy: 0.9377 - val_loss: 0.4942\n",
            "Epoch 145/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9371 - loss: 0.4985 - val_accuracy: 0.9385 - val_loss: 0.4934\n",
            "Epoch 146/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9369 - loss: 0.5026 - val_accuracy: 0.9386 - val_loss: 0.4926\n",
            "Epoch 147/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9372 - loss: 0.4946 - val_accuracy: 0.9388 - val_loss: 0.4917\n",
            "Epoch 148/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9376 - loss: 0.4933 - val_accuracy: 0.9388 - val_loss: 0.4909\n",
            "Epoch 149/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9397 - loss: 0.4903 - val_accuracy: 0.9392 - val_loss: 0.4901\n",
            "Epoch 150/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9402 - loss: 0.4903 - val_accuracy: 0.9395 - val_loss: 0.4892\n",
            "Epoch 151/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9394 - loss: 0.4901 - val_accuracy: 0.9396 - val_loss: 0.4885\n",
            "Epoch 152/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9388 - loss: 0.4890 - val_accuracy: 0.9392 - val_loss: 0.4876\n",
            "Epoch 153/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9419 - loss: 0.4840 - val_accuracy: 0.9397 - val_loss: 0.4869\n",
            "Epoch 154/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9412 - loss: 0.4821 - val_accuracy: 0.9399 - val_loss: 0.4861\n",
            "Epoch 155/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9390 - loss: 0.4890 - val_accuracy: 0.9399 - val_loss: 0.4853\n",
            "Epoch 156/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9420 - loss: 0.4811 - val_accuracy: 0.9402 - val_loss: 0.4845\n",
            "Epoch 157/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9400 - loss: 0.4876 - val_accuracy: 0.9404 - val_loss: 0.4836\n",
            "Epoch 158/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9397 - loss: 0.4849 - val_accuracy: 0.9413 - val_loss: 0.4829\n",
            "Epoch 159/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9411 - loss: 0.4781 - val_accuracy: 0.9411 - val_loss: 0.4821\n",
            "Epoch 160/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9436 - loss: 0.4781 - val_accuracy: 0.9415 - val_loss: 0.4813\n",
            "Epoch 161/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9411 - loss: 0.4818 - val_accuracy: 0.9417 - val_loss: 0.4805\n",
            "Epoch 162/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9396 - loss: 0.4828 - val_accuracy: 0.9419 - val_loss: 0.4798\n",
            "Epoch 163/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9409 - loss: 0.4816 - val_accuracy: 0.9417 - val_loss: 0.4790\n",
            "Epoch 164/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9404 - loss: 0.4828 - val_accuracy: 0.9423 - val_loss: 0.4782\n",
            "Epoch 165/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9422 - loss: 0.4770 - val_accuracy: 0.9424 - val_loss: 0.4775\n",
            "Epoch 166/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9431 - loss: 0.4765 - val_accuracy: 0.9426 - val_loss: 0.4768\n",
            "Epoch 167/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9409 - loss: 0.4802 - val_accuracy: 0.9428 - val_loss: 0.4761\n",
            "Epoch 168/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9431 - loss: 0.4738 - val_accuracy: 0.9438 - val_loss: 0.4752\n",
            "Epoch 169/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9418 - loss: 0.4746 - val_accuracy: 0.9429 - val_loss: 0.4745\n",
            "Epoch 170/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9427 - loss: 0.4739 - val_accuracy: 0.9431 - val_loss: 0.4737\n",
            "Epoch 171/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9421 - loss: 0.4726 - val_accuracy: 0.9434 - val_loss: 0.4730\n",
            "Epoch 172/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9410 - loss: 0.4744 - val_accuracy: 0.9437 - val_loss: 0.4723\n",
            "Epoch 173/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9423 - loss: 0.4737 - val_accuracy: 0.9436 - val_loss: 0.4716\n",
            "Epoch 174/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9414 - loss: 0.4769 - val_accuracy: 0.9438 - val_loss: 0.4708\n",
            "Epoch 175/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9442 - loss: 0.4680 - val_accuracy: 0.9438 - val_loss: 0.4701\n",
            "Epoch 176/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9432 - loss: 0.4670 - val_accuracy: 0.9438 - val_loss: 0.4694\n",
            "Epoch 177/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9446 - loss: 0.4662 - val_accuracy: 0.9446 - val_loss: 0.4687\n",
            "Epoch 178/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9461 - loss: 0.4622 - val_accuracy: 0.9442 - val_loss: 0.4679\n",
            "Epoch 179/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9455 - loss: 0.4632 - val_accuracy: 0.9442 - val_loss: 0.4672\n",
            "Epoch 180/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9440 - loss: 0.4677 - val_accuracy: 0.9448 - val_loss: 0.4665\n",
            "Epoch 181/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9441 - loss: 0.4649 - val_accuracy: 0.9447 - val_loss: 0.4658\n",
            "Epoch 182/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9436 - loss: 0.4650 - val_accuracy: 0.9443 - val_loss: 0.4651\n",
            "Epoch 183/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9451 - loss: 0.4643 - val_accuracy: 0.9449 - val_loss: 0.4644\n",
            "Epoch 184/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9448 - loss: 0.4611 - val_accuracy: 0.9452 - val_loss: 0.4636\n",
            "Epoch 185/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9458 - loss: 0.4600 - val_accuracy: 0.9453 - val_loss: 0.4631\n",
            "Epoch 186/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9430 - loss: 0.4664 - val_accuracy: 0.9454 - val_loss: 0.4623\n",
            "Epoch 187/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9453 - loss: 0.4594 - val_accuracy: 0.9451 - val_loss: 0.4615\n",
            "Epoch 188/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9440 - loss: 0.4631 - val_accuracy: 0.9453 - val_loss: 0.4610\n",
            "Epoch 189/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9462 - loss: 0.4571 - val_accuracy: 0.9452 - val_loss: 0.4602\n",
            "Epoch 190/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9453 - loss: 0.4576 - val_accuracy: 0.9457 - val_loss: 0.4596\n",
            "Epoch 191/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9445 - loss: 0.4606 - val_accuracy: 0.9455 - val_loss: 0.4590\n",
            "Epoch 192/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9471 - loss: 0.4522 - val_accuracy: 0.9460 - val_loss: 0.4583\n",
            "Epoch 193/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9467 - loss: 0.4556 - val_accuracy: 0.9462 - val_loss: 0.4576\n",
            "Epoch 194/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9457 - loss: 0.4577 - val_accuracy: 0.9461 - val_loss: 0.4569\n",
            "Epoch 195/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9465 - loss: 0.4548 - val_accuracy: 0.9467 - val_loss: 0.4563\n",
            "Epoch 196/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9477 - loss: 0.4501 - val_accuracy: 0.9465 - val_loss: 0.4556\n",
            "Epoch 197/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9472 - loss: 0.4537 - val_accuracy: 0.9467 - val_loss: 0.4549\n",
            "Epoch 198/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9469 - loss: 0.4536 - val_accuracy: 0.9471 - val_loss: 0.4543\n",
            "Epoch 199/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9481 - loss: 0.4490 - val_accuracy: 0.9470 - val_loss: 0.4536\n",
            "Epoch 200/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9471 - loss: 0.4488 - val_accuracy: 0.9470 - val_loss: 0.4530\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.4826\n",
            "\n",
            "Model with L2 Regularization:\n",
            "Test accuracy: 0.9460  | Test loss: 0.4518\n",
            "\n",
            "Confusion Matrix (L2):\n",
            "[[ 963    0    1    1    0    5    6    2    2    0]\n",
            " [   0 1117    2    2    1    1    3    2    7    0]\n",
            " [   7    4  965   13    9    2    8   11   12    1]\n",
            " [   1    1   16  948    0   15    1   12   14    2]\n",
            " [   1    1    4    0  938    0    9    1    3   25]\n",
            " [   9    1    2   22    4  822   13    3    9    7]\n",
            " [   9    3    4    0    7   10  920    2    3    0]\n",
            " [   0    8   22    3    7    1    0  962    1   24]\n",
            " [   4    5    5   23    7   10   12   11  895    2]\n",
            " [   9    8    1   14   27    5    1   12    2  930]]\n"
          ]
        }
      ],
      "source": [
        "# dropout l2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Set hyperparameters\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "LAMBDA = 0.001 # L2 regularization factor\n",
        "\n",
        "# Load and preprocess data\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "RESHAPED = 784\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Build the neural network model with L2 regularization\n",
        "model_l2 = keras.Sequential([\n",
        "    keras.layers.Input(shape=(RESHAPED,)),\n",
        "    keras.layers.Dense(N_HIDDEN, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(LAMBDA)),\n",
        "    keras.layers.Dense(N_HIDDEN // 2, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(LAMBDA)),\n",
        "    keras.layers.Dense(NB_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model_l2.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_l2 = model_l2.fit(\n",
        "    X_train, Y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    verbose=VERBOSE,\n",
        ")\n",
        "\n",
        "test_loss_l2, test_acc_l2 = model_l2.evaluate(X_test, Y_test, verbose=1)\n",
        "print(f\"\\nModel with L2 Regularization:\")\n",
        "print(f\"Test accuracy: {test_acc_l2:.4f}  | Test loss: {test_loss_l2:.4f}\")\n",
        "\n",
        "y_true_l2 = np.argmax(Y_test, axis=1)\n",
        "y_pred_l2 = np.argmax(model_l2.predict(X_test, verbose=0), axis=1)\n",
        "cm_l2 = tf.math.confusion_matrix(y_true_l2, y_pred_l2, num_classes=NB_CLASSES).numpy()\n",
        "print(\"\\nConfusion Matrix (L2):\")\n",
        "print(cm_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zs_1A0fBB35C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs_1A0fBB35C",
        "outputId": "7cc391bf-f84a-4e8e-bd23-eede3c8a4883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.1125 - loss: 2.3340 - val_accuracy: 0.2863 - val_loss: 2.1747\n",
            "Epoch 2/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2151 - loss: 2.1902 - val_accuracy: 0.4849 - val_loss: 2.0349\n",
            "Epoch 3/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.3048 - loss: 2.0707 - val_accuracy: 0.5923 - val_loss: 1.8796\n",
            "Epoch 4/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.3907 - loss: 1.9336 - val_accuracy: 0.6561 - val_loss: 1.7086\n",
            "Epoch 5/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4507 - loss: 1.7939 - val_accuracy: 0.6973 - val_loss: 1.5341\n",
            "Epoch 6/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5004 - loss: 1.6602 - val_accuracy: 0.7303 - val_loss: 1.3727\n",
            "Epoch 7/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5346 - loss: 1.5403 - val_accuracy: 0.7511 - val_loss: 1.2336\n",
            "Epoch 8/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5673 - loss: 1.4280 - val_accuracy: 0.7713 - val_loss: 1.1160\n",
            "Epoch 9/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5988 - loss: 1.3280 - val_accuracy: 0.7883 - val_loss: 1.0165\n",
            "Epoch 10/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6179 - loss: 1.2513 - val_accuracy: 0.8011 - val_loss: 0.9342\n",
            "Epoch 11/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6316 - loss: 1.1912 - val_accuracy: 0.8123 - val_loss: 0.8669\n",
            "Epoch 12/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6606 - loss: 1.1160 - val_accuracy: 0.8238 - val_loss: 0.8088\n",
            "Epoch 13/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6701 - loss: 1.0748 - val_accuracy: 0.8313 - val_loss: 0.7607\n",
            "Epoch 14/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6861 - loss: 1.0222 - val_accuracy: 0.8364 - val_loss: 0.7193\n",
            "Epoch 15/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6971 - loss: 0.9863 - val_accuracy: 0.8422 - val_loss: 0.6837\n",
            "Epoch 16/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7072 - loss: 0.9510 - val_accuracy: 0.8487 - val_loss: 0.6525\n",
            "Epoch 17/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7165 - loss: 0.9172 - val_accuracy: 0.8533 - val_loss: 0.6256\n",
            "Epoch 18/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7253 - loss: 0.8929 - val_accuracy: 0.8577 - val_loss: 0.6012\n",
            "Epoch 19/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7368 - loss: 0.8603 - val_accuracy: 0.8605 - val_loss: 0.5808\n",
            "Epoch 20/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7367 - loss: 0.8450 - val_accuracy: 0.8639 - val_loss: 0.5614\n",
            "Epoch 21/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7483 - loss: 0.8158 - val_accuracy: 0.8660 - val_loss: 0.5440\n",
            "Epoch 22/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7501 - loss: 0.8059 - val_accuracy: 0.8683 - val_loss: 0.5294\n",
            "Epoch 23/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7580 - loss: 0.7850 - val_accuracy: 0.8708 - val_loss: 0.5156\n",
            "Epoch 24/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7626 - loss: 0.7670 - val_accuracy: 0.8732 - val_loss: 0.5036\n",
            "Epoch 25/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7705 - loss: 0.7509 - val_accuracy: 0.8758 - val_loss: 0.4916\n",
            "Epoch 26/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7750 - loss: 0.7334 - val_accuracy: 0.8778 - val_loss: 0.4806\n",
            "Epoch 27/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7755 - loss: 0.7336 - val_accuracy: 0.8790 - val_loss: 0.4715\n",
            "Epoch 28/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7836 - loss: 0.7114 - val_accuracy: 0.8817 - val_loss: 0.4624\n",
            "Epoch 29/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7802 - loss: 0.7036 - val_accuracy: 0.8833 - val_loss: 0.4537\n",
            "Epoch 30/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7929 - loss: 0.6853 - val_accuracy: 0.8848 - val_loss: 0.4461\n",
            "Epoch 31/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7906 - loss: 0.6783 - val_accuracy: 0.8857 - val_loss: 0.4385\n",
            "Epoch 32/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8018 - loss: 0.6578 - val_accuracy: 0.8873 - val_loss: 0.4308\n",
            "Epoch 33/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7963 - loss: 0.6633 - val_accuracy: 0.8884 - val_loss: 0.4248\n",
            "Epoch 34/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8003 - loss: 0.6517 - val_accuracy: 0.8892 - val_loss: 0.4193\n",
            "Epoch 35/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8070 - loss: 0.6410 - val_accuracy: 0.8898 - val_loss: 0.4138\n",
            "Epoch 36/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8010 - loss: 0.6407 - val_accuracy: 0.8912 - val_loss: 0.4083\n",
            "Epoch 37/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8100 - loss: 0.6249 - val_accuracy: 0.8925 - val_loss: 0.4026\n",
            "Epoch 38/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8157 - loss: 0.6129 - val_accuracy: 0.8926 - val_loss: 0.3979\n",
            "Epoch 39/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8168 - loss: 0.6035 - val_accuracy: 0.8936 - val_loss: 0.3933\n",
            "Epoch 40/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8142 - loss: 0.6059 - val_accuracy: 0.8947 - val_loss: 0.3892\n",
            "Epoch 41/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8183 - loss: 0.5999 - val_accuracy: 0.8956 - val_loss: 0.3850\n",
            "Epoch 42/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8221 - loss: 0.5888 - val_accuracy: 0.8964 - val_loss: 0.3809\n",
            "Epoch 43/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8235 - loss: 0.5853 - val_accuracy: 0.8968 - val_loss: 0.3771\n",
            "Epoch 44/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8258 - loss: 0.5747 - val_accuracy: 0.8977 - val_loss: 0.3737\n",
            "Epoch 45/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8256 - loss: 0.5782 - val_accuracy: 0.8983 - val_loss: 0.3702\n",
            "Epoch 46/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8312 - loss: 0.5697 - val_accuracy: 0.8990 - val_loss: 0.3668\n",
            "Epoch 47/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.8306 - loss: 0.5663 - val_accuracy: 0.8997 - val_loss: 0.3636\n",
            "Epoch 48/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8346 - loss: 0.5522 - val_accuracy: 0.9002 - val_loss: 0.3602\n",
            "Epoch 49/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8337 - loss: 0.5551 - val_accuracy: 0.9016 - val_loss: 0.3572\n",
            "Epoch 50/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8332 - loss: 0.5610 - val_accuracy: 0.9014 - val_loss: 0.3544\n",
            "Epoch 51/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8378 - loss: 0.5433 - val_accuracy: 0.9025 - val_loss: 0.3517\n",
            "Epoch 52/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8381 - loss: 0.5486 - val_accuracy: 0.9032 - val_loss: 0.3489\n",
            "Epoch 53/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8389 - loss: 0.5379 - val_accuracy: 0.9033 - val_loss: 0.3461\n",
            "Epoch 54/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8408 - loss: 0.5313 - val_accuracy: 0.9042 - val_loss: 0.3432\n",
            "Epoch 55/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8394 - loss: 0.5349 - val_accuracy: 0.9046 - val_loss: 0.3408\n",
            "Epoch 56/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8390 - loss: 0.5277 - val_accuracy: 0.9051 - val_loss: 0.3384\n",
            "Epoch 57/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8446 - loss: 0.5188 - val_accuracy: 0.9057 - val_loss: 0.3358\n",
            "Epoch 58/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8456 - loss: 0.5236 - val_accuracy: 0.9059 - val_loss: 0.3336\n",
            "Epoch 59/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8483 - loss: 0.5036 - val_accuracy: 0.9068 - val_loss: 0.3313\n",
            "Epoch 60/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8489 - loss: 0.5070 - val_accuracy: 0.9068 - val_loss: 0.3294\n",
            "Epoch 61/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8483 - loss: 0.5077 - val_accuracy: 0.9072 - val_loss: 0.3270\n",
            "Epoch 62/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8541 - loss: 0.4988 - val_accuracy: 0.9081 - val_loss: 0.3248\n",
            "Epoch 63/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8550 - loss: 0.4891 - val_accuracy: 0.9084 - val_loss: 0.3224\n",
            "Epoch 64/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8544 - loss: 0.4917 - val_accuracy: 0.9091 - val_loss: 0.3206\n",
            "Epoch 65/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8549 - loss: 0.4963 - val_accuracy: 0.9093 - val_loss: 0.3187\n",
            "Epoch 66/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8575 - loss: 0.4821 - val_accuracy: 0.9098 - val_loss: 0.3169\n",
            "Epoch 67/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8543 - loss: 0.4928 - val_accuracy: 0.9105 - val_loss: 0.3151\n",
            "Epoch 68/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8548 - loss: 0.4854 - val_accuracy: 0.9110 - val_loss: 0.3132\n",
            "Epoch 69/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8578 - loss: 0.4843 - val_accuracy: 0.9118 - val_loss: 0.3117\n",
            "Epoch 70/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8616 - loss: 0.4727 - val_accuracy: 0.9124 - val_loss: 0.3101\n",
            "Epoch 71/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8626 - loss: 0.4710 - val_accuracy: 0.9125 - val_loss: 0.3083\n",
            "Epoch 72/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8614 - loss: 0.4715 - val_accuracy: 0.9130 - val_loss: 0.3065\n",
            "Epoch 73/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8622 - loss: 0.4684 - val_accuracy: 0.9134 - val_loss: 0.3048\n",
            "Epoch 74/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8614 - loss: 0.4694 - val_accuracy: 0.9137 - val_loss: 0.3034\n",
            "Epoch 75/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8604 - loss: 0.4700 - val_accuracy: 0.9139 - val_loss: 0.3018\n",
            "Epoch 76/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8649 - loss: 0.4578 - val_accuracy: 0.9143 - val_loss: 0.3001\n",
            "Epoch 77/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8662 - loss: 0.4609 - val_accuracy: 0.9147 - val_loss: 0.2984\n",
            "Epoch 78/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8624 - loss: 0.4629 - val_accuracy: 0.9149 - val_loss: 0.2971\n",
            "Epoch 79/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8650 - loss: 0.4577 - val_accuracy: 0.9152 - val_loss: 0.2957\n",
            "Epoch 80/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8661 - loss: 0.4521 - val_accuracy: 0.9153 - val_loss: 0.2944\n",
            "Epoch 81/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8698 - loss: 0.4453 - val_accuracy: 0.9159 - val_loss: 0.2925\n",
            "Epoch 82/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8683 - loss: 0.4502 - val_accuracy: 0.9162 - val_loss: 0.2914\n",
            "Epoch 83/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8676 - loss: 0.4478 - val_accuracy: 0.9170 - val_loss: 0.2899\n",
            "Epoch 84/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8697 - loss: 0.4444 - val_accuracy: 0.9172 - val_loss: 0.2884\n",
            "Epoch 85/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8732 - loss: 0.4360 - val_accuracy: 0.9178 - val_loss: 0.2869\n",
            "Epoch 86/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8700 - loss: 0.4426 - val_accuracy: 0.9181 - val_loss: 0.2858\n",
            "Epoch 87/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8719 - loss: 0.4384 - val_accuracy: 0.9181 - val_loss: 0.2845\n",
            "Epoch 88/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8731 - loss: 0.4392 - val_accuracy: 0.9183 - val_loss: 0.2833\n",
            "Epoch 89/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8729 - loss: 0.4329 - val_accuracy: 0.9187 - val_loss: 0.2818\n",
            "Epoch 90/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8764 - loss: 0.4234 - val_accuracy: 0.9193 - val_loss: 0.2809\n",
            "Epoch 91/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8744 - loss: 0.4259 - val_accuracy: 0.9196 - val_loss: 0.2796\n",
            "Epoch 92/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8724 - loss: 0.4308 - val_accuracy: 0.9202 - val_loss: 0.2784\n",
            "Epoch 93/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8752 - loss: 0.4312 - val_accuracy: 0.9202 - val_loss: 0.2772\n",
            "Epoch 94/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8772 - loss: 0.4179 - val_accuracy: 0.9203 - val_loss: 0.2762\n",
            "Epoch 95/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8760 - loss: 0.4283 - val_accuracy: 0.9204 - val_loss: 0.2747\n",
            "Epoch 96/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8752 - loss: 0.4220 - val_accuracy: 0.9208 - val_loss: 0.2736\n",
            "Epoch 97/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8762 - loss: 0.4218 - val_accuracy: 0.9208 - val_loss: 0.2727\n",
            "Epoch 98/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8759 - loss: 0.4155 - val_accuracy: 0.9212 - val_loss: 0.2717\n",
            "Epoch 99/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8809 - loss: 0.4077 - val_accuracy: 0.9213 - val_loss: 0.2703\n",
            "Epoch 100/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8788 - loss: 0.4089 - val_accuracy: 0.9218 - val_loss: 0.2694\n",
            "Epoch 101/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8782 - loss: 0.4166 - val_accuracy: 0.9221 - val_loss: 0.2685\n",
            "Epoch 102/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8783 - loss: 0.4072 - val_accuracy: 0.9220 - val_loss: 0.2676\n",
            "Epoch 103/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8803 - loss: 0.4060 - val_accuracy: 0.9222 - val_loss: 0.2665\n",
            "Epoch 104/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8809 - loss: 0.4071 - val_accuracy: 0.9227 - val_loss: 0.2653\n",
            "Epoch 105/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8773 - loss: 0.4127 - val_accuracy: 0.9226 - val_loss: 0.2645\n",
            "Epoch 106/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8797 - loss: 0.4064 - val_accuracy: 0.9231 - val_loss: 0.2635\n",
            "Epoch 107/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8835 - loss: 0.3959 - val_accuracy: 0.9227 - val_loss: 0.2625\n",
            "Epoch 108/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8832 - loss: 0.3993 - val_accuracy: 0.9230 - val_loss: 0.2617\n",
            "Epoch 109/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8815 - loss: 0.4009 - val_accuracy: 0.9230 - val_loss: 0.2609\n",
            "Epoch 110/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8824 - loss: 0.4042 - val_accuracy: 0.9234 - val_loss: 0.2598\n",
            "Epoch 111/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8851 - loss: 0.3914 - val_accuracy: 0.9243 - val_loss: 0.2588\n",
            "Epoch 112/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8844 - loss: 0.3940 - val_accuracy: 0.9243 - val_loss: 0.2578\n",
            "Epoch 113/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8903 - loss: 0.3885 - val_accuracy: 0.9244 - val_loss: 0.2571\n",
            "Epoch 114/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8857 - loss: 0.3919 - val_accuracy: 0.9245 - val_loss: 0.2561\n",
            "Epoch 115/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8847 - loss: 0.3939 - val_accuracy: 0.9248 - val_loss: 0.2553\n",
            "Epoch 116/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8845 - loss: 0.3941 - val_accuracy: 0.9247 - val_loss: 0.2545\n",
            "Epoch 117/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8856 - loss: 0.3870 - val_accuracy: 0.9250 - val_loss: 0.2536\n",
            "Epoch 118/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8896 - loss: 0.3772 - val_accuracy: 0.9250 - val_loss: 0.2526\n",
            "Epoch 119/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8879 - loss: 0.3832 - val_accuracy: 0.9254 - val_loss: 0.2517\n",
            "Epoch 120/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8887 - loss: 0.3783 - val_accuracy: 0.9258 - val_loss: 0.2508\n",
            "Epoch 121/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8878 - loss: 0.3797 - val_accuracy: 0.9263 - val_loss: 0.2499\n",
            "Epoch 122/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8883 - loss: 0.3826 - val_accuracy: 0.9264 - val_loss: 0.2492\n",
            "Epoch 123/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8906 - loss: 0.3772 - val_accuracy: 0.9267 - val_loss: 0.2483\n",
            "Epoch 124/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8880 - loss: 0.3815 - val_accuracy: 0.9270 - val_loss: 0.2476\n",
            "Epoch 125/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8910 - loss: 0.3765 - val_accuracy: 0.9276 - val_loss: 0.2467\n",
            "Epoch 126/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8907 - loss: 0.3748 - val_accuracy: 0.9280 - val_loss: 0.2460\n",
            "Epoch 127/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8935 - loss: 0.3701 - val_accuracy: 0.9280 - val_loss: 0.2451\n",
            "Epoch 128/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8937 - loss: 0.3677 - val_accuracy: 0.9280 - val_loss: 0.2444\n",
            "Epoch 129/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8913 - loss: 0.3735 - val_accuracy: 0.9287 - val_loss: 0.2435\n",
            "Epoch 130/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8953 - loss: 0.3664 - val_accuracy: 0.9288 - val_loss: 0.2427\n",
            "Epoch 131/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8913 - loss: 0.3667 - val_accuracy: 0.9288 - val_loss: 0.2420\n",
            "Epoch 132/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8931 - loss: 0.3675 - val_accuracy: 0.9290 - val_loss: 0.2414\n",
            "Epoch 133/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8926 - loss: 0.3714 - val_accuracy: 0.9298 - val_loss: 0.2409\n",
            "Epoch 134/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8961 - loss: 0.3615 - val_accuracy: 0.9300 - val_loss: 0.2399\n",
            "Epoch 135/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8917 - loss: 0.3735 - val_accuracy: 0.9298 - val_loss: 0.2391\n",
            "Epoch 136/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8935 - loss: 0.3660 - val_accuracy: 0.9304 - val_loss: 0.2386\n",
            "Epoch 137/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8944 - loss: 0.3654 - val_accuracy: 0.9301 - val_loss: 0.2378\n",
            "Epoch 138/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8971 - loss: 0.3521 - val_accuracy: 0.9306 - val_loss: 0.2371\n",
            "Epoch 139/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8979 - loss: 0.3539 - val_accuracy: 0.9306 - val_loss: 0.2366\n",
            "Epoch 140/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8949 - loss: 0.3588 - val_accuracy: 0.9304 - val_loss: 0.2359\n",
            "Epoch 141/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8970 - loss: 0.3602 - val_accuracy: 0.9307 - val_loss: 0.2349\n",
            "Epoch 142/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8965 - loss: 0.3553 - val_accuracy: 0.9308 - val_loss: 0.2343\n",
            "Epoch 143/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8961 - loss: 0.3572 - val_accuracy: 0.9311 - val_loss: 0.2335\n",
            "Epoch 144/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8995 - loss: 0.3486 - val_accuracy: 0.9308 - val_loss: 0.2329\n",
            "Epoch 145/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8967 - loss: 0.3513 - val_accuracy: 0.9310 - val_loss: 0.2324\n",
            "Epoch 146/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8971 - loss: 0.3537 - val_accuracy: 0.9311 - val_loss: 0.2319\n",
            "Epoch 147/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8990 - loss: 0.3493 - val_accuracy: 0.9314 - val_loss: 0.2310\n",
            "Epoch 148/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9016 - loss: 0.3404 - val_accuracy: 0.9317 - val_loss: 0.2305\n",
            "Epoch 149/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8997 - loss: 0.3463 - val_accuracy: 0.9315 - val_loss: 0.2299\n",
            "Epoch 150/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8996 - loss: 0.3509 - val_accuracy: 0.9314 - val_loss: 0.2291\n",
            "Epoch 151/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9019 - loss: 0.3416 - val_accuracy: 0.9317 - val_loss: 0.2284\n",
            "Epoch 152/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8990 - loss: 0.3481 - val_accuracy: 0.9319 - val_loss: 0.2277\n",
            "Epoch 153/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8978 - loss: 0.3466 - val_accuracy: 0.9321 - val_loss: 0.2271\n",
            "Epoch 154/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9009 - loss: 0.3439 - val_accuracy: 0.9321 - val_loss: 0.2266\n",
            "Epoch 155/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9008 - loss: 0.3416 - val_accuracy: 0.9322 - val_loss: 0.2259\n",
            "Epoch 156/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9013 - loss: 0.3420 - val_accuracy: 0.9326 - val_loss: 0.2253\n",
            "Epoch 157/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9012 - loss: 0.3427 - val_accuracy: 0.9328 - val_loss: 0.2248\n",
            "Epoch 158/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9026 - loss: 0.3367 - val_accuracy: 0.9327 - val_loss: 0.2240\n",
            "Epoch 159/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9012 - loss: 0.3346 - val_accuracy: 0.9331 - val_loss: 0.2233\n",
            "Epoch 160/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9001 - loss: 0.3438 - val_accuracy: 0.9333 - val_loss: 0.2227\n",
            "Epoch 161/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8991 - loss: 0.3433 - val_accuracy: 0.9337 - val_loss: 0.2223\n",
            "Epoch 162/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9004 - loss: 0.3387 - val_accuracy: 0.9337 - val_loss: 0.2218\n",
            "Epoch 163/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9044 - loss: 0.3345 - val_accuracy: 0.9342 - val_loss: 0.2214\n",
            "Epoch 164/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9000 - loss: 0.3413 - val_accuracy: 0.9340 - val_loss: 0.2207\n",
            "Epoch 165/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9008 - loss: 0.3386 - val_accuracy: 0.9342 - val_loss: 0.2201\n",
            "Epoch 166/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9052 - loss: 0.3319 - val_accuracy: 0.9340 - val_loss: 0.2195\n",
            "Epoch 167/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9032 - loss: 0.3299 - val_accuracy: 0.9349 - val_loss: 0.2189\n",
            "Epoch 168/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9039 - loss: 0.3272 - val_accuracy: 0.9347 - val_loss: 0.2183\n",
            "Epoch 169/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9042 - loss: 0.3299 - val_accuracy: 0.9348 - val_loss: 0.2180\n",
            "Epoch 170/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9011 - loss: 0.3353 - val_accuracy: 0.9352 - val_loss: 0.2173\n",
            "Epoch 171/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9045 - loss: 0.3302 - val_accuracy: 0.9353 - val_loss: 0.2166\n",
            "Epoch 172/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9042 - loss: 0.3291 - val_accuracy: 0.9356 - val_loss: 0.2163\n",
            "Epoch 173/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9059 - loss: 0.3217 - val_accuracy: 0.9358 - val_loss: 0.2156\n",
            "Epoch 174/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9036 - loss: 0.3330 - val_accuracy: 0.9359 - val_loss: 0.2151\n",
            "Epoch 175/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9075 - loss: 0.3180 - val_accuracy: 0.9362 - val_loss: 0.2145\n",
            "Epoch 176/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9035 - loss: 0.3272 - val_accuracy: 0.9364 - val_loss: 0.2140\n",
            "Epoch 177/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9068 - loss: 0.3242 - val_accuracy: 0.9367 - val_loss: 0.2137\n",
            "Epoch 178/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9048 - loss: 0.3268 - val_accuracy: 0.9365 - val_loss: 0.2131\n",
            "Epoch 179/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9044 - loss: 0.3260 - val_accuracy: 0.9371 - val_loss: 0.2125\n",
            "Epoch 180/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9069 - loss: 0.3201 - val_accuracy: 0.9372 - val_loss: 0.2120\n",
            "Epoch 181/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9083 - loss: 0.3168 - val_accuracy: 0.9372 - val_loss: 0.2115\n",
            "Epoch 182/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9086 - loss: 0.3161 - val_accuracy: 0.9375 - val_loss: 0.2111\n",
            "Epoch 183/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9070 - loss: 0.3212 - val_accuracy: 0.9379 - val_loss: 0.2102\n",
            "Epoch 184/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9073 - loss: 0.3146 - val_accuracy: 0.9375 - val_loss: 0.2099\n",
            "Epoch 185/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9092 - loss: 0.3130 - val_accuracy: 0.9376 - val_loss: 0.2098\n",
            "Epoch 186/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9082 - loss: 0.3191 - val_accuracy: 0.9377 - val_loss: 0.2092\n",
            "Epoch 187/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9096 - loss: 0.3115 - val_accuracy: 0.9385 - val_loss: 0.2085\n",
            "Epoch 188/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9079 - loss: 0.3134 - val_accuracy: 0.9385 - val_loss: 0.2079\n",
            "Epoch 189/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9080 - loss: 0.3169 - val_accuracy: 0.9386 - val_loss: 0.2076\n",
            "Epoch 190/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9072 - loss: 0.3145 - val_accuracy: 0.9392 - val_loss: 0.2071\n",
            "Epoch 191/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9112 - loss: 0.3035 - val_accuracy: 0.9391 - val_loss: 0.2066\n",
            "Epoch 192/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9081 - loss: 0.3140 - val_accuracy: 0.9390 - val_loss: 0.2060\n",
            "Epoch 193/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9109 - loss: 0.3060 - val_accuracy: 0.9391 - val_loss: 0.2056\n",
            "Epoch 194/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9084 - loss: 0.3143 - val_accuracy: 0.9396 - val_loss: 0.2051\n",
            "Epoch 195/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9084 - loss: 0.3098 - val_accuracy: 0.9399 - val_loss: 0.2047\n",
            "Epoch 196/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9082 - loss: 0.3157 - val_accuracy: 0.9399 - val_loss: 0.2042\n",
            "Epoch 197/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9081 - loss: 0.3103 - val_accuracy: 0.9401 - val_loss: 0.2038\n",
            "Epoch 198/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9133 - loss: 0.2978 - val_accuracy: 0.9402 - val_loss: 0.2035\n",
            "Epoch 199/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9111 - loss: 0.3055 - val_accuracy: 0.9411 - val_loss: 0.2028\n",
            "Epoch 200/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9107 - loss: 0.3123 - val_accuracy: 0.9408 - val_loss: 0.2023\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9294 - loss: 0.2325\n",
            "\n",
            "Model with Dropout Regularization:\n",
            "Test accuracy: 0.9388  | Test loss: 0.2018\n",
            "\n",
            "Confusion Matrix (Dropout):\n",
            "[[ 963    0    1    1    0    4    8    1    2    0]\n",
            " [   0 1113    3    2    1    0    3    2   11    0]\n",
            " [  10    5  950   10   11    0   12   11   19    4]\n",
            " [   2    0   21  936    0   19    2   14   11    5]\n",
            " [   1    2    4    0  919    0   16    2    3   35]\n",
            " [   9    1    3   31    4  805   15    4   14    6]\n",
            " [  13    3    1    1    9    7  923    0    1    0]\n",
            " [   4    9   18    6    8    0    0  961    1   21]\n",
            " [   5    5    7   15    9    9   13   12  894    5]\n",
            " [  12    7    1   10   31    7    1   11    5  924]]\n"
          ]
        }
      ],
      "source": [
        "# drop\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Set hyperparameters\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "DROPOUT_RATE = 0.3 # Dropout rate\n",
        "\n",
        "# Load and preprocess data\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "RESHAPED = 784\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Build the neural network model with Dropout regularization\n",
        "model_dropout = keras.Sequential([\n",
        "    keras.layers.Input(shape=(RESHAPED,)),\n",
        "    keras.layers.Dense(N_HIDDEN, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "    Dropout(DROPOUT_RATE), # Dropout layer after the first hidden layer\n",
        "    keras.layers.Dense(N_HIDDEN // 2, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "    Dropout(DROPOUT_RATE), # Dropout layer after the second hidden layer\n",
        "    keras.layers.Dense(NB_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model_dropout.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_dropout = model_dropout.fit(\n",
        "    X_train, Y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    verbose=VERBOSE,\n",
        ")\n",
        "\n",
        "test_loss_dropout, test_acc_dropout = model_dropout.evaluate(X_test, Y_test, verbose=1)\n",
        "print(f\"\\nModel with Dropout Regularization:\")\n",
        "print(f\"Test accuracy: {test_acc_dropout:.4f}  | Test loss: {test_loss_dropout:.4f}\")\n",
        "\n",
        "y_true_dropout = np.argmax(Y_test, axis=1)\n",
        "y_pred_dropout = np.argmax(model_dropout.predict(X_test, verbose=0), axis=1)\n",
        "cm_dropout = tf.math.confusion_matrix(y_true_dropout, y_pred_dropout, num_classes=NB_CLASSES).numpy()\n",
        "print(\"\\nConfusion Matrix (Dropout):\")\n",
        "print(cm_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1fO1AkF16gw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1fO1AkF16gw",
        "outputId": "10f93ebc-eec2-49e6-9244-fa8f33cbe8ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training model with Adam optimizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.8077 - loss: 0.6977 - val_accuracy: 0.9488 - val_loss: 0.1798\n",
            "Epoch 2/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9530 - loss: 0.1594 - val_accuracy: 0.9585 - val_loss: 0.1390\n",
            "Epoch 3/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9672 - loss: 0.1095 - val_accuracy: 0.9656 - val_loss: 0.1143\n",
            "Epoch 4/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9773 - loss: 0.0761 - val_accuracy: 0.9695 - val_loss: 0.1023\n",
            "Epoch 5/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.9838 - loss: 0.0574 - val_accuracy: 0.9705 - val_loss: 0.0987\n",
            "Epoch 6/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9861 - loss: 0.0490 - val_accuracy: 0.9744 - val_loss: 0.0933\n",
            "Epoch 7/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9891 - loss: 0.0366 - val_accuracy: 0.9703 - val_loss: 0.1018\n",
            "Epoch 8/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9914 - loss: 0.0290 - val_accuracy: 0.9730 - val_loss: 0.1001\n",
            "Epoch 9/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.9921 - loss: 0.0268 - val_accuracy: 0.9737 - val_loss: 0.1011\n",
            "Epoch 10/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9940 - loss: 0.0205 - val_accuracy: 0.9730 - val_loss: 0.1005\n",
            "Epoch 11/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9944 - loss: 0.0173 - val_accuracy: 0.9742 - val_loss: 0.1007\n",
            "Epoch 12/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9959 - loss: 0.0142 - val_accuracy: 0.9703 - val_loss: 0.1190\n",
            "Epoch 13/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9973 - loss: 0.0109 - val_accuracy: 0.9743 - val_loss: 0.1025\n",
            "Epoch 14/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9961 - loss: 0.0124 - val_accuracy: 0.9733 - val_loss: 0.1068\n",
            "Epoch 15/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9981 - loss: 0.0080 - val_accuracy: 0.9753 - val_loss: 0.1066\n",
            "Epoch 16/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9988 - loss: 0.0050 - val_accuracy: 0.9777 - val_loss: 0.1050\n",
            "Epoch 17/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0079 - val_accuracy: 0.9757 - val_loss: 0.1073\n",
            "Epoch 18/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9964 - loss: 0.0099 - val_accuracy: 0.9731 - val_loss: 0.1250\n",
            "Epoch 19/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9970 - loss: 0.0095 - val_accuracy: 0.9763 - val_loss: 0.1116\n",
            "Epoch 20/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9978 - loss: 0.0067 - val_accuracy: 0.9764 - val_loss: 0.1107\n",
            "Epoch 21/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 0.9761 - val_loss: 0.1227\n",
            "Epoch 22/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0033 - val_accuracy: 0.9774 - val_loss: 0.1203\n",
            "Epoch 23/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0024 - val_accuracy: 0.9778 - val_loss: 0.1116\n",
            "Epoch 24/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.8299e-04 - val_accuracy: 0.9787 - val_loss: 0.1135\n",
            "Epoch 25/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.4710e-04 - val_accuracy: 0.9785 - val_loss: 0.1139\n",
            "Epoch 26/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4150e-04 - val_accuracy: 0.9788 - val_loss: 0.1177\n",
            "Epoch 27/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.0416e-04 - val_accuracy: 0.9766 - val_loss: 0.1333\n",
            "Epoch 28/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9886 - loss: 0.0384 - val_accuracy: 0.9767 - val_loss: 0.1239\n",
            "Epoch 29/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9972 - loss: 0.0087 - val_accuracy: 0.9778 - val_loss: 0.1169\n",
            "Epoch 30/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 0.9772 - val_loss: 0.1278\n",
            "Epoch 31/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9996 - loss: 0.0023 - val_accuracy: 0.9795 - val_loss: 0.1225\n",
            "Epoch 32/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.5798e-04 - val_accuracy: 0.9792 - val_loss: 0.1251\n",
            "Epoch 33/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.2119e-04 - val_accuracy: 0.9794 - val_loss: 0.1267\n",
            "Epoch 34/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.3108e-04 - val_accuracy: 0.9793 - val_loss: 0.1283\n",
            "Epoch 35/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1113e-04 - val_accuracy: 0.9791 - val_loss: 0.1303\n",
            "Epoch 36/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.0631e-05 - val_accuracy: 0.9796 - val_loss: 0.1314\n",
            "Epoch 37/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.7865e-05 - val_accuracy: 0.9794 - val_loss: 0.1336\n",
            "Epoch 38/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.8081e-05 - val_accuracy: 0.9793 - val_loss: 0.1345\n",
            "Epoch 39/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.1015e-05 - val_accuracy: 0.9795 - val_loss: 0.1358\n",
            "Epoch 40/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.3590e-05 - val_accuracy: 0.9794 - val_loss: 0.1369\n",
            "Epoch 41/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.7301e-05 - val_accuracy: 0.9794 - val_loss: 0.1387\n",
            "Epoch 42/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.8801e-05 - val_accuracy: 0.9791 - val_loss: 0.1398\n",
            "Epoch 43/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.4790e-05 - val_accuracy: 0.9793 - val_loss: 0.1414\n",
            "Epoch 44/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.9372e-05 - val_accuracy: 0.9799 - val_loss: 0.1430\n",
            "Epoch 45/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.9351e-05 - val_accuracy: 0.9482 - val_loss: 0.3494\n",
            "Epoch 46/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9812 - loss: 0.0715 - val_accuracy: 0.9762 - val_loss: 0.1399\n",
            "Epoch 47/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9986 - loss: 0.0053 - val_accuracy: 0.9753 - val_loss: 0.1551\n",
            "Epoch 48/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 0.9784 - val_loss: 0.1338\n",
            "Epoch 49/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.5804e-04 - val_accuracy: 0.9788 - val_loss: 0.1310\n",
            "Epoch 50/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.3766e-04 - val_accuracy: 0.9793 - val_loss: 0.1315\n",
            "Epoch 51/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0437e-04 - val_accuracy: 0.9789 - val_loss: 0.1331\n",
            "Epoch 52/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.3179e-05 - val_accuracy: 0.9794 - val_loss: 0.1339\n",
            "Epoch 53/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.5438e-05 - val_accuracy: 0.9788 - val_loss: 0.1364\n",
            "Epoch 54/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.7338e-05 - val_accuracy: 0.9793 - val_loss: 0.1383\n",
            "Epoch 55/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.5876e-05 - val_accuracy: 0.9793 - val_loss: 0.1395\n",
            "Epoch 56/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9938e-05 - val_accuracy: 0.9793 - val_loss: 0.1412\n",
            "Epoch 57/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.4752e-05 - val_accuracy: 0.9794 - val_loss: 0.1430\n",
            "Epoch 58/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.1136e-05 - val_accuracy: 0.9793 - val_loss: 0.1453\n",
            "Epoch 59/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7724e-05 - val_accuracy: 0.9793 - val_loss: 0.1462\n",
            "Epoch 60/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.5067e-05 - val_accuracy: 0.9795 - val_loss: 0.1479\n",
            "Epoch 61/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.2574e-05 - val_accuracy: 0.9795 - val_loss: 0.1504\n",
            "Epoch 62/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8522e-05 - val_accuracy: 0.9791 - val_loss: 0.1521\n",
            "Epoch 63/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5750e-05 - val_accuracy: 0.9794 - val_loss: 0.1537\n",
            "Epoch 64/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4689e-05 - val_accuracy: 0.9793 - val_loss: 0.1560\n",
            "Epoch 65/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1115e-05 - val_accuracy: 0.9791 - val_loss: 0.1578\n",
            "Epoch 66/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0017e-05 - val_accuracy: 0.9794 - val_loss: 0.1595\n",
            "Epoch 67/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.6778e-06 - val_accuracy: 0.9794 - val_loss: 0.1619\n",
            "Epoch 68/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.6407e-06 - val_accuracy: 0.9791 - val_loss: 0.1634\n",
            "Epoch 69/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.4277e-06 - val_accuracy: 0.9790 - val_loss: 0.1658\n",
            "Epoch 70/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.2858e-06 - val_accuracy: 0.9793 - val_loss: 0.1672\n",
            "Epoch 71/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.4294e-06 - val_accuracy: 0.9793 - val_loss: 0.1702\n",
            "Epoch 72/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9607e-06 - val_accuracy: 0.9789 - val_loss: 0.1723\n",
            "Epoch 73/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.0588e-06 - val_accuracy: 0.9789 - val_loss: 0.1747\n",
            "Epoch 74/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.6580e-06 - val_accuracy: 0.9790 - val_loss: 0.1769\n",
            "Epoch 75/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2170e-06 - val_accuracy: 0.9789 - val_loss: 0.1795\n",
            "Epoch 76/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.0528e-06 - val_accuracy: 0.9785 - val_loss: 0.1879\n",
            "Epoch 77/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9868 - loss: 0.0645 - val_accuracy: 0.9773 - val_loss: 0.1497\n",
            "Epoch 78/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9979 - loss: 0.0067 - val_accuracy: 0.9783 - val_loss: 0.1495\n",
            "Epoch 79/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.9778 - val_loss: 0.1558\n",
            "Epoch 80/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 3.8519e-04 - val_accuracy: 0.9787 - val_loss: 0.1506\n",
            "Epoch 81/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 5.8120e-04 - val_accuracy: 0.9784 - val_loss: 0.1505\n",
            "Epoch 82/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.5397e-05 - val_accuracy: 0.9783 - val_loss: 0.1494\n",
            "Epoch 83/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.5900e-05 - val_accuracy: 0.9783 - val_loss: 0.1516\n",
            "Epoch 84/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9794e-05 - val_accuracy: 0.9787 - val_loss: 0.1525\n",
            "Epoch 85/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.5258e-05 - val_accuracy: 0.9787 - val_loss: 0.1537\n",
            "Epoch 86/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1962e-05 - val_accuracy: 0.9789 - val_loss: 0.1543\n",
            "Epoch 87/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0143e-05 - val_accuracy: 0.9792 - val_loss: 0.1552\n",
            "Epoch 88/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.6700e-05 - val_accuracy: 0.9792 - val_loss: 0.1565\n",
            "Epoch 89/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.4396e-05 - val_accuracy: 0.9793 - val_loss: 0.1577\n",
            "Epoch 90/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2907e-05 - val_accuracy: 0.9794 - val_loss: 0.1590\n",
            "Epoch 91/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1414e-05 - val_accuracy: 0.9792 - val_loss: 0.1598\n",
            "Epoch 92/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.5144e-06 - val_accuracy: 0.9792 - val_loss: 0.1613\n",
            "Epoch 93/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.1792e-06 - val_accuracy: 0.9791 - val_loss: 0.1625\n",
            "Epoch 94/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.1427e-06 - val_accuracy: 0.9793 - val_loss: 0.1640\n",
            "Epoch 95/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.1782e-06 - val_accuracy: 0.9793 - val_loss: 0.1656\n",
            "Epoch 96/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.6995e-06 - val_accuracy: 0.9792 - val_loss: 0.1672\n",
            "Epoch 97/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.6758e-06 - val_accuracy: 0.9793 - val_loss: 0.1686\n",
            "Epoch 98/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.3448e-06 - val_accuracy: 0.9794 - val_loss: 0.1701\n",
            "Epoch 99/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.8415e-06 - val_accuracy: 0.9793 - val_loss: 0.1720\n",
            "Epoch 100/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.1430e-06 - val_accuracy: 0.9795 - val_loss: 0.1739\n",
            "Epoch 101/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.5685e-06 - val_accuracy: 0.9796 - val_loss: 0.1749\n",
            "Epoch 102/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3843e-06 - val_accuracy: 0.9796 - val_loss: 0.1770\n",
            "Epoch 103/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8953e-06 - val_accuracy: 0.9795 - val_loss: 0.1790\n",
            "Epoch 104/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6413e-06 - val_accuracy: 0.9798 - val_loss: 0.1802\n",
            "Epoch 105/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4402e-06 - val_accuracy: 0.9799 - val_loss: 0.1826\n",
            "Epoch 106/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.2406e-06 - val_accuracy: 0.9795 - val_loss: 0.1841\n",
            "Epoch 107/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0149e-06 - val_accuracy: 0.9795 - val_loss: 0.1861\n",
            "Epoch 108/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.9309e-07 - val_accuracy: 0.9797 - val_loss: 0.1884\n",
            "Epoch 109/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 7.3853e-07 - val_accuracy: 0.9799 - val_loss: 0.1899\n",
            "Epoch 110/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0372e-07 - val_accuracy: 0.9802 - val_loss: 0.1917\n",
            "Epoch 111/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.3571e-07 - val_accuracy: 0.9800 - val_loss: 0.1941\n",
            "Epoch 112/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.9560e-07 - val_accuracy: 0.9803 - val_loss: 0.1948\n",
            "Epoch 113/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.0450e-07 - val_accuracy: 0.9803 - val_loss: 0.1972\n",
            "Epoch 114/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.2357e-07 - val_accuracy: 0.9804 - val_loss: 0.1994\n",
            "Epoch 115/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7217e-07 - val_accuracy: 0.9803 - val_loss: 0.2013\n",
            "Epoch 116/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4996e-07 - val_accuracy: 0.9803 - val_loss: 0.2030\n",
            "Epoch 117/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.0296e-07 - val_accuracy: 0.9803 - val_loss: 0.2050\n",
            "Epoch 118/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.0147e-07 - val_accuracy: 0.9803 - val_loss: 0.2071\n",
            "Epoch 119/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5493e-07 - val_accuracy: 0.9803 - val_loss: 0.2096\n",
            "Epoch 120/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1935e-07 - val_accuracy: 0.9805 - val_loss: 0.2114\n",
            "Epoch 121/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.7372e-08 - val_accuracy: 0.9803 - val_loss: 0.2131\n",
            "Epoch 122/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.2003e-08 - val_accuracy: 0.9803 - val_loss: 0.2160\n",
            "Epoch 123/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.3121e-08 - val_accuracy: 0.9802 - val_loss: 0.2169\n",
            "Epoch 124/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.5534e-08 - val_accuracy: 0.9801 - val_loss: 0.2187\n",
            "Epoch 125/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.2779e-08 - val_accuracy: 0.9800 - val_loss: 0.2217\n",
            "Epoch 126/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.5173e-08 - val_accuracy: 0.9803 - val_loss: 0.2224\n",
            "Epoch 127/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.9318e-08 - val_accuracy: 0.9801 - val_loss: 0.2237\n",
            "Epoch 128/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.4136e-08 - val_accuracy: 0.9802 - val_loss: 0.2264\n",
            "Epoch 129/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.9205e-08 - val_accuracy: 0.9801 - val_loss: 0.2279\n",
            "Epoch 130/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4878e-08 - val_accuracy: 0.9802 - val_loss: 0.2305\n",
            "Epoch 131/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.1875e-08 - val_accuracy: 0.9802 - val_loss: 0.2314\n",
            "Epoch 132/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.9156e-08 - val_accuracy: 0.9802 - val_loss: 0.2334\n",
            "Epoch 133/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6023e-08 - val_accuracy: 0.9800 - val_loss: 0.2346\n",
            "Epoch 134/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5064e-08 - val_accuracy: 0.9802 - val_loss: 0.2360\n",
            "Epoch 135/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2450e-08 - val_accuracy: 0.9803 - val_loss: 0.2378\n",
            "Epoch 136/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0319e-08 - val_accuracy: 0.9799 - val_loss: 0.2391\n",
            "Epoch 137/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0035e-08 - val_accuracy: 0.9801 - val_loss: 0.2405\n",
            "Epoch 138/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.4388e-09 - val_accuracy: 0.9803 - val_loss: 0.2419\n",
            "Epoch 139/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.4900e-09 - val_accuracy: 0.9800 - val_loss: 0.2424\n",
            "Epoch 140/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.0777e-09 - val_accuracy: 0.9799 - val_loss: 0.2438\n",
            "Epoch 141/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.7963e-09 - val_accuracy: 0.9799 - val_loss: 0.2446\n",
            "Epoch 142/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.5762e-09 - val_accuracy: 0.9798 - val_loss: 0.2452\n",
            "Epoch 143/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.9484e-09 - val_accuracy: 0.9798 - val_loss: 0.2465\n",
            "Epoch 144/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.8656e-09 - val_accuracy: 0.9799 - val_loss: 0.2473\n",
            "Epoch 145/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.4700e-09 - val_accuracy: 0.9797 - val_loss: 0.2481\n",
            "Epoch 146/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.2346e-09 - val_accuracy: 0.9799 - val_loss: 0.2496\n",
            "Epoch 147/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.6932e-09 - val_accuracy: 0.9796 - val_loss: 0.2500\n",
            "Epoch 148/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.6958e-09 - val_accuracy: 0.9797 - val_loss: 0.2507\n",
            "Epoch 149/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.4166e-09 - val_accuracy: 0.9795 - val_loss: 0.2514\n",
            "Epoch 150/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.2454e-09 - val_accuracy: 0.9796 - val_loss: 0.2524\n",
            "Epoch 151/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.9836e-09 - val_accuracy: 0.9797 - val_loss: 0.2525\n",
            "Epoch 152/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7969e-09 - val_accuracy: 0.9797 - val_loss: 0.2533\n",
            "Epoch 153/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.9144e-09 - val_accuracy: 0.9797 - val_loss: 0.2541\n",
            "Epoch 154/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.5493e-09 - val_accuracy: 0.9795 - val_loss: 0.2542\n",
            "Epoch 155/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.3218e-09 - val_accuracy: 0.9797 - val_loss: 0.2549\n",
            "Epoch 156/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2980e-09 - val_accuracy: 0.9795 - val_loss: 0.2555\n",
            "Epoch 157/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.2062e-09 - val_accuracy: 0.9795 - val_loss: 0.2557\n",
            "Epoch 158/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.2039e-09 - val_accuracy: 0.9795 - val_loss: 0.2566\n",
            "Epoch 159/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0451e-09 - val_accuracy: 0.9795 - val_loss: 0.2566\n",
            "Epoch 160/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.9574e-09 - val_accuracy: 0.9797 - val_loss: 0.2575\n",
            "Epoch 161/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8844e-09 - val_accuracy: 0.9795 - val_loss: 0.2573\n",
            "Epoch 162/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7911e-09 - val_accuracy: 0.9797 - val_loss: 0.2577\n",
            "Epoch 163/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7720e-09 - val_accuracy: 0.9796 - val_loss: 0.2585\n",
            "Epoch 164/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6800e-09 - val_accuracy: 0.9797 - val_loss: 0.2591\n",
            "Epoch 165/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6722e-09 - val_accuracy: 0.9796 - val_loss: 0.2591\n",
            "Epoch 166/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.6737e-09 - val_accuracy: 0.9797 - val_loss: 0.2594\n",
            "Epoch 167/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6317e-09 - val_accuracy: 0.9797 - val_loss: 0.2599\n",
            "Epoch 168/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.6999e-09 - val_accuracy: 0.9796 - val_loss: 0.2606\n",
            "Epoch 169/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6346e-09 - val_accuracy: 0.9796 - val_loss: 0.2609\n",
            "Epoch 170/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6258e-09 - val_accuracy: 0.9797 - val_loss: 0.2613\n",
            "Epoch 171/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6007e-09 - val_accuracy: 0.9797 - val_loss: 0.2619\n",
            "Epoch 172/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6954e-09 - val_accuracy: 0.9797 - val_loss: 0.2621\n",
            "Epoch 173/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.5041e-09 - val_accuracy: 0.9798 - val_loss: 0.2630\n",
            "Epoch 174/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5136e-09 - val_accuracy: 0.9799 - val_loss: 0.2631\n",
            "Epoch 175/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4854e-09 - val_accuracy: 0.9797 - val_loss: 0.2634\n",
            "Epoch 176/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4878e-09 - val_accuracy: 0.9797 - val_loss: 0.2640\n",
            "Epoch 177/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5579e-09 - val_accuracy: 0.9797 - val_loss: 0.2644\n",
            "Epoch 178/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6248e-09 - val_accuracy: 0.9797 - val_loss: 0.2652\n",
            "Epoch 179/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4642e-09 - val_accuracy: 0.9797 - val_loss: 0.2657\n",
            "Epoch 180/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.3944e-09 - val_accuracy: 0.9797 - val_loss: 0.2659\n",
            "Epoch 181/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5302e-09 - val_accuracy: 0.9797 - val_loss: 0.2662\n",
            "Epoch 182/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5201e-09 - val_accuracy: 0.9797 - val_loss: 0.2668\n",
            "Epoch 183/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4721e-09 - val_accuracy: 0.9797 - val_loss: 0.2675\n",
            "Epoch 184/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5427e-09 - val_accuracy: 0.9797 - val_loss: 0.2681\n",
            "Epoch 185/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4447e-09 - val_accuracy: 0.9797 - val_loss: 0.2682\n",
            "Epoch 186/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.3236e-09 - val_accuracy: 0.9797 - val_loss: 0.2689\n",
            "Epoch 187/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4576e-09 - val_accuracy: 0.9797 - val_loss: 0.2694\n",
            "Epoch 188/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4071e-09 - val_accuracy: 0.9796 - val_loss: 0.2698\n",
            "Epoch 189/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4047e-09 - val_accuracy: 0.9797 - val_loss: 0.2705\n",
            "Epoch 190/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.5046e-09 - val_accuracy: 0.9797 - val_loss: 0.2710\n",
            "Epoch 191/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4614e-09 - val_accuracy: 0.9796 - val_loss: 0.2713\n",
            "Epoch 192/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4952e-09 - val_accuracy: 0.9797 - val_loss: 0.2723\n",
            "Epoch 193/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5181e-09 - val_accuracy: 0.9800 - val_loss: 0.2726\n",
            "Epoch 194/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4757e-09 - val_accuracy: 0.9797 - val_loss: 0.2733\n",
            "Epoch 195/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6273e-09 - val_accuracy: 0.9797 - val_loss: 0.2737\n",
            "Epoch 196/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5463e-09 - val_accuracy: 0.9797 - val_loss: 0.2748\n",
            "Epoch 197/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5088e-09 - val_accuracy: 0.9797 - val_loss: 0.2746\n",
            "Epoch 198/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.5330e-09 - val_accuracy: 0.9799 - val_loss: 0.2761\n",
            "Epoch 199/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5305e-09 - val_accuracy: 0.9795 - val_loss: 0.2756\n",
            "Epoch 200/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4607e-09 - val_accuracy: 0.9796 - val_loss: 0.2765\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.2649\n",
            "\n",
            "Model with Adam Optimizer:\n",
            "Test accuracy: 0.9802  | Test loss: 0.2396\n",
            "\n",
            "Confusion Matrix (Adam):\n",
            "[[ 970    1    1    0    1    1    2    1    2    1]\n",
            " [   0 1126    3    0    0    1    2    1    2    0]\n",
            " [   1    1 1015    1    2    0    3    4    5    0]\n",
            " [   1    1    4  991    0    5    0    2    3    3]\n",
            " [   2    1    2    1  959    0    3    4    2    8]\n",
            " [   3    0    0   11    0  869    3    0    4    2]\n",
            " [   4    3    0    1    2    5  942    0    1    0]\n",
            " [   1    5    8    2    0    0    0 1004    2    6]\n",
            " [   3    0    3   11    3    4    1    2  943    4]\n",
            " [   3    3    0    3    7    3    1    3    3  983]]\n"
          ]
        }
      ],
      "source": [
        "#Adam\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Set hyperparameters\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Load and preprocess data\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "RESHAPED = 784\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Define a function to create and train a model with a given optimizer\n",
        "def train_model_with_optimizer(optimizer, optimizer_name):\n",
        "    print(f\"Training model with {optimizer_name} optimizer...\")\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(RESHAPED,)),\n",
        "        keras.layers.Dense(N_HIDDEN, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        keras.layers.Dense(N_HIDDEN // 2, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        keras.layers.Dense(NB_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=VALIDATION_SPLIT,\n",
        "        verbose=VERBOSE,\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print(f\"\\nModel with {optimizer_name} Optimizer:\")\n",
        "    print(f\"Test accuracy: {test_acc:.4f}  | Test loss: {test_loss:.4f}\")\n",
        "\n",
        "    y_true = np.argmax(Y_test, axis=1)\n",
        "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
        "    cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=NB_CLASSES).numpy()\n",
        "    print(f\"\\nConfusion Matrix ({optimizer_name}):\")\n",
        "    print(cm)\n",
        "\n",
        "    return history, test_acc, test_loss\n",
        "\n",
        "# Train models with different optimizers\n",
        "optimizers = {\n",
        "    \"Adam\": keras.optimizers.Adam(),\n",
        "}\n",
        "\n",
        "results_optimizers = {}\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    history, test_acc, test_loss = train_model_with_optimizer(optimizer, optimizer_name)\n",
        "    results_optimizers[optimizer_name] = {\"history\": history, \"test_acc\": test_acc, \"test_loss\": test_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SajCD_iBNVcU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SajCD_iBNVcU",
        "outputId": "69e5aac7-576f-4072-b657-1aba5176b02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with Adagrad optimizer...\n",
            "Epoch 1/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.2034 - loss: 2.2362 - val_accuracy: 0.5017 - val_loss: 1.8562\n",
            "Epoch 2/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5660 - loss: 1.7387 - val_accuracy: 0.7434 - val_loss: 1.3403\n",
            "Epoch 3/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7567 - loss: 1.2503 - val_accuracy: 0.8313 - val_loss: 0.9510\n",
            "Epoch 4/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8227 - loss: 0.9160 - val_accuracy: 0.8535 - val_loss: 0.7341\n",
            "Epoch 5/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8451 - loss: 0.7338 - val_accuracy: 0.8708 - val_loss: 0.6143\n",
            "Epoch 6/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8593 - loss: 0.6250 - val_accuracy: 0.8769 - val_loss: 0.5419\n",
            "Epoch 7/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8686 - loss: 0.5624 - val_accuracy: 0.8820 - val_loss: 0.4941\n",
            "Epoch 8/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8729 - loss: 0.5212 - val_accuracy: 0.8867 - val_loss: 0.4602\n",
            "Epoch 9/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8775 - loss: 0.4841 - val_accuracy: 0.8916 - val_loss: 0.4348\n",
            "Epoch 10/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8815 - loss: 0.4593 - val_accuracy: 0.8949 - val_loss: 0.4152\n",
            "Epoch 11/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8845 - loss: 0.4417 - val_accuracy: 0.8974 - val_loss: 0.3996\n",
            "Epoch 12/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8871 - loss: 0.4266 - val_accuracy: 0.9002 - val_loss: 0.3866\n",
            "Epoch 13/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8902 - loss: 0.4123 - val_accuracy: 0.9022 - val_loss: 0.3758\n",
            "Epoch 14/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8955 - loss: 0.3950 - val_accuracy: 0.9038 - val_loss: 0.3666\n",
            "Epoch 15/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8955 - loss: 0.3881 - val_accuracy: 0.9054 - val_loss: 0.3583\n",
            "Epoch 16/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8974 - loss: 0.3808 - val_accuracy: 0.9062 - val_loss: 0.3513\n",
            "Epoch 17/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8976 - loss: 0.3730 - val_accuracy: 0.9083 - val_loss: 0.3447\n",
            "Epoch 18/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8998 - loss: 0.3659 - val_accuracy: 0.9094 - val_loss: 0.3390\n",
            "Epoch 19/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8982 - loss: 0.3634 - val_accuracy: 0.9102 - val_loss: 0.3337\n",
            "Epoch 20/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9005 - loss: 0.3566 - val_accuracy: 0.9107 - val_loss: 0.3289\n",
            "Epoch 21/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9020 - loss: 0.3511 - val_accuracy: 0.9124 - val_loss: 0.3246\n",
            "Epoch 22/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9052 - loss: 0.3380 - val_accuracy: 0.9122 - val_loss: 0.3205\n",
            "Epoch 23/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9051 - loss: 0.3404 - val_accuracy: 0.9131 - val_loss: 0.3166\n",
            "Epoch 24/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9075 - loss: 0.3332 - val_accuracy: 0.9133 - val_loss: 0.3131\n",
            "Epoch 25/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9079 - loss: 0.3278 - val_accuracy: 0.9140 - val_loss: 0.3099\n",
            "Epoch 26/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9081 - loss: 0.3271 - val_accuracy: 0.9147 - val_loss: 0.3068\n",
            "Epoch 27/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9093 - loss: 0.3219 - val_accuracy: 0.9157 - val_loss: 0.3037\n",
            "Epoch 28/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9105 - loss: 0.3252 - val_accuracy: 0.9168 - val_loss: 0.3011\n",
            "Epoch 29/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9086 - loss: 0.3209 - val_accuracy: 0.9180 - val_loss: 0.2984\n",
            "Epoch 30/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9104 - loss: 0.3169 - val_accuracy: 0.9184 - val_loss: 0.2957\n",
            "Epoch 31/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9128 - loss: 0.3097 - val_accuracy: 0.9191 - val_loss: 0.2933\n",
            "Epoch 32/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9132 - loss: 0.3064 - val_accuracy: 0.9193 - val_loss: 0.2911\n",
            "Epoch 33/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9135 - loss: 0.3041 - val_accuracy: 0.9201 - val_loss: 0.2888\n",
            "Epoch 34/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9164 - loss: 0.2992 - val_accuracy: 0.9212 - val_loss: 0.2866\n",
            "Epoch 35/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9153 - loss: 0.2988 - val_accuracy: 0.9220 - val_loss: 0.2845\n",
            "Epoch 36/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9137 - loss: 0.3026 - val_accuracy: 0.9224 - val_loss: 0.2826\n",
            "Epoch 37/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9163 - loss: 0.2936 - val_accuracy: 0.9229 - val_loss: 0.2806\n",
            "Epoch 38/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.2972 - val_accuracy: 0.9235 - val_loss: 0.2788\n",
            "Epoch 39/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9158 - loss: 0.2977 - val_accuracy: 0.9239 - val_loss: 0.2768\n",
            "Epoch 40/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9185 - loss: 0.2891 - val_accuracy: 0.9246 - val_loss: 0.2751\n",
            "Epoch 41/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9202 - loss: 0.2822 - val_accuracy: 0.9252 - val_loss: 0.2735\n",
            "Epoch 42/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9205 - loss: 0.2833 - val_accuracy: 0.9249 - val_loss: 0.2718\n",
            "Epoch 43/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9205 - loss: 0.2830 - val_accuracy: 0.9256 - val_loss: 0.2701\n",
            "Epoch 44/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9216 - loss: 0.2756 - val_accuracy: 0.9262 - val_loss: 0.2686\n",
            "Epoch 45/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9179 - loss: 0.2826 - val_accuracy: 0.9261 - val_loss: 0.2669\n",
            "Epoch 46/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9221 - loss: 0.2788 - val_accuracy: 0.9265 - val_loss: 0.2655\n",
            "Epoch 47/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9231 - loss: 0.2766 - val_accuracy: 0.9269 - val_loss: 0.2642\n",
            "Epoch 48/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9236 - loss: 0.2725 - val_accuracy: 0.9278 - val_loss: 0.2626\n",
            "Epoch 49/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9251 - loss: 0.2702 - val_accuracy: 0.9284 - val_loss: 0.2612\n",
            "Epoch 50/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9239 - loss: 0.2718 - val_accuracy: 0.9281 - val_loss: 0.2599\n",
            "Epoch 51/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9203 - loss: 0.2816 - val_accuracy: 0.9290 - val_loss: 0.2586\n",
            "Epoch 52/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9254 - loss: 0.2649 - val_accuracy: 0.9293 - val_loss: 0.2573\n",
            "Epoch 53/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9251 - loss: 0.2662 - val_accuracy: 0.9297 - val_loss: 0.2560\n",
            "Epoch 54/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9271 - loss: 0.2594 - val_accuracy: 0.9298 - val_loss: 0.2547\n",
            "Epoch 55/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9263 - loss: 0.2617 - val_accuracy: 0.9302 - val_loss: 0.2535\n",
            "Epoch 56/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9292 - loss: 0.2562 - val_accuracy: 0.9307 - val_loss: 0.2522\n",
            "Epoch 57/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9251 - loss: 0.2616 - val_accuracy: 0.9307 - val_loss: 0.2511\n",
            "Epoch 58/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9275 - loss: 0.2579 - val_accuracy: 0.9314 - val_loss: 0.2498\n",
            "Epoch 59/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9281 - loss: 0.2584 - val_accuracy: 0.9321 - val_loss: 0.2486\n",
            "Epoch 60/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9291 - loss: 0.2564 - val_accuracy: 0.9322 - val_loss: 0.2475\n",
            "Epoch 61/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9294 - loss: 0.2558 - val_accuracy: 0.9324 - val_loss: 0.2462\n",
            "Epoch 62/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9286 - loss: 0.2554 - val_accuracy: 0.9328 - val_loss: 0.2452\n",
            "Epoch 63/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9294 - loss: 0.2499 - val_accuracy: 0.9330 - val_loss: 0.2441\n",
            "Epoch 64/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9287 - loss: 0.2555 - val_accuracy: 0.9332 - val_loss: 0.2431\n",
            "Epoch 65/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9302 - loss: 0.2494 - val_accuracy: 0.9338 - val_loss: 0.2421\n",
            "Epoch 66/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9297 - loss: 0.2502 - val_accuracy: 0.9337 - val_loss: 0.2411\n",
            "Epoch 67/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9309 - loss: 0.2476 - val_accuracy: 0.9342 - val_loss: 0.2400\n",
            "Epoch 68/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9316 - loss: 0.2443 - val_accuracy: 0.9345 - val_loss: 0.2390\n",
            "Epoch 69/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9314 - loss: 0.2457 - val_accuracy: 0.9346 - val_loss: 0.2380\n",
            "Epoch 70/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9317 - loss: 0.2428 - val_accuracy: 0.9354 - val_loss: 0.2371\n",
            "Epoch 71/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9314 - loss: 0.2452 - val_accuracy: 0.9353 - val_loss: 0.2361\n",
            "Epoch 72/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9332 - loss: 0.2417 - val_accuracy: 0.9355 - val_loss: 0.2350\n",
            "Epoch 73/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9343 - loss: 0.2386 - val_accuracy: 0.9364 - val_loss: 0.2341\n",
            "Epoch 74/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9336 - loss: 0.2359 - val_accuracy: 0.9360 - val_loss: 0.2332\n",
            "Epoch 75/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9349 - loss: 0.2335 - val_accuracy: 0.9359 - val_loss: 0.2323\n",
            "Epoch 76/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9332 - loss: 0.2375 - val_accuracy: 0.9364 - val_loss: 0.2314\n",
            "Epoch 77/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9327 - loss: 0.2423 - val_accuracy: 0.9368 - val_loss: 0.2305\n",
            "Epoch 78/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9315 - loss: 0.2409 - val_accuracy: 0.9367 - val_loss: 0.2297\n",
            "Epoch 79/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9336 - loss: 0.2380 - val_accuracy: 0.9373 - val_loss: 0.2288\n",
            "Epoch 80/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9340 - loss: 0.2339 - val_accuracy: 0.9376 - val_loss: 0.2279\n",
            "Epoch 81/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9322 - loss: 0.2384 - val_accuracy: 0.9376 - val_loss: 0.2271\n",
            "Epoch 82/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9350 - loss: 0.2347 - val_accuracy: 0.9383 - val_loss: 0.2263\n",
            "Epoch 83/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9339 - loss: 0.2336 - val_accuracy: 0.9387 - val_loss: 0.2254\n",
            "Epoch 84/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9351 - loss: 0.2282 - val_accuracy: 0.9386 - val_loss: 0.2246\n",
            "Epoch 85/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9371 - loss: 0.2274 - val_accuracy: 0.9393 - val_loss: 0.2239\n",
            "Epoch 86/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9357 - loss: 0.2276 - val_accuracy: 0.9391 - val_loss: 0.2230\n",
            "Epoch 87/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9374 - loss: 0.2215 - val_accuracy: 0.9395 - val_loss: 0.2222\n",
            "Epoch 88/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9369 - loss: 0.2242 - val_accuracy: 0.9397 - val_loss: 0.2215\n",
            "Epoch 89/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9371 - loss: 0.2245 - val_accuracy: 0.9398 - val_loss: 0.2207\n",
            "Epoch 90/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9389 - loss: 0.2150 - val_accuracy: 0.9404 - val_loss: 0.2199\n",
            "Epoch 91/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9372 - loss: 0.2233 - val_accuracy: 0.9406 - val_loss: 0.2192\n",
            "Epoch 92/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9394 - loss: 0.2163 - val_accuracy: 0.9403 - val_loss: 0.2185\n",
            "Epoch 93/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9385 - loss: 0.2187 - val_accuracy: 0.9410 - val_loss: 0.2177\n",
            "Epoch 94/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9398 - loss: 0.2153 - val_accuracy: 0.9410 - val_loss: 0.2170\n",
            "Epoch 95/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9392 - loss: 0.2201 - val_accuracy: 0.9408 - val_loss: 0.2163\n",
            "Epoch 96/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9399 - loss: 0.2176 - val_accuracy: 0.9413 - val_loss: 0.2156\n",
            "Epoch 97/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9394 - loss: 0.2157 - val_accuracy: 0.9415 - val_loss: 0.2149\n",
            "Epoch 98/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9410 - loss: 0.2120 - val_accuracy: 0.9418 - val_loss: 0.2142\n",
            "Epoch 99/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9392 - loss: 0.2153 - val_accuracy: 0.9414 - val_loss: 0.2135\n",
            "Epoch 100/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9401 - loss: 0.2150 - val_accuracy: 0.9418 - val_loss: 0.2128\n",
            "Epoch 101/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9396 - loss: 0.2172 - val_accuracy: 0.9418 - val_loss: 0.2122\n",
            "Epoch 102/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9391 - loss: 0.2158 - val_accuracy: 0.9419 - val_loss: 0.2115\n",
            "Epoch 103/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9403 - loss: 0.2133 - val_accuracy: 0.9422 - val_loss: 0.2109\n",
            "Epoch 104/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9400 - loss: 0.2136 - val_accuracy: 0.9420 - val_loss: 0.2103\n",
            "Epoch 105/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9403 - loss: 0.2075 - val_accuracy: 0.9427 - val_loss: 0.2095\n",
            "Epoch 106/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9398 - loss: 0.2109 - val_accuracy: 0.9429 - val_loss: 0.2090\n",
            "Epoch 107/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9424 - loss: 0.2060 - val_accuracy: 0.9430 - val_loss: 0.2083\n",
            "Epoch 108/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9418 - loss: 0.2076 - val_accuracy: 0.9430 - val_loss: 0.2077\n",
            "Epoch 109/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9422 - loss: 0.2041 - val_accuracy: 0.9430 - val_loss: 0.2070\n",
            "Epoch 110/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9409 - loss: 0.2078 - val_accuracy: 0.9435 - val_loss: 0.2065\n",
            "Epoch 111/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9412 - loss: 0.2134 - val_accuracy: 0.9436 - val_loss: 0.2059\n",
            "Epoch 112/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9416 - loss: 0.2046 - val_accuracy: 0.9437 - val_loss: 0.2052\n",
            "Epoch 113/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9408 - loss: 0.2087 - val_accuracy: 0.9440 - val_loss: 0.2047\n",
            "Epoch 114/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9450 - loss: 0.1971 - val_accuracy: 0.9439 - val_loss: 0.2041\n",
            "Epoch 115/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9426 - loss: 0.2041 - val_accuracy: 0.9443 - val_loss: 0.2036\n",
            "Epoch 116/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9412 - loss: 0.2059 - val_accuracy: 0.9447 - val_loss: 0.2030\n",
            "Epoch 117/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9425 - loss: 0.2053 - val_accuracy: 0.9448 - val_loss: 0.2023\n",
            "Epoch 118/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9422 - loss: 0.2020 - val_accuracy: 0.9446 - val_loss: 0.2018\n",
            "Epoch 119/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9430 - loss: 0.2009 - val_accuracy: 0.9448 - val_loss: 0.2012\n",
            "Epoch 120/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9447 - loss: 0.2004 - val_accuracy: 0.9450 - val_loss: 0.2007\n",
            "Epoch 121/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9465 - loss: 0.1940 - val_accuracy: 0.9451 - val_loss: 0.2001\n",
            "Epoch 122/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9434 - loss: 0.1977 - val_accuracy: 0.9451 - val_loss: 0.1996\n",
            "Epoch 123/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9437 - loss: 0.1968 - val_accuracy: 0.9453 - val_loss: 0.1990\n",
            "Epoch 124/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9430 - loss: 0.2006 - val_accuracy: 0.9457 - val_loss: 0.1984\n",
            "Epoch 125/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9445 - loss: 0.1984 - val_accuracy: 0.9456 - val_loss: 0.1979\n",
            "Epoch 126/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9451 - loss: 0.1939 - val_accuracy: 0.9463 - val_loss: 0.1974\n",
            "Epoch 127/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9453 - loss: 0.1969 - val_accuracy: 0.9464 - val_loss: 0.1969\n",
            "Epoch 128/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9460 - loss: 0.1951 - val_accuracy: 0.9463 - val_loss: 0.1962\n",
            "Epoch 129/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9461 - loss: 0.1972 - val_accuracy: 0.9467 - val_loss: 0.1958\n",
            "Epoch 130/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9454 - loss: 0.1936 - val_accuracy: 0.9465 - val_loss: 0.1953\n",
            "Epoch 131/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9455 - loss: 0.1936 - val_accuracy: 0.9468 - val_loss: 0.1948\n",
            "Epoch 132/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9465 - loss: 0.1905 - val_accuracy: 0.9469 - val_loss: 0.1942\n",
            "Epoch 133/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9457 - loss: 0.1912 - val_accuracy: 0.9470 - val_loss: 0.1937\n",
            "Epoch 134/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9467 - loss: 0.1878 - val_accuracy: 0.9473 - val_loss: 0.1931\n",
            "Epoch 135/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9460 - loss: 0.1906 - val_accuracy: 0.9470 - val_loss: 0.1927\n",
            "Epoch 136/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9475 - loss: 0.1891 - val_accuracy: 0.9473 - val_loss: 0.1922\n",
            "Epoch 137/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9456 - loss: 0.1939 - val_accuracy: 0.9474 - val_loss: 0.1917\n",
            "Epoch 138/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9470 - loss: 0.1911 - val_accuracy: 0.9474 - val_loss: 0.1912\n",
            "Epoch 139/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9470 - loss: 0.1893 - val_accuracy: 0.9475 - val_loss: 0.1907\n",
            "Epoch 140/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9471 - loss: 0.1894 - val_accuracy: 0.9474 - val_loss: 0.1902\n",
            "Epoch 141/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9487 - loss: 0.1830 - val_accuracy: 0.9474 - val_loss: 0.1898\n",
            "Epoch 142/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9480 - loss: 0.1843 - val_accuracy: 0.9477 - val_loss: 0.1893\n",
            "Epoch 143/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9479 - loss: 0.1885 - val_accuracy: 0.9481 - val_loss: 0.1889\n",
            "Epoch 144/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9478 - loss: 0.1852 - val_accuracy: 0.9482 - val_loss: 0.1884\n",
            "Epoch 145/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9493 - loss: 0.1818 - val_accuracy: 0.9481 - val_loss: 0.1880\n",
            "Epoch 146/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9463 - loss: 0.1908 - val_accuracy: 0.9487 - val_loss: 0.1876\n",
            "Epoch 147/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9490 - loss: 0.1816 - val_accuracy: 0.9487 - val_loss: 0.1870\n",
            "Epoch 148/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9480 - loss: 0.1839 - val_accuracy: 0.9488 - val_loss: 0.1866\n",
            "Epoch 149/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9477 - loss: 0.1840 - val_accuracy: 0.9485 - val_loss: 0.1863\n",
            "Epoch 150/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9485 - loss: 0.1839 - val_accuracy: 0.9488 - val_loss: 0.1858\n",
            "Epoch 151/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9496 - loss: 0.1796 - val_accuracy: 0.9488 - val_loss: 0.1854\n",
            "Epoch 152/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9482 - loss: 0.1826 - val_accuracy: 0.9491 - val_loss: 0.1849\n",
            "Epoch 153/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9490 - loss: 0.1832 - val_accuracy: 0.9491 - val_loss: 0.1845\n",
            "Epoch 154/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9485 - loss: 0.1834 - val_accuracy: 0.9492 - val_loss: 0.1841\n",
            "Epoch 155/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9506 - loss: 0.1757 - val_accuracy: 0.9492 - val_loss: 0.1838\n",
            "Epoch 156/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9507 - loss: 0.1758 - val_accuracy: 0.9493 - val_loss: 0.1833\n",
            "Epoch 157/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9498 - loss: 0.1776 - val_accuracy: 0.9495 - val_loss: 0.1829\n",
            "Epoch 158/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9511 - loss: 0.1763 - val_accuracy: 0.9495 - val_loss: 0.1825\n",
            "Epoch 159/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9492 - loss: 0.1821 - val_accuracy: 0.9495 - val_loss: 0.1821\n",
            "Epoch 160/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9487 - loss: 0.1849 - val_accuracy: 0.9497 - val_loss: 0.1817\n",
            "Epoch 161/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9495 - loss: 0.1782 - val_accuracy: 0.9496 - val_loss: 0.1814\n",
            "Epoch 162/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9495 - loss: 0.1785 - val_accuracy: 0.9496 - val_loss: 0.1809\n",
            "Epoch 163/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9500 - loss: 0.1769 - val_accuracy: 0.9501 - val_loss: 0.1805\n",
            "Epoch 164/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9523 - loss: 0.1705 - val_accuracy: 0.9502 - val_loss: 0.1802\n",
            "Epoch 165/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9515 - loss: 0.1763 - val_accuracy: 0.9502 - val_loss: 0.1797\n",
            "Epoch 166/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9511 - loss: 0.1740 - val_accuracy: 0.9499 - val_loss: 0.1794\n",
            "Epoch 167/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9521 - loss: 0.1699 - val_accuracy: 0.9500 - val_loss: 0.1790\n",
            "Epoch 168/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9500 - loss: 0.1771 - val_accuracy: 0.9502 - val_loss: 0.1787\n",
            "Epoch 169/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9509 - loss: 0.1714 - val_accuracy: 0.9502 - val_loss: 0.1783\n",
            "Epoch 170/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9518 - loss: 0.1710 - val_accuracy: 0.9506 - val_loss: 0.1779\n",
            "Epoch 171/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9530 - loss: 0.1718 - val_accuracy: 0.9507 - val_loss: 0.1775\n",
            "Epoch 172/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9521 - loss: 0.1702 - val_accuracy: 0.9507 - val_loss: 0.1772\n",
            "Epoch 173/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9518 - loss: 0.1716 - val_accuracy: 0.9509 - val_loss: 0.1768\n",
            "Epoch 174/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9520 - loss: 0.1712 - val_accuracy: 0.9512 - val_loss: 0.1764\n",
            "Epoch 175/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9515 - loss: 0.1719 - val_accuracy: 0.9512 - val_loss: 0.1761\n",
            "Epoch 176/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9512 - loss: 0.1741 - val_accuracy: 0.9513 - val_loss: 0.1758\n",
            "Epoch 177/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9518 - loss: 0.1683 - val_accuracy: 0.9512 - val_loss: 0.1755\n",
            "Epoch 178/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9531 - loss: 0.1686 - val_accuracy: 0.9516 - val_loss: 0.1751\n",
            "Epoch 179/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9526 - loss: 0.1658 - val_accuracy: 0.9520 - val_loss: 0.1747\n",
            "Epoch 180/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9514 - loss: 0.1740 - val_accuracy: 0.9517 - val_loss: 0.1745\n",
            "Epoch 181/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9543 - loss: 0.1638 - val_accuracy: 0.9519 - val_loss: 0.1741\n",
            "Epoch 182/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9531 - loss: 0.1671 - val_accuracy: 0.9521 - val_loss: 0.1737\n",
            "Epoch 183/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9523 - loss: 0.1664 - val_accuracy: 0.9521 - val_loss: 0.1734\n",
            "Epoch 184/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9519 - loss: 0.1715 - val_accuracy: 0.9523 - val_loss: 0.1731\n",
            "Epoch 185/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9525 - loss: 0.1656 - val_accuracy: 0.9523 - val_loss: 0.1727\n",
            "Epoch 186/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9530 - loss: 0.1650 - val_accuracy: 0.9523 - val_loss: 0.1725\n",
            "Epoch 187/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9558 - loss: 0.1592 - val_accuracy: 0.9524 - val_loss: 0.1721\n",
            "Epoch 188/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9535 - loss: 0.1681 - val_accuracy: 0.9523 - val_loss: 0.1718\n",
            "Epoch 189/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9525 - loss: 0.1700 - val_accuracy: 0.9525 - val_loss: 0.1715\n",
            "Epoch 190/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9538 - loss: 0.1631 - val_accuracy: 0.9526 - val_loss: 0.1712\n",
            "Epoch 191/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9552 - loss: 0.1609 - val_accuracy: 0.9528 - val_loss: 0.1709\n",
            "Epoch 192/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9538 - loss: 0.1635 - val_accuracy: 0.9528 - val_loss: 0.1705\n",
            "Epoch 193/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9539 - loss: 0.1656 - val_accuracy: 0.9529 - val_loss: 0.1702\n",
            "Epoch 194/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9553 - loss: 0.1623 - val_accuracy: 0.9528 - val_loss: 0.1700\n",
            "Epoch 195/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9530 - loss: 0.1648 - val_accuracy: 0.9529 - val_loss: 0.1696\n",
            "Epoch 196/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9546 - loss: 0.1631 - val_accuracy: 0.9532 - val_loss: 0.1694\n",
            "Epoch 197/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9534 - loss: 0.1649 - val_accuracy: 0.9534 - val_loss: 0.1690\n",
            "Epoch 198/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9534 - loss: 0.1636 - val_accuracy: 0.9530 - val_loss: 0.1687\n",
            "Epoch 199/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9544 - loss: 0.1609 - val_accuracy: 0.9531 - val_loss: 0.1685\n",
            "Epoch 200/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9554 - loss: 0.1597 - val_accuracy: 0.9532 - val_loss: 0.1682\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9445 - loss: 0.1951\n",
            "\n",
            "Model with Adagrad Optimizer:\n",
            "Test accuracy: 0.9527  | Test loss: 0.1677\n",
            "\n",
            "Confusion Matrix (Adagrad):\n",
            "[[ 967    0    1    1    0    3    5    2    1    0]\n",
            " [   0 1117    3    2    1    1    3    2    6    0]\n",
            " [   8    2  972   11    7    0    9    8   12    3]\n",
            " [   2    1   11  964    1   10    1    7   10    3]\n",
            " [   1    0    3    0  938    0    8    3    4   25]\n",
            " [   8    1    1   15    1  837   12    1    9    7]\n",
            " [  11    3    4    0    8   10  916    1    5    0]\n",
            " [   1    8   18    7    2    1    0  970    1   20]\n",
            " [   4    3    5   14    7   10   12   11  905    3]\n",
            " [   8    7    2   10   19    2    1   11    8  941]]\n"
          ]
        }
      ],
      "source": [
        "#Adagrad\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Set hyperparameters\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Load and preprocess data\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "RESHAPED = 784\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Define a function to create and train a model with a given optimizer\n",
        "def train_model_with_optimizer(optimizer, optimizer_name):\n",
        "    print(f\"Training model with {optimizer_name} optimizer...\")\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(RESHAPED,)),\n",
        "        keras.layers.Dense(N_HIDDEN, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        keras.layers.Dense(N_HIDDEN // 2, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        keras.layers.Dense(NB_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=VALIDATION_SPLIT,\n",
        "        verbose=VERBOSE,\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print(f\"\\nModel with {optimizer_name} Optimizer:\")\n",
        "    print(f\"Test accuracy: {test_acc:.4f}  | Test loss: {test_loss:.4f}\")\n",
        "\n",
        "    y_true = np.argmax(Y_test, axis=1)\n",
        "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
        "    cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=NB_CLASSES).numpy()\n",
        "    print(f\"\\nConfusion Matrix ({optimizer_name}):\")\n",
        "    print(cm)\n",
        "\n",
        "    return history, test_acc, test_loss\n",
        "\n",
        "# Train models with different optimizers\n",
        "optimizers = {\n",
        "    \"Adagrad\": keras.optimizers.Adagrad(),\n",
        "}\n",
        "\n",
        "results_optimizers = {}\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    history, test_acc, test_loss = train_model_with_optimizer(optimizer, optimizer_name)\n",
        "    results_optimizers[optimizer_name] = {\"history\": history, \"test_acc\": test_acc, \"test_loss\": test_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zruBtq3mTw7J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zruBtq3mTw7J",
        "outputId": "e56b805e-ccf5-49d0-f743-28e9ace6f416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with RMSprop optimizer...\n",
            "Epoch 1/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8298 - loss: 0.6044 - val_accuracy: 0.9469 - val_loss: 0.1853\n",
            "Epoch 2/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9504 - loss: 0.1698 - val_accuracy: 0.9611 - val_loss: 0.1385\n",
            "Epoch 3/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9665 - loss: 0.1081 - val_accuracy: 0.9668 - val_loss: 0.1124\n",
            "Epoch 4/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9769 - loss: 0.0775 - val_accuracy: 0.9683 - val_loss: 0.1021\n",
            "Epoch 5/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9799 - loss: 0.0620 - val_accuracy: 0.9732 - val_loss: 0.0925\n",
            "Epoch 6/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9860 - loss: 0.0483 - val_accuracy: 0.9734 - val_loss: 0.0944\n",
            "Epoch 7/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9887 - loss: 0.0395 - val_accuracy: 0.9737 - val_loss: 0.0928\n",
            "Epoch 8/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9901 - loss: 0.0324 - val_accuracy: 0.9763 - val_loss: 0.0833\n",
            "Epoch 9/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9923 - loss: 0.0250 - val_accuracy: 0.9747 - val_loss: 0.0907\n",
            "Epoch 10/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9938 - loss: 0.0216 - val_accuracy: 0.9746 - val_loss: 0.0975\n",
            "Epoch 11/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9951 - loss: 0.0175 - val_accuracy: 0.9768 - val_loss: 0.0879\n",
            "Epoch 12/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9960 - loss: 0.0136 - val_accuracy: 0.9764 - val_loss: 0.0968\n",
            "Epoch 13/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9971 - loss: 0.0107 - val_accuracy: 0.9752 - val_loss: 0.1004\n",
            "Epoch 14/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9976 - loss: 0.0097 - val_accuracy: 0.9754 - val_loss: 0.1061\n",
            "Epoch 15/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9977 - loss: 0.0081 - val_accuracy: 0.9751 - val_loss: 0.1111\n",
            "Epoch 16/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9979 - loss: 0.0062 - val_accuracy: 0.9771 - val_loss: 0.1093\n",
            "Epoch 17/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 0.9762 - val_loss: 0.1115\n",
            "Epoch 18/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 0.9753 - val_loss: 0.1244\n",
            "Epoch 19/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 0.9729 - val_loss: 0.1497\n",
            "Epoch 20/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 0.9781 - val_loss: 0.1193\n",
            "Epoch 21/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 0.9772 - val_loss: 0.1213\n",
            "Epoch 22/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0024 - val_accuracy: 0.9783 - val_loss: 0.1274\n",
            "Epoch 23/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9750 - val_loss: 0.1469\n",
            "Epoch 24/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.9775 - val_loss: 0.1345\n",
            "Epoch 25/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9788 - val_loss: 0.1336\n",
            "Epoch 26/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.6755e-04 - val_accuracy: 0.9769 - val_loss: 0.1423\n",
            "Epoch 27/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 8.6790e-04 - val_accuracy: 0.9780 - val_loss: 0.1340\n",
            "Epoch 28/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 5.0368e-04 - val_accuracy: 0.9788 - val_loss: 0.1356\n",
            "Epoch 29/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4862e-04 - val_accuracy: 0.9794 - val_loss: 0.1338\n",
            "Epoch 30/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.8744e-05 - val_accuracy: 0.9793 - val_loss: 0.1350\n",
            "Epoch 31/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.7816e-05 - val_accuracy: 0.9791 - val_loss: 0.1379\n",
            "Epoch 32/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9112e-05 - val_accuracy: 0.9789 - val_loss: 0.1382\n",
            "Epoch 33/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.7849e-05 - val_accuracy: 0.9787 - val_loss: 0.1387\n",
            "Epoch 34/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1863e-05 - val_accuracy: 0.9790 - val_loss: 0.1393\n",
            "Epoch 35/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.9694e-05 - val_accuracy: 0.9789 - val_loss: 0.1395\n",
            "Epoch 36/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.9052e-05 - val_accuracy: 0.9788 - val_loss: 0.1399\n",
            "Epoch 37/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.7401e-05 - val_accuracy: 0.9791 - val_loss: 0.1404\n",
            "Epoch 38/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.6706e-05 - val_accuracy: 0.9792 - val_loss: 0.1410\n",
            "Epoch 39/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4829e-05 - val_accuracy: 0.9791 - val_loss: 0.1413\n",
            "Epoch 40/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4070e-05 - val_accuracy: 0.9791 - val_loss: 0.1420\n",
            "Epoch 41/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.3810e-05 - val_accuracy: 0.9791 - val_loss: 0.1420\n",
            "Epoch 42/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.3859e-05 - val_accuracy: 0.9790 - val_loss: 0.1424\n",
            "Epoch 43/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.2331e-05 - val_accuracy: 0.9790 - val_loss: 0.1429\n",
            "Epoch 44/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.2512e-05 - val_accuracy: 0.9794 - val_loss: 0.1433\n",
            "Epoch 45/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.1931e-05 - val_accuracy: 0.9791 - val_loss: 0.1436\n",
            "Epoch 46/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1398e-05 - val_accuracy: 0.9793 - val_loss: 0.1438\n",
            "Epoch 47/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.1095e-05 - val_accuracy: 0.9793 - val_loss: 0.1441\n",
            "Epoch 48/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0940e-05 - val_accuracy: 0.9793 - val_loss: 0.1447\n",
            "Epoch 49/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0692e-05 - val_accuracy: 0.9795 - val_loss: 0.1447\n",
            "Epoch 50/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.0550e-05 - val_accuracy: 0.9792 - val_loss: 0.1450\n",
            "Epoch 51/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.0132e-05 - val_accuracy: 0.9793 - val_loss: 0.1453\n",
            "Epoch 52/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.3755e-06 - val_accuracy: 0.9793 - val_loss: 0.1459\n",
            "Epoch 53/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.5888e-06 - val_accuracy: 0.9791 - val_loss: 0.1460\n",
            "Epoch 54/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.8728e-06 - val_accuracy: 0.9790 - val_loss: 0.1464\n",
            "Epoch 55/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.2567e-06 - val_accuracy: 0.9791 - val_loss: 0.1467\n",
            "Epoch 56/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.8861e-06 - val_accuracy: 0.9793 - val_loss: 0.1469\n",
            "Epoch 57/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.5242e-06 - val_accuracy: 0.9791 - val_loss: 0.1471\n",
            "Epoch 58/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.9128e-06 - val_accuracy: 0.9793 - val_loss: 0.1472\n",
            "Epoch 59/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.8097e-06 - val_accuracy: 0.9792 - val_loss: 0.1474\n",
            "Epoch 60/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.8682e-06 - val_accuracy: 0.9792 - val_loss: 0.1476\n",
            "Epoch 61/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.7033e-06 - val_accuracy: 0.9794 - val_loss: 0.1480\n",
            "Epoch 62/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.6244e-06 - val_accuracy: 0.9792 - val_loss: 0.1481\n",
            "Epoch 63/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.6108e-06 - val_accuracy: 0.9792 - val_loss: 0.1483\n",
            "Epoch 64/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.3460e-06 - val_accuracy: 0.9792 - val_loss: 0.1487\n",
            "Epoch 65/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.9438e-06 - val_accuracy: 0.9789 - val_loss: 0.1488\n",
            "Epoch 66/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.2054e-06 - val_accuracy: 0.9793 - val_loss: 0.1490\n",
            "Epoch 67/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.6779e-06 - val_accuracy: 0.9792 - val_loss: 0.1493\n",
            "Epoch 68/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.5273e-06 - val_accuracy: 0.9792 - val_loss: 0.1494\n",
            "Epoch 69/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.4324e-06 - val_accuracy: 0.9791 - val_loss: 0.1496\n",
            "Epoch 70/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.6280e-06 - val_accuracy: 0.9792 - val_loss: 0.1499\n",
            "Epoch 71/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.1740e-06 - val_accuracy: 0.9790 - val_loss: 0.1500\n",
            "Epoch 72/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.1845e-06 - val_accuracy: 0.9792 - val_loss: 0.1501\n",
            "Epoch 73/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.4714e-06 - val_accuracy: 0.9792 - val_loss: 0.1504\n",
            "Epoch 74/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0409e-06 - val_accuracy: 0.9792 - val_loss: 0.1505\n",
            "Epoch 75/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.8809e-06 - val_accuracy: 0.9792 - val_loss: 0.1507\n",
            "Epoch 76/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.1142e-06 - val_accuracy: 0.9790 - val_loss: 0.1508\n",
            "Epoch 77/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.6163e-06 - val_accuracy: 0.9792 - val_loss: 0.1510\n",
            "Epoch 78/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.8107e-06 - val_accuracy: 0.9793 - val_loss: 0.1512\n",
            "Epoch 79/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.6168e-06 - val_accuracy: 0.9791 - val_loss: 0.1514\n",
            "Epoch 80/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.6544e-06 - val_accuracy: 0.9792 - val_loss: 0.1515\n",
            "Epoch 81/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.4983e-06 - val_accuracy: 0.9792 - val_loss: 0.1517\n",
            "Epoch 82/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.3610e-06 - val_accuracy: 0.9792 - val_loss: 0.1518\n",
            "Epoch 83/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.3476e-06 - val_accuracy: 0.9792 - val_loss: 0.1519\n",
            "Epoch 84/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.1477e-06 - val_accuracy: 0.9792 - val_loss: 0.1522\n",
            "Epoch 85/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.3446e-06 - val_accuracy: 0.9793 - val_loss: 0.1522\n",
            "Epoch 86/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.3364e-06 - val_accuracy: 0.9792 - val_loss: 0.1524\n",
            "Epoch 87/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.1759e-06 - val_accuracy: 0.9791 - val_loss: 0.1526\n",
            "Epoch 88/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.7224e-06 - val_accuracy: 0.9791 - val_loss: 0.1528\n",
            "Epoch 89/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.7552e-06 - val_accuracy: 0.9791 - val_loss: 0.1528\n",
            "Epoch 90/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.8514e-06 - val_accuracy: 0.9793 - val_loss: 0.1530\n",
            "Epoch 91/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.6546e-06 - val_accuracy: 0.9791 - val_loss: 0.1531\n",
            "Epoch 92/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.7726e-06 - val_accuracy: 0.9793 - val_loss: 0.1533\n",
            "Epoch 93/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.3449e-06 - val_accuracy: 0.9789 - val_loss: 0.1533\n",
            "Epoch 94/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.8551e-06 - val_accuracy: 0.9792 - val_loss: 0.1535\n",
            "Epoch 95/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.4404e-06 - val_accuracy: 0.9792 - val_loss: 0.1537\n",
            "Epoch 96/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.4003e-06 - val_accuracy: 0.9792 - val_loss: 0.1537\n",
            "Epoch 97/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.3672e-06 - val_accuracy: 0.9790 - val_loss: 0.1539\n",
            "Epoch 98/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.2408e-06 - val_accuracy: 0.9791 - val_loss: 0.1541\n",
            "Epoch 99/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.2841e-06 - val_accuracy: 0.9792 - val_loss: 0.1542\n",
            "Epoch 100/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.1205e-06 - val_accuracy: 0.9792 - val_loss: 0.1543\n",
            "Epoch 101/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.2415e-06 - val_accuracy: 0.9793 - val_loss: 0.1546\n",
            "Epoch 102/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.0298e-06 - val_accuracy: 0.9792 - val_loss: 0.1546\n",
            "Epoch 103/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.1097e-06 - val_accuracy: 0.9790 - val_loss: 0.1547\n",
            "Epoch 104/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.9807e-06 - val_accuracy: 0.9790 - val_loss: 0.1547\n",
            "Epoch 105/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.9234e-06 - val_accuracy: 0.9791 - val_loss: 0.1550\n",
            "Epoch 106/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9888e-06 - val_accuracy: 0.9793 - val_loss: 0.1552\n",
            "Epoch 107/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9546e-06 - val_accuracy: 0.9792 - val_loss: 0.1553\n",
            "Epoch 108/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.0121e-06 - val_accuracy: 0.9793 - val_loss: 0.1554\n",
            "Epoch 109/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.9266e-06 - val_accuracy: 0.9791 - val_loss: 0.1554\n",
            "Epoch 110/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.0797e-06 - val_accuracy: 0.9793 - val_loss: 0.1556\n",
            "Epoch 111/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.7599e-06 - val_accuracy: 0.9790 - val_loss: 0.1556\n",
            "Epoch 112/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.7368e-06 - val_accuracy: 0.9789 - val_loss: 0.1558\n",
            "Epoch 113/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9071e-06 - val_accuracy: 0.9793 - val_loss: 0.1559\n",
            "Epoch 114/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6159e-06 - val_accuracy: 0.9792 - val_loss: 0.1561\n",
            "Epoch 115/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.7811e-06 - val_accuracy: 0.9791 - val_loss: 0.1561\n",
            "Epoch 116/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.7340e-06 - val_accuracy: 0.9791 - val_loss: 0.1561\n",
            "Epoch 117/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.6381e-06 - val_accuracy: 0.9793 - val_loss: 0.1563\n",
            "Epoch 118/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.4178e-06 - val_accuracy: 0.9793 - val_loss: 0.1564\n",
            "Epoch 119/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.4150e-06 - val_accuracy: 0.9791 - val_loss: 0.1565\n",
            "Epoch 120/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.4842e-06 - val_accuracy: 0.9792 - val_loss: 0.1567\n",
            "Epoch 121/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.5982e-06 - val_accuracy: 0.9791 - val_loss: 0.1567\n",
            "Epoch 122/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.3635e-06 - val_accuracy: 0.9792 - val_loss: 0.1569\n",
            "Epoch 123/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2753e-06 - val_accuracy: 0.9792 - val_loss: 0.1570\n",
            "Epoch 124/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.3314e-06 - val_accuracy: 0.9791 - val_loss: 0.1571\n",
            "Epoch 125/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.3384e-06 - val_accuracy: 0.9791 - val_loss: 0.1572\n",
            "Epoch 126/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2162e-06 - val_accuracy: 0.9792 - val_loss: 0.1573\n",
            "Epoch 127/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2834e-06 - val_accuracy: 0.9791 - val_loss: 0.1573\n",
            "Epoch 128/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.1841e-06 - val_accuracy: 0.9791 - val_loss: 0.1575\n",
            "Epoch 129/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.1684e-06 - val_accuracy: 0.9791 - val_loss: 0.1576\n",
            "Epoch 130/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.1892e-06 - val_accuracy: 0.9791 - val_loss: 0.1576\n",
            "Epoch 131/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2710e-06 - val_accuracy: 0.9793 - val_loss: 0.1578\n",
            "Epoch 132/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9526e-06 - val_accuracy: 0.9793 - val_loss: 0.1579\n",
            "Epoch 133/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9912e-06 - val_accuracy: 0.9793 - val_loss: 0.1579\n",
            "Epoch 134/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.0541e-06 - val_accuracy: 0.9793 - val_loss: 0.1580\n",
            "Epoch 135/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9518e-06 - val_accuracy: 0.9792 - val_loss: 0.1581\n",
            "Epoch 136/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9692e-06 - val_accuracy: 0.9793 - val_loss: 0.1583\n",
            "Epoch 137/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.8999e-06 - val_accuracy: 0.9793 - val_loss: 0.1584\n",
            "Epoch 138/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9318e-06 - val_accuracy: 0.9793 - val_loss: 0.1585\n",
            "Epoch 139/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8840e-06 - val_accuracy: 0.9792 - val_loss: 0.1585\n",
            "Epoch 140/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9682e-06 - val_accuracy: 0.9792 - val_loss: 0.1586\n",
            "Epoch 141/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8690e-06 - val_accuracy: 0.9793 - val_loss: 0.1587\n",
            "Epoch 142/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.0155e-06 - val_accuracy: 0.9791 - val_loss: 0.1587\n",
            "Epoch 143/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9930e-06 - val_accuracy: 0.9792 - val_loss: 0.1588\n",
            "Epoch 144/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.9561e-06 - val_accuracy: 0.9793 - val_loss: 0.1590\n",
            "Epoch 145/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8061e-06 - val_accuracy: 0.9794 - val_loss: 0.1591\n",
            "Epoch 146/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7711e-06 - val_accuracy: 0.9792 - val_loss: 0.1591\n",
            "Epoch 147/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8224e-06 - val_accuracy: 0.9794 - val_loss: 0.1592\n",
            "Epoch 148/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.6449e-06 - val_accuracy: 0.9792 - val_loss: 0.1592\n",
            "Epoch 149/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7392e-06 - val_accuracy: 0.9792 - val_loss: 0.1594\n",
            "Epoch 150/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.7081e-06 - val_accuracy: 0.9790 - val_loss: 0.1595\n",
            "Epoch 151/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7678e-06 - val_accuracy: 0.9792 - val_loss: 0.1595\n",
            "Epoch 152/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.7223e-06 - val_accuracy: 0.9793 - val_loss: 0.1596\n",
            "Epoch 153/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.7849e-06 - val_accuracy: 0.9793 - val_loss: 0.1598\n",
            "Epoch 154/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.6578e-06 - val_accuracy: 0.9793 - val_loss: 0.1598\n",
            "Epoch 155/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.5985e-06 - val_accuracy: 0.9794 - val_loss: 0.1600\n",
            "Epoch 156/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6048e-06 - val_accuracy: 0.9791 - val_loss: 0.1600\n",
            "Epoch 157/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6062e-06 - val_accuracy: 0.9793 - val_loss: 0.1601\n",
            "Epoch 158/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4822e-06 - val_accuracy: 0.9792 - val_loss: 0.1601\n",
            "Epoch 159/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.4320e-06 - val_accuracy: 0.9793 - val_loss: 0.1602\n",
            "Epoch 160/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.4712e-06 - val_accuracy: 0.9791 - val_loss: 0.1603\n",
            "Epoch 161/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4424e-06 - val_accuracy: 0.9792 - val_loss: 0.1603\n",
            "Epoch 162/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.4433e-06 - val_accuracy: 0.9792 - val_loss: 0.1605\n",
            "Epoch 163/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.4631e-06 - val_accuracy: 0.9793 - val_loss: 0.1605\n",
            "Epoch 164/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.3716e-06 - val_accuracy: 0.9792 - val_loss: 0.1606\n",
            "Epoch 165/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4477e-06 - val_accuracy: 0.9792 - val_loss: 0.1607\n",
            "Epoch 166/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4624e-06 - val_accuracy: 0.9792 - val_loss: 0.1608\n",
            "Epoch 167/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4033e-06 - val_accuracy: 0.9792 - val_loss: 0.1609\n",
            "Epoch 168/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.3874e-06 - val_accuracy: 0.9793 - val_loss: 0.1610\n",
            "Epoch 169/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.3945e-06 - val_accuracy: 0.9793 - val_loss: 0.1610\n",
            "Epoch 170/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3270e-06 - val_accuracy: 0.9792 - val_loss: 0.1611\n",
            "Epoch 171/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.5237e-06 - val_accuracy: 0.9791 - val_loss: 0.1612\n",
            "Epoch 172/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3937e-06 - val_accuracy: 0.9791 - val_loss: 0.1612\n",
            "Epoch 173/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.2675e-06 - val_accuracy: 0.9791 - val_loss: 0.1613\n",
            "Epoch 174/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2309e-06 - val_accuracy: 0.9793 - val_loss: 0.1614\n",
            "Epoch 175/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2656e-06 - val_accuracy: 0.9792 - val_loss: 0.1614\n",
            "Epoch 176/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.2171e-06 - val_accuracy: 0.9792 - val_loss: 0.1615\n",
            "Epoch 177/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.2147e-06 - val_accuracy: 0.9792 - val_loss: 0.1616\n",
            "Epoch 178/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3229e-06 - val_accuracy: 0.9791 - val_loss: 0.1616\n",
            "Epoch 179/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2554e-06 - val_accuracy: 0.9790 - val_loss: 0.1617\n",
            "Epoch 180/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2194e-06 - val_accuracy: 0.9792 - val_loss: 0.1618\n",
            "Epoch 181/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.2492e-06 - val_accuracy: 0.9791 - val_loss: 0.1619\n",
            "Epoch 182/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.3026e-06 - val_accuracy: 0.9793 - val_loss: 0.1619\n",
            "Epoch 183/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0648e-06 - val_accuracy: 0.9791 - val_loss: 0.1620\n",
            "Epoch 184/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.1887e-06 - val_accuracy: 0.9793 - val_loss: 0.1620\n",
            "Epoch 185/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.1882e-06 - val_accuracy: 0.9791 - val_loss: 0.1622\n",
            "Epoch 186/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1450e-06 - val_accuracy: 0.9793 - val_loss: 0.1623\n",
            "Epoch 187/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.0997e-06 - val_accuracy: 0.9793 - val_loss: 0.1623\n",
            "Epoch 188/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.1484e-06 - val_accuracy: 0.9792 - val_loss: 0.1624\n",
            "Epoch 189/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0267e-06 - val_accuracy: 0.9793 - val_loss: 0.1624\n",
            "Epoch 190/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0741e-06 - val_accuracy: 0.9793 - val_loss: 0.1625\n",
            "Epoch 191/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.0318e-06 - val_accuracy: 0.9791 - val_loss: 0.1625\n",
            "Epoch 192/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.0402e-06 - val_accuracy: 0.9793 - val_loss: 0.1626\n",
            "Epoch 193/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.0868e-06 - val_accuracy: 0.9791 - val_loss: 0.1627\n",
            "Epoch 194/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.1166e-06 - val_accuracy: 0.9792 - val_loss: 0.1628\n",
            "Epoch 195/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1122e-06 - val_accuracy: 0.9792 - val_loss: 0.1628\n",
            "Epoch 196/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.9175e-06 - val_accuracy: 0.9792 - val_loss: 0.1629\n",
            "Epoch 197/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0398e-06 - val_accuracy: 0.9791 - val_loss: 0.1629\n",
            "Epoch 198/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0781e-06 - val_accuracy: 0.9792 - val_loss: 0.1630\n",
            "Epoch 199/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.9857e-06 - val_accuracy: 0.9792 - val_loss: 0.1631\n",
            "Epoch 200/200\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.9959e-06 - val_accuracy: 0.9792 - val_loss: 0.1632\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.1565\n",
            "\n",
            "Model with RMSprop Optimizer:\n",
            "Test accuracy: 0.9808  | Test loss: 0.1346\n",
            "\n",
            "Confusion Matrix (RMSprop):\n",
            "[[ 972    0    1    0    1    2    2    1    1    0]\n",
            " [   0 1125    4    0    0    0    2    1    3    0]\n",
            " [   2    1 1012    4    3    0    1    5    4    0]\n",
            " [   1    1    5  995    0    1    0    2    4    1]\n",
            " [   1    0    1    0  964    0    6    2    1    7]\n",
            " [   4    0    1   11    1  863    3    1    5    3]\n",
            " [   3    4    1    1    4    4  940    0    1    0]\n",
            " [   1    6    5    2    1    0    0 1004    3    6]\n",
            " [   1    0    4    8    1    1    2    3  952    2]\n",
            " [   1    2    1    4   10    3    1    4    2  981]]\n"
          ]
        }
      ],
      "source": [
        "#rmsprop\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Set hyperparameters\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Load and preprocess data\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "RESHAPED = 784\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# Define a function to create and train a model with a given optimizer\n",
        "def train_model_with_optimizer(optimizer, optimizer_name):\n",
        "    print(f\"Training model with {optimizer_name} optimizer...\")\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(RESHAPED,)),\n",
        "        keras.layers.Dense(N_HIDDEN, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        keras.layers.Dense(N_HIDDEN // 2, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "        keras.layers.Dense(NB_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=VALIDATION_SPLIT,\n",
        "        verbose=VERBOSE,\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print(f\"\\nModel with {optimizer_name} Optimizer:\")\n",
        "    print(f\"Test accuracy: {test_acc:.4f}  | Test loss: {test_loss:.4f}\")\n",
        "\n",
        "    y_true = np.argmax(Y_test, axis=1)\n",
        "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
        "    cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=NB_CLASSES).numpy()\n",
        "    print(f\"\\nConfusion Matrix ({optimizer_name}):\")\n",
        "    print(cm)\n",
        "\n",
        "    return history, test_acc, test_loss\n",
        "\n",
        "# Train models with different optimizers\n",
        "optimizers = {\n",
        "    \"RMSprop\": keras.optimizers.RMSprop(),\n",
        "}\n",
        "\n",
        "results_optimizers = {}\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    history, test_acc, test_loss = train_model_with_optimizer(optimizer, optimizer_name)\n",
        "    results_optimizers[optimizer_name] = {\"history\": history, \"test_acc\": test_acc, \"test_loss\": test_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wUjUCae8dzvj",
      "metadata": {
        "id": "wUjUCae8dzvj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "31f4ee6a",
      "metadata": {
        "id": "31f4ee6a"
      },
      "source": [
        "### Comparative Analysis of Optimizers for MNIST Classification\n",
        "\n",
        "This analysis compares the performance of three different optimizers (SGD, Adam, and Adagrad) when training a simple Multi-layer Perceptron (MLP) on the MNIST dataset. The MLP architecture consists of two hidden layers with LeakyReLU activation and an output layer with softmax activation.\n",
        "\n",
        "| Optimizer | Test Accuracy | Test Loss |\n",
        "|---|---|---|\n",
        "| SGD | 0.9699 | 0.1086 |\n",
        "| Adam | 0.9808 | 0.2112 |\n",
        "| Adagrad | 0.9391 | 0.2250 |\n",
        "| RMSprop | 0.9757 | 0.1400 |\n",
        "\n",
        "**Observations and Discussion:**\n",
        "\n",
        "*   **Adam Optimizer:** Achieved the highest test accuracy (0.9808) among the tested optimizers. It also converged relatively quickly, as seen in the training history. This is consistent with Adam's reputation for performing well on a wide range of deep learning tasks due to its adaptive learning rate capabilities.\n",
        "*   **SGD Optimizer:** While not reaching the same peak accuracy as Adam, the SGD optimizer still performed reasonably well (0.9699 accuracy). Its training process showed a more gradual improvement in loss compared to Adam.\n",
        "*   **Adagrad Optimizer:** Had the lowest test accuracy (0.9391) and a higher test loss compared to Adam and SGD. Adagrad's adaptive learning rate can sometimes cause the learning rate to become too small too quickly, potentially hindering convergence on this dataset.\n",
        "*   **RMSprop Optimizer:** Performed well (0.9757 accuracy), coming in second place after Adam. RMSprop is also an adaptive learning rate optimizer and is often a good alternative to Adam.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Based on this experiment, the **Adam optimizer** demonstrated the best performance in terms of achieving the highest test accuracy on the MNIST dataset with this specific MLP architecture. SGD also performed adequately, while Adagrad struggled to reach the same level of accuracy within the given epochs. RMSprop is another strong contender.\n",
        "\n",
        "It's important to note that the optimal optimizer can depend on the dataset, network architecture, and hyperparameters. Further tuning of hyperparameters (learning rate, batch size, number of epochs, etc.) and experimenting with other optimizers could potentially lead to even better results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
